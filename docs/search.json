[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the oscillatory regime (activity rank \\(\\sim50\\)): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j \\in [10^{-2}, 10^{-1}]\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n136 iterations completed across 12 blocks (34 batches). The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). 100% autocatalytic 3-cycles.\n\n\n\n\n\n\n\n\n\nThe primary metric is raw R² computed on all 256 reactions (after MLP\\(_{\\text{sub}}\\) scalar correction). The trimmed R² excludes outlier reactions (\\(|\\Delta \\log_{10} k| &gt; 0.3\\)) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 137))\nr2 = [\n    # Block 1 (Iter 1-12): Initial exploration\n    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep\n    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough\n    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation\n    # Block 2 (Iter 13-24): k_floor breakthrough\n    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough\n    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor\n    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best\n    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough\n    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns\n    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches\n    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH\n    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7\n    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub\n    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed\n    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST\n    # Block 5 (Iter 49-60): Confirming optimum\n    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests\n    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds\n    0.701, 0.718, 0.603, 0.603,  # Batch 15: seed=99 promising\n    # Block 6 (Iter 61-72): Variance discovery\n    0.688, 0.430, 0.616, 0.704,  # Batch 16: k_floor=1.5 promising\n    0.565, 0.674, 0.664, 0.609,  # Batch 17: k_floor non-monotonic\n    0.658, 0.639, 0.473, 0.550,  # Batch 18: HIGH VARIANCE discovered\n    # Block 7 (Iter 73-84): seed=77 breakthrough\n    0.722, 0.583, 0.682, 0.636,  # Batch 19: seed=123 improved\n    0.694, 0.515, 0.473, 0.748,  # Batch 20: seed=77 NEW BEST\n    0.661, 0.764, 0.387, 0.689,  # Batch 21: seed=77+sub_diff=8 PEAK\n    # Block 8 (Iter 85-96): seed=79 breakthrough + aug=5000\n    0.619, 0.512, 0.748, 0.720,  # Batch 22: seed=79 golden seed discovered\n    0.822, 0.640, 0.572, 0.704,  # Batch 23: seed=79 replicate R²=0.82\n    0.592, 0.619, 0.546, 0.851,  # Batch 24: seed=79+aug=5000 NEW BEST\n    # Block 9 (Iter 97-108): seed=77+sub_diff=6 discovery\n    0.720, 0.609, 0.742, 0.471,  # Batch 25: aug=5500 hurts seed=79\n    0.754, 0.634, 0.730, 0.764,  # Batch 26: sub_diff=7 for seed=77\n    0.780, 0.804, 0.683, 0.623,  # Batch 27: sub_diff=6 2ND BEST\n    # Block 10 (Iter 109-120): GLOBAL BEST iter 116\n    0.550, 0.572, 0.655, 0.589,  # Batch 28: sub_diff=6 cross-seed (variance!)\n    0.637, 0.643, 0.566, 0.869,  # Batch 29: seed=77+sub_diff=6+aug=5500 GLOBAL BEST\n    0.657, 0.518, 0.581, 0.812,  # Batch 30: aug=5750 hurts, sub_diff=7 good\n    # Block 11 (Iter 121-132): Robustness testing\n    0.734, 0.733, 0.574, 0.509,  # Batch 31: sub_diff=7 robust, sub_diff=8 hurts\n    0.782, 0.680, 0.544, 0.670,  # Batch 32: sub_diff=7 variance ~0.05\n    0.509, 0.675, 0.721, 0.846,  # Batch 33: k_floor=1.5 2ND BEST\n    # Block 12 (Iter 133-136): k_floor=1.5 variance confirmed\n    0.635, 0.512, 0.677, 0.692,  # Batch 34: k_floor=1.5 not reproducible\n]\noutliers = [\n    43, 47, 45, 53,\n    61, 36, 45, 32,\n    27, 38, 57, 30,\n    28, 33, 36, 33,\n    17, 24, 26, 35,\n    16, 29, 19, 24,\n    18, 18, 15, 17,\n    19, 14, 21, 21,\n    21, 21, 15, 22,  # Batch 9\n    16, 19, 20, 21,  # Batch 10\n    21, 16, 18, 19,  # Batch 11\n    15, 20, 23, 21,  # Batch 12\n    12, 21, 12, 25,  # Batch 13\n    21, 21, 18, 19,  # Batch 14\n    17, 17, 16, 20,  # Batch 15\n    17, 20, 17, 16,  # Batch 16\n    17, 14, 16, 14,  # Batch 17\n    19, 14, 22, 17,  # Batch 18\n    19, 16, 22, 20,  # Batch 19\n    16, 16, 32, 12,  # Batch 20\n    16, 15, 23, 18,  # Batch 21\n    16, 22, 15, 20,  # Batch 22\n    13, 13, 15, 17,  # Batch 23\n    22, 14, 21, 11,  # Batch 24\n    18, 20, 15, 16,  # Batch 25\n    17, 16, 16, 13,  # Batch 26\n    16, 12, 14, 12,  # Batch 27\n    14, 15, 18, 17,  # Batch 28\n    15, 17, 17, 10,  # Batch 29\n    18, 20, 15, 10,  # Batch 30\n    12, 10, 17, 14,  # Batch 31\n    10, 10, 15, 14,  # Batch 32\n    18, 10, 12, 12,  # Batch 33\n    18, 21, 11, 20,  # Batch 34\n]\n\n# Color by block\nblock_colors = [\n    '#3498db', '#2ecc71', '#e67e22', '#9b59b6',\n    '#1abc9c', '#e84393', '#f39c12', '#e74c3c',\n    '#2c3e50', '#d35400', '#8e44ad', '#27ae60',\n]\nblock_bounds = [12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 136]\ncolors = []\nfor i in range(len(r2)):\n    for b, bound in enumerate(block_bounds):\n        if i &lt; bound:\n            colors.append(block_colors[b])\n            break\n\nfig, ax1 = plt.subplots(figsize=(18, 5))\n\nbars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)\nax1.axhline(y=0.869, color='#e74c3c', linestyle='--', alpha=0.4, label='Peak R² = 0.869')\nax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')\n\n# Block separators and labels\nfor b in range(len(block_bounds) - 1):\n    ax1.axvline(x=block_bounds[b] + 0.5, color='gray', linestyle=':', alpha=0.4)\nblock_labels = [\n    'Block 1', 'Block 2', 'Block 3', 'Block 4', 'Block 5', 'Block 6',\n    'Block 7', 'Block 8', 'Block 9', 'Block 10', 'Block 11', 'Block 12',\n]\nblock_centers = [6.5, 18.5, 30.5, 42.5, 54.5, 66.5, 78.5, 90.5, 102.5, 114.5, 126.5, 134.5]\nfor i, (lbl, cx) in enumerate(zip(block_labels, block_centers)):\n    ax1.text(cx, 1.05, lbl, ha='center', fontsize=7, color=block_colors[i],\n             bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='none', alpha=0.8))\n\n# Annotate key events\nax1.annotate('k_floor=1.0\\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.25),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(33, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('sub_diff=7', xy=(45, 0.736), xytext=(45, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('seed=77+\\nsub_diff=8', xy=(82, 0.764), xytext=(82, 0.90),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('seed=79+\\naug=5000', xy=(96, 0.851), xytext=(93, 0.95),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('sub_diff=6\\nR²=0.80', xy=(106, 0.804), xytext=(106, 0.90),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('GLOBAL BEST\\nR²=0.87', xy=(116, 0.869), xytext=(116, 0.96),\n            arrowprops=dict(arrowstyle='-&gt;', color='red'), fontsize=7, ha='center', color='red')\nax1.annotate('k_floor=1.5\\nR²=0.85', xy=(132, 0.846), xytext=(132, 0.93),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('rate_constants R² (raw)')\nax1.set_ylim(-0.02, 1.05)\nax1.set_xticks([1, 12, 24, 36, 48, 60, 72, 84, 96, 106, 108, 116, 120, 132, 136])\n\n# outlier count on secondary axis\nax2 = ax1.twinx()\nax2.plot(iters, outliers, 'o', color='black', markersize=3, alpha=0.4, label='outliers')\nax2.set_ylabel('outlier count')\nax2.tick_params(axis='y')\nax2.set_ylim(0, 70)\n\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)\n\nax1.set_title('UCB Exploration: Rate Constant Recovery (136 iterations)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Raw R² (bars) and outlier count (dots) across 136 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74), seed=77+sub_diff=8 (iter 82, 0.76), seed=79+aug=5000 (iter 96, 0.85), seed=77+sub_diff=6 (iter 106, 0.80), seed=77+sub_diff=6+aug=5500 (iter 116, 0.87). Training variance of ±0.2 R² is a dominant factor.\n\n\n\n\n\n\n\n\nAll 12 iterations achieved raw R² &lt; 0.07. The key discovery was that coeff_MLP_sub_norm=1.0 is essential — it corrects the MLP\\(_{\\text{sub}}\\) function shapes (c² becomes quadratic instead of linear) and enables MLP\\(_{\\text{node}}\\) to learn homeostasis.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n1 (iter 1–4)\nlr sweep\n0.044\nAll failed; MLP\\(_{\\text{node}}\\) dead, MLP\\(_{\\text{sub}}\\) c² linear\n\n\n2 (iter 5–8)\nsub_norm=1.0\n0.067\nMLP\\(_{\\text{sub}}\\) normalization is the single most effective change\n\n\n3 (iter 9–12)\ncombine best\n0.061\nMLP\\(_{\\text{node}}\\) activated; lr_node=0.005 hurts\n\n\n\n\n\n\nThe coeff_k_floor=1.0 penalty at iteration 14 produced a 10× improvement in R² (0.06 → 0.51) by preventing outlier \\(\\log k\\) values from drifting below the true minimum. Longer training then pushed R² to 0.69.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n4 (iter 13–16)\nk_floor=1.0\n0.508\nBreakthrough — R² jumped 10×\n\n\n5 (iter 17–20)\naug=3000\n0.642\nLonger training + k_floor synergistic\n\n\n6 (iter 21–24)\naug=4000\n0.690\nFirst to reach 0.69; \\(\\alpha = 0.85\\), 16 outliers\n\n\n\nBest result — Iteration 116: seed=77, coeff_MLP_sub_diff=6, lr_sub=0.001, data_augmentation_loop=5500, coeff_k_floor=1.0, coeff_MLP_sub_norm=1.0\n\n\n\nMetric\nValue\n\n\n\n\nRaw R²\n0.869\n\n\nOutliers\n10 / 256 (3.9%)\n\n\nSlope\n0.98\n\n\n\\(\\alpha\\)\n0.93\n\n\n\nNote: Optimal sub_diff and aug are seed-dependent: sub_diff=6 for seed=77 (R²=0.87), sub_diff=7 for seed=42 (R²=0.74), sub_diff=8 for seed=79 (R²=0.85). seed=77 benefits from aug=5500, seed=79 from aug=5000. Training has intrinsic variance of $$0.2–0.3 R² even with identical configuration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline). R² = 0.044, 43 outliers. Before k_floor and sub_norm — predicted \\(\\log k\\) values scatter widely with no correlation to ground truth.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The k_floor=1.0 penalty prevents outlier \\(\\log k\\) from drifting below the true minimum, producing a 10\\(\\times\\) R² jump.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Iteration 116 (best). R² = 0.869, 10 outliers. seed=77 + sub_diff=6 + aug=5500 yields the tightest clustering around the diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Iteration 1 (baseline). Top-left: ground-truth \\(dc/dt\\); top-right: GNN prediction; bottom-left: residual; bottom-right: scatter. The untrained GNN produces near-zero predictions — no reaction dynamics captured.\n\n\n\n\n\n\n\n\n\nFigure 8: Iteration 116 (best, seed=77, sub_diff=6, aug=5500). Same layout. The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure. The residual reveals the missing homeostatic signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Iteration 1 (baseline). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.42 — under-scaled. MLP\\(_{\\text{node}}\\): flat at zero.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Iteration 116 (best R²). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.93 — close to the true scale. MLP\\(_{\\text{node}}\\): still flat at zero across all 136 iterations — homeostasis not learned.\n\n\n\n\n\n\n\n\n\nTwelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling lr_sub from 0.0005 to 0.001 broke through to R² = 0.726.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n7 (iter 25–28)\naug=5000, seed, batch_size\n0.652\naug=5000 hurt R² but \\(\\alpha=0.95\\) (best ever)\n\n\n8 (iter 29–32)\nL1=0, sub_norm=2.0, lr_k\n0.619\nsub_norm=2.0: fewest outliers (14), best slope (0.99)\n\n\n9 (iter 33–36)\nsub_norm=2.0+aug=3500, recurrent, lr_sub=0.001, aug=3500\n0.726\nlr_sub=0.001 broke the plateau\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstrategies = {\n    'No k_floor\\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],\n    'k_floor\\nsub_diff=5\\nseed=42': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],\n    'sub_diff=7\\nseed=42': [0.736, 0.696, 0.655, 0.662, 0.560, 0.701, 0.718, 0.658, 0.720, 0.574],\n    'seed=77\\nsub_diff=6': [0.804, 0.550, 0.869, 0.657, 0.581, 0.670, 0.512, 0.635],\n    'seed=77\\nsub_diff=7': [0.764, 0.780, 0.643, 0.812, 0.734, 0.733, 0.782, 0.680, 0.509, 0.675, 0.846],\n    'seed=79\\nsub_diff=8': [0.748, 0.822, 0.592, 0.619, 0.851, 0.720, 0.609, 0.634, 0.637],\n    'seed=77\\nsub_diff=8': [0.748, 0.661, 0.764, 0.619, 0.704, 0.742, 0.754, 0.509],\n    'k_floor=1.5': [0.704, 0.674, 0.846, 0.635, 0.512, 0.677, 0.692],\n}\n\nfig, ax = plt.subplots(figsize=(16, 5))\npositions = []\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#d35400', '#e84393', '#9b59b6', '#95a5a6']\nfor i, (label, vals) in enumerate(strategies.items()):\n    x = np.random.normal(i, 0.08, len(vals))\n    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3, color=colors[i], edgecolors='none')\n    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)\n    positions.append(i)\n\nax.set_xticks(positions)\nax.set_xticklabels(list(strategies.keys()), fontsize=8)\nax.set_ylabel('raw R²', fontsize=12)\nax.set_ylim(-0.05, 0.95)\nax.axhline(y=0.869, color='#f39c12', linestyle='--', alpha=0.3, label='Peak R² = 0.869')\nax.grid(axis='y', alpha=0.3)\nax.legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: Impact of key strategies on R². Each dot is one iteration. seed=77+sub_diff=6+aug=5500 achieved the global best R²=0.869 (iter 116). Training has extreme intrinsic variance of ±0.2–0.3 R².\n\n\n\n\n\n\n\n\nBlock 4 first tested combinations with lr_sub=0.001 and revealed seed sensitivity ($$0.2 R²). Then sub_diff=7 (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n10 (iter 37–40)\nsub_norm=2.0, lr_sub=0.002, lr_node=0.002, L1=0\n0.662\nAll combinations with lr_sub=0.001 hurt R² vs baseline\n\n\n11 (iter 41–44)\nseed=123, aug=4500, sub_diff=3, lr_k=0.007\n0.690\naug=4500 most stable (\\(\\alpha = 0.94\\)); seed=123 crashed to R²=0.49\n\n\n12 (iter 45–48)\nsub_diff=7, lr_k=0.004, hidden_dim=128, batch_size=16\n0.736\nsub_diff=7 new best; wider/deeper MLP and larger batch hurt\n\n\n\nIteration 45 (sub_diff=7, aug=4500) achieved R² = 0.736 with \\(\\alpha = 0.90\\) and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46–48 confirmed that lr_k=0.004 is too slow, hidden_dim_sub=128 allows degenerate solutions, and batch_size=16 degrades convergence.\n\n\n\nBlock 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n13 (iter 49–52)\naug=5000, sub_diff=8, seed=123, n_layers=4\n0.696\naug=5000 hurts (confirmed twice); sub_diff=7 improves seed robustness ($$0.08 vs $$0.24)\n\n\n14 (iter 53–56)\naug=4250, sub_diff=6, seed=123+aug=4000, lr_sub=0.0015\n0.701\nAll variations hurt; sub_diff=6 too weak, lr_sub=0.0015 too high\n\n\n15 (iter 57–60)\nseed=123+aug=3500, seed=99, L1=0.5, sub_norm=0.5\n0.718\nseed=99 promising (R²=0.72, \\(\\alpha=0.92\\)); sub_norm=0.5 confirmed essential principle\n\n\n\nTight bounds confirmed: aug=4500 (not 4000, 4250, or 5000), sub_diff=7 (not 5, 6, or 8), lr_sub=0.001 (not 0.0015 or 0.002), lr_k=0.005 (not 0.004 or 0.007). sub_diff=7 improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with sub_diff=5) to 0.08.\n\n\n\nBlock 6 explored new seeds and k_floor variations, then discovered that training has high intrinsic variance — an exact replica of the best configuration (iter 45) achieved only R²=0.66 instead of 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n16 (iter 61–64)\nseed=7, seed=99+aug=4250, lr_node=0.0005, k_floor=1.5\n0.704\nk_floor=1.5 promising (R²=0.70, \\(\\alpha=0.92\\)); seed=99 very sensitive to aug\n\n\n17 (iter 65–68)\nk_floor=1.25, k_floor=1.5+seed=99, aug=4750, sub_diff=6+k_floor=1.5\n0.674\nk_floor response is non-monotonic: 1.25 &lt; 1.0 and 1.5; aug=4750 hurts\n\n\n18 (iter 69–72)\nExact replica of iter 45, lr_k=0.0045, hidden_dim_node=32, sub_norm=1.5\n0.658\nHIGH VARIANCE: replica got R²=0.66 vs original 0.74; MLP\\(_{\\text{sub}}\\) c² failure mode\n\n\n\nThe high variance discovery is significant: identical configurations can produce R² values differing by 0.08, meaning many earlier “improvements” may have been within noise. This makes seed selection and reproducibility central concerns.\n\n\n\nBlock 7 explored new seeds and found that seed=77 is a “golden seed” — achieving R²=0.748 (iter 80) and then R²=0.764 with sub_diff=8 (iter 82), surpassing the previous best.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n19 (iter 73–76)\nseed=123, lr_sub=0.0012, sub_diff=9, aug=4000\n0.722\nseed=123 improved to R²=0.72 (vs 0.66 earlier); all other variations hurt\n\n\n20 (iter 77–80)\nseed=7, k_floor=1.5, n_layers=4, seed=77\n0.748\nseed=77 new best seed (R²=0.748, 12 outliers); n_layers=4 confirmed harmful\n\n\n21 (iter 81–84)\nReplicate seed=77, seed=77+sub_diff=8, seed=78, aug=5000\n0.764\nseed=77+sub_diff=8 = new global best; replica got R²=0.66 (variance); seed=78 poor (R²=0.39)\n\n\n\nThe key finding: optimal sub_diff is seed-dependent. sub_diff=8 hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76). Adjacent seeds (78, 79) do not share seed=77’s properties — “golden seeds” are rare and unpredictable.\n\n\n\nBlock 8 discovered seed=79 as a new golden seed (R²=0.748, iter 87) and then broke the R²=0.85 barrier when combined with aug=5000 (R²=0.851, iter 96) — refuting the established principle that aug&gt;4500 always hurts.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n22 (iter 85–88)\nreplicate, seed=76, seed=79, seed=42+sub_diff=8\n0.748\nseed=79 is a golden seed (R²=0.75); seed=42+sub_diff=8 works with lr_sub=0.001 (R²=0.72, refutes earlier principle)\n\n\n23 (iter 89–92)\nreplicate seed=79, sub_diff=9, seed=80, seed=77+sub_diff=9\n0.822\nseed=79 replicate broke 0.80 barrier (R²=0.82); sub_diff=9 hurts all golden seeds (3× confirmed)\n\n\n24 (iter 93–96)\nreplicate, sub_diff=7, seed=81, seed=79+aug=5000\n0.851\nNEW GLOBAL BEST R²=0.851; aug=5000 helps seed=79 (principle refuted for this seed)\n\n\n\nThe block revealed extreme training variance ($$0.2 R²): the same seed=79+sub_diff=8 config gave R²=0.82 (iter 89) and R²=0.59 (iter 93). The aug=5000 result (iter 96, R²=0.851, 11 outliers, \\(\\alpha\\)=0.94, slope=0.99) was the strongest single run at the time.\n\n\n\nBlock 9 explored aug boundaries for golden seeds and discovered that sub_diff=6 is optimal for seed=77 — achieving R²=0.804 (iter 106), the second-best result at the time.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n25 (iter 97–100)\nreplicate seed=79, aug=5500, seed=42+aug=5000, seed=79+sub_diff=7\n0.742\naug=5500 hurts seed=79 (R²=0.61); sub_diff=7 catastrophic for seed=79 (R²=0.47)\n\n\n26 (iter 101–104)\nseed=77+aug=4500, replicate seed=79, seed=42+aug=5000, seed=77+sub_diff=7\n0.764\nseed=77 consistent R²≈0.75; seed=79 extreme variance (0.85→0.63)\n\n\n27 (iter 105–108)\nreplicate, seed=77+sub_diff=6, seed=42+sub_diff=7, seed=77+sub_diff=9\n0.804\nsub_diff=6 is optimal for seed=77; sub_diff=9 confirmed harmful (3× refuted)\n\n\n\nThe key insight: optimal sub_diff is not just seed-dependent but follows a pattern — lower values (6) work best for seed=77 while higher values (8) suit seed=79. aug=5500 was found to hurt seed=79 but would later prove essential for seed=77.\n\n\n\nBlock 10 tested sub_diff=6 cross-seed transfer and then achieved the global best R²=0.869 at iteration 116 with seed=77+sub_diff=6+aug=5500.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n28 (iter 109–112)\nreplicate iter 106, seed=77+sub_diff=5, seed=79+sub_diff=6, seed=42+sub_diff=6\n0.655\nExtreme variance: replica of R²=0.80 got R²=0.55; sub_diff=6 doesn’t transfer to seed=79/42\n\n\n29 (iter 113–116)\nreplicate seed=79, seed=77+sub_diff=7+aug=5000, seed=79+sub_diff=7, seed=77+sub_diff=6+aug=5500\n0.869\nNEW GLOBAL BEST R²=0.869 (iter 116); aug=5500 helps seed=77 (refutes “aug&gt;4500 hurts”)\n\n\n30 (iter 117–120)\nreplicate iter 116, aug=5750, sub_diff=5, sub_diff=7+aug=5500\n0.812\naug=5750 hurts (R²=0.52); sub_diff=7+aug=5500 also strong (R²=0.81)\n\n\n\nIteration 116 (R²=0.869, 10 outliers, \\(\\alpha\\)=0.93) established the current global best. The aug=5500 finding refuted the principle that longer training always hurts — it is seed-dependent. However, aug=5750 overshoots even for seed=77.\n\n\n\nBlock 11 systematically tested robustness of the top configurations and discovered that k_floor=1.5 combined with sub_diff=7 can achieve R²=0.846 (iter 132, second-best overall).\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n31 (iter 121–124)\nreplicate sub_diff=7, aug=5250, seed=42+sub_diff=7, seed=77+sub_diff=8\n0.734\nsub_diff=7 more robust than sub_diff=6 (variance ~0.08 vs ~0.21); sub_diff=8 hurts seed=77\n\n\n32 (iter 125–128)\nreplicate, aug=5750, seed=79+sub_diff=7, seed=77+sub_diff=6\n0.782\naug=5750 hurts sub_diff=7 too; sub_diff=7 doesn’t transfer to seed=79\n\n\n33 (iter 129–132)\nworst replicate, aug=5250, seed=42+sub_diff=6, seed=77+sub_diff=7+k_floor=1.5\n0.846\n2ND BEST R²=0.846 with k_floor=1.5; extreme variance persists (R²=0.51–0.85)\n\n\n\nThe sub_diff=7 configuration proved more robust than sub_diff=6 for seed=77: R² range 0.73–0.81 (variance ~0.08) vs 0.55–0.87 (variance ~0.21). The trade-off is clear — sub_diff=6 has a higher ceiling but lower floor.\n\n\n\nBlock 12 tested whether k_floor=1.5 reliably improves R² and found that variance dominates configuration differences.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n34 (iter 133–136)\nreplicate iter 132, sub_diff=6+k_floor=1.5, seed=42+k_floor=1.5, k_floor=2.0\n0.692\nReplicate R²=0.64 vs 0.85 (variance ~0.21); sub_diff=6 requires k_floor=1.0; k_floor=2.0 not catastrophic\n\n\n\nThe main conclusion: with $$0.2–0.3 R² training variance, configuration differences below ~0.15 R² are indistinguishable from noise.\n\n\n\n\n\n\n\n\n\nFigure 12: UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the k_floor=1.0 breakthrough (node 14) and converged on the aug=4000 regime. The tree has since grown to 136 nodes; the overall best is node 116 (R² = 0.869).\n\n\n\n\n\n\nThe LLM’s persistent memory has accumulated these validated principles after 136 iterations:\n\ncoeff_MLP_sub_norm=1.0 is essential — enables correct MLP shapes: c² becomes quadratic, MLP\\(_{\\text{node}}\\) activates. sub_norm=0.5 and sub_norm=1.5 both hurt (iter 60, 72).\ncoeff_k_floor=1.0–1.5 is the optimal range — R² jumped from 0.06 to 0.51 (10× improvement) with k_floor=1.0. k_floor=1.5+sub_diff=7 achieved R²=0.85 (iter 132, 2nd best). Response is non-monotonic: k_floor=1.25 &lt; both 1.0 and 1.5 (iter 65). k_floor response is sub_diff-dependent: sub_diff=6 requires k_floor=1.0 (iter 134).\nOptimal aug is seed-dependent — aug=5500 is optimal for seed=77 (R²=0.87, iter 116), aug=5000 for seed=79 (R²=0.85, iter 96), aug=4500 for seed=42. aug=5750 hurts all configs (iter 118, 126). aug=5500 is the hard ceiling.\nlr_k=0.005 is optimal — lower (0.003, 0.004, 0.0045) too slow, higher (0.007, 0.01) destabilizes (confirmed across 6 iterations)\nlr_sub=0.001 is optimal — 2× increase from 0.0005 broke the R² plateau; lr_sub=0.002 too high (iter 38); lr_sub=0.0015 hurts (iter 56); lr_sub=0.0012 hurts (iter 74)\nlr_node=0.001 is optimal — lr_node=0.005 destabilizes (iter 11); lr_node=0.0005 hurts (iter 63)\nOptimal sub_diff is seed-specific — sub_diff=6 for seed=77 (R²=0.87, iter 116), sub_diff=7 for seed=42 (R²=0.74, iter 45), sub_diff=8 for seed=79 (R²=0.85, iter 96). sub_diff=9 hurts all golden seeds (3× confirmed). sub_diff=5 underperforms (iter 119).\n**Training has extreme intrinsic variance ($\\(0.2--0.3 R²)** — exact replicas: iter 132 R²=0.85 vs iter 133 R²=0.64 (\\)\\(=0.21); iter 106 R²=0.80 vs iter 109 R²=0.55 (\\)$=0.25). Configuration differences below ~0.15 R² are indistinguishable from noise.\nseed=77 is the best seed — R²=0.869 with sub_diff=6+aug=5500 (iter 116, global best). sub_diff=7 more robust (variance ~0.08) but lower ceiling. seed=79 second best (R²=0.851, iter 96) but higher variance. Adjacent seeds (76, 78, 80, 81) are poor — “golden seeds” are rare.\nbatch_size=8 is optimal — batch_size=16 hurts R² (iter 48)\nDefault MLP architecture is optimal — hidden_dim_sub=128 allows degenerate solutions (iter 47); hidden_dim_node=32 significantly worse (iter 71); n_layers_sub=4 hurts (iter 52, 79)\ncoeff_MLP_node_L1=1.0 is optimal — L1=0.0 + long training harmful (iter 22, 29, 40); L1=0.5 hurts (iter 59)\nsub_diff=7 is the most robust setting — variance ~0.08 R² for seed=77 (range 0.73–0.81) vs sub_diff=6’s ~0.21 (range 0.55–0.87). Does not transfer well to seed=42 or seed=79.\nMLP\\(_{\\text{node}}\\) remains flat across all 136 iterations — homeostasis \\(-\\lambda(c - c^{\\text{base}})\\) is never learned, regardless of configuration\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nEvidence\n\n\n\n\nRecurrent training breaks degeneracy\nNo R² improvement, 3.5× slower (iter 13, 34)\n\n\nSmaller MLP improves k recovery\nWorst R² = 0.011 (iter 15)\n\n\nHigher lr_node activates MLP\\(_{\\text{node}}\\)\nlr_node=0.005 hurts (iter 11); 0.002 no effect (iter 26, 39)\n\n\nDifferent seed breaks degeneracy\nSame MLP\\(_{\\text{node}}\\) flatness (iter 27); R² variance $$0.2 (iter 41)\n\n\nStronger monotonicity (sub_diff=10) helps\nR² dropped to 0.41 (iter 32)\n\n\nWeaker monotonicity (sub_diff=3) helps\nR² dropped to 0.61 (iter 43)\n\n\nSmaller batch size helps convergence\nR² dropped (iter 28)\n\n\naug=5000 continues to improve\nConfirmed 3×: iter 25 (0.65), iter 49 (0.70), iter 84 (0.69)\n\n\nlr_sub=0.002 improves over 0.001\nR² dropped from 0.73 to 0.52 (iter 38)\n\n\nsub_norm=2.0 improves R²\nImproves \\(\\alpha\\) but hurts R² (iter 30, 37)\n\n\nCombining lr_sub=0.001 with other changes\nAll combinations hurt R² vs baseline (iter 37–40)\n\n\nWider MLP\\(_{\\text{sub}}\\) (hidden_dim=128) helps\nR² dropped to 0.56 — allows degenerate solutions (iter 47)\n\n\nLarger batch_size=16 stabilizes gradients\nR² dropped to 0.56 (iter 48)\n\n\nsub_diff=8 hurts all seeds\nHurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76)\n\n\nsub_diff=6 is better than 7\nR² dropped from 0.74 to 0.56 (iter 54)\n\n\nDeeper MLP\\(_{\\text{sub}}\\) (n_layers=4) helps\nR² dropped to 0.55 (iter 52); confirmed iter 79 (R²=0.47)\n\n\nlr_sub=0.0015 (intermediate) helps\nR² dropped from 0.74 to 0.60 (iter 56)\n\n\nlr_k=0.004 gives finer convergence\nR² dropped to 0.48 — too slow (iter 46)\n\n\nsub_norm=0.5 helps\nR² dropped from 0.74 to 0.60 (iter 60)\n\n\nIntermediate k_floor=1.25 is optimal\nR²=0.56, worse than both 1.0 and 1.5 — non-monotonic (iter 65)\n\n\naug=4750 within safe range\nR² dropped to 0.66 despite \\(\\alpha\\)=0.96 (iter 67)\n\n\nsub_norm=1.5 better than 1.0\nR²=0.55 vs 0.66 (iter 72)\n\n\nhidden_dim_node=32 simpler is better\nR²=0.47, significantly worse (iter 71)\n\n\nlr_sub=0.0012 helps MLP\\(_{\\text{sub}}\\)\nR²=0.58 vs 0.66 (iter 74)\n\n\nseed=42 is the best seed\nseed=79 achieved R²=0.851 (iter 96), seed=77 R²=0.764 (iter 82)\n\n\nAdjacent seeds share properties\nseed=76 R²=0.51 (iter 86), seed=78 R²=0.39 (iter 83), seed=80 R²=0.57 (iter 91), seed=81 R²=0.55 (iter 95) — all poor\n\n\nk_floor=1.5 is reproducible\nSame config gave R²=0.70 (iter 64) and R²=0.51 (iter 78)\n\n\nsub_diff=8 hurts seed=42\nRefuted: iter 88 got R²=0.72 with seed=42+sub_diff=8 (key: lr_sub=0.001 not 0.0005)\n\n\nsub_diff=9 forces c² quadratic\nHurt all golden seeds: seed=79 0.75→0.64 (iter 90), seed=77 0.76→0.70 (iter 92), 3× confirmed\n\n\naug=5000 always hurts R²\nRefuted for seed=79: R²=0.851 with aug=5000 (iter 96); then aug=5500 helps seed=77 to R²=0.869 (iter 116)\n\n\naug=5500 hurts all seeds\nRefuted: seed=77+sub_diff=6+aug=5500 gave R²=0.869 (iter 116, global best)\n\n\nsub_diff=6 transfers across seeds\nsub_diff=6 doesn’t help seed=79 (R²=0.65, iter 111) or seed=42 (R²=0.59, iter 112)\n\n\nsub_diff=7 transfers to all seeds\nseed=42 R²=0.57 (iter 123), seed=79 R²=0.54 (iter 127)\n\n\nsub_diff=8 works for seed=77\nR²=0.51 (iter 124) vs sub_diff=6 R²=0.87 and sub_diff=7 R²=0.81\n\n\naug=5750 tolerates any sub_diff\nHurts both sub_diff=6 (iter 118, R²=0.52) and sub_diff=7 (iter 126, R²=0.68)\n\n\nk_floor=1.5 reduces variance\nSame config: iter 132 R²=0.85 vs iter 133 R²=0.64 (variance ~0.21 persists)\n\n\nk_floor=1.5 helps sub_diff=6\nR²=0.51 (iter 134) vs k_floor=1.0 R²=0.87 (iter 116)\n\n\nk_floor=2.0 is catastrophically worse\nR²=0.69 (iter 136), within variance range of k_floor=1.5\n\n\n\n\n\n\n\nIs R² \\(\\geq\\) 0.90 achievable? After 136 iterations, the best R²=0.869 (seed=77+sub_diff=6+aug=5500). Training variance of $$0.2–0.3 means a lucky run could reach 0.90+, but systematic improvement requires reducing variance.\nCan variance be reduced? The dominant bottleneck is now stochastic variance ($$0.2–0.3 R²), not hyperparameter tuning. Ensemble averaging, longer training with learning rate scheduling, or gradient accumulation could help.\nAre there more “golden seeds”? Only seeds 77, 79, and 42 are confirmed productive. Adjacent seeds (76, 78, 80, 81) are all poor. What makes a seed “golden” remains unknown.\nWhy is the k_floor response non-monotonic? k_floor=1.25 is worse than both 1.0 and 1.5 (iter 65). The mechanism is unknown.\nIs the remaining 13% error from identifiability issues? Multiple \\(k\\) combinations may produce similar \\(dc/dt\\), setting a fundamental ceiling on single-run recovery."
  },
  {
    "objectID": "results.html#rate-constant-recovery-oscillatory-regime-rank-50",
    "href": "results.html#rate-constant-recovery-oscillatory-regime-rank-50",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the oscillatory regime (activity rank \\(\\sim50\\)): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j \\in [10^{-2}, 10^{-1}]\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n136 iterations completed across 12 blocks (34 batches). The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). 100% autocatalytic 3-cycles.\n\n\n\n\n\n\n\n\n\nThe primary metric is raw R² computed on all 256 reactions (after MLP\\(_{\\text{sub}}\\) scalar correction). The trimmed R² excludes outlier reactions (\\(|\\Delta \\log_{10} k| &gt; 0.3\\)) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 137))\nr2 = [\n    # Block 1 (Iter 1-12): Initial exploration\n    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep\n    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough\n    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation\n    # Block 2 (Iter 13-24): k_floor breakthrough\n    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough\n    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor\n    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best\n    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough\n    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns\n    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches\n    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH\n    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7\n    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub\n    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed\n    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST\n    # Block 5 (Iter 49-60): Confirming optimum\n    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests\n    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds\n    0.701, 0.718, 0.603, 0.603,  # Batch 15: seed=99 promising\n    # Block 6 (Iter 61-72): Variance discovery\n    0.688, 0.430, 0.616, 0.704,  # Batch 16: k_floor=1.5 promising\n    0.565, 0.674, 0.664, 0.609,  # Batch 17: k_floor non-monotonic\n    0.658, 0.639, 0.473, 0.550,  # Batch 18: HIGH VARIANCE discovered\n    # Block 7 (Iter 73-84): seed=77 breakthrough\n    0.722, 0.583, 0.682, 0.636,  # Batch 19: seed=123 improved\n    0.694, 0.515, 0.473, 0.748,  # Batch 20: seed=77 NEW BEST\n    0.661, 0.764, 0.387, 0.689,  # Batch 21: seed=77+sub_diff=8 PEAK\n    # Block 8 (Iter 85-96): seed=79 breakthrough + aug=5000\n    0.619, 0.512, 0.748, 0.720,  # Batch 22: seed=79 golden seed discovered\n    0.822, 0.640, 0.572, 0.704,  # Batch 23: seed=79 replicate R²=0.82\n    0.592, 0.619, 0.546, 0.851,  # Batch 24: seed=79+aug=5000 NEW BEST\n    # Block 9 (Iter 97-108): seed=77+sub_diff=6 discovery\n    0.720, 0.609, 0.742, 0.471,  # Batch 25: aug=5500 hurts seed=79\n    0.754, 0.634, 0.730, 0.764,  # Batch 26: sub_diff=7 for seed=77\n    0.780, 0.804, 0.683, 0.623,  # Batch 27: sub_diff=6 2ND BEST\n    # Block 10 (Iter 109-120): GLOBAL BEST iter 116\n    0.550, 0.572, 0.655, 0.589,  # Batch 28: sub_diff=6 cross-seed (variance!)\n    0.637, 0.643, 0.566, 0.869,  # Batch 29: seed=77+sub_diff=6+aug=5500 GLOBAL BEST\n    0.657, 0.518, 0.581, 0.812,  # Batch 30: aug=5750 hurts, sub_diff=7 good\n    # Block 11 (Iter 121-132): Robustness testing\n    0.734, 0.733, 0.574, 0.509,  # Batch 31: sub_diff=7 robust, sub_diff=8 hurts\n    0.782, 0.680, 0.544, 0.670,  # Batch 32: sub_diff=7 variance ~0.05\n    0.509, 0.675, 0.721, 0.846,  # Batch 33: k_floor=1.5 2ND BEST\n    # Block 12 (Iter 133-136): k_floor=1.5 variance confirmed\n    0.635, 0.512, 0.677, 0.692,  # Batch 34: k_floor=1.5 not reproducible\n]\noutliers = [\n    43, 47, 45, 53,\n    61, 36, 45, 32,\n    27, 38, 57, 30,\n    28, 33, 36, 33,\n    17, 24, 26, 35,\n    16, 29, 19, 24,\n    18, 18, 15, 17,\n    19, 14, 21, 21,\n    21, 21, 15, 22,  # Batch 9\n    16, 19, 20, 21,  # Batch 10\n    21, 16, 18, 19,  # Batch 11\n    15, 20, 23, 21,  # Batch 12\n    12, 21, 12, 25,  # Batch 13\n    21, 21, 18, 19,  # Batch 14\n    17, 17, 16, 20,  # Batch 15\n    17, 20, 17, 16,  # Batch 16\n    17, 14, 16, 14,  # Batch 17\n    19, 14, 22, 17,  # Batch 18\n    19, 16, 22, 20,  # Batch 19\n    16, 16, 32, 12,  # Batch 20\n    16, 15, 23, 18,  # Batch 21\n    16, 22, 15, 20,  # Batch 22\n    13, 13, 15, 17,  # Batch 23\n    22, 14, 21, 11,  # Batch 24\n    18, 20, 15, 16,  # Batch 25\n    17, 16, 16, 13,  # Batch 26\n    16, 12, 14, 12,  # Batch 27\n    14, 15, 18, 17,  # Batch 28\n    15, 17, 17, 10,  # Batch 29\n    18, 20, 15, 10,  # Batch 30\n    12, 10, 17, 14,  # Batch 31\n    10, 10, 15, 14,  # Batch 32\n    18, 10, 12, 12,  # Batch 33\n    18, 21, 11, 20,  # Batch 34\n]\n\n# Color by block\nblock_colors = [\n    '#3498db', '#2ecc71', '#e67e22', '#9b59b6',\n    '#1abc9c', '#e84393', '#f39c12', '#e74c3c',\n    '#2c3e50', '#d35400', '#8e44ad', '#27ae60',\n]\nblock_bounds = [12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 136]\ncolors = []\nfor i in range(len(r2)):\n    for b, bound in enumerate(block_bounds):\n        if i &lt; bound:\n            colors.append(block_colors[b])\n            break\n\nfig, ax1 = plt.subplots(figsize=(18, 5))\n\nbars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)\nax1.axhline(y=0.869, color='#e74c3c', linestyle='--', alpha=0.4, label='Peak R² = 0.869')\nax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')\n\n# Block separators and labels\nfor b in range(len(block_bounds) - 1):\n    ax1.axvline(x=block_bounds[b] + 0.5, color='gray', linestyle=':', alpha=0.4)\nblock_labels = [\n    'Block 1', 'Block 2', 'Block 3', 'Block 4', 'Block 5', 'Block 6',\n    'Block 7', 'Block 8', 'Block 9', 'Block 10', 'Block 11', 'Block 12',\n]\nblock_centers = [6.5, 18.5, 30.5, 42.5, 54.5, 66.5, 78.5, 90.5, 102.5, 114.5, 126.5, 134.5]\nfor i, (lbl, cx) in enumerate(zip(block_labels, block_centers)):\n    ax1.text(cx, 1.05, lbl, ha='center', fontsize=7, color=block_colors[i],\n             bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='none', alpha=0.8))\n\n# Annotate key events\nax1.annotate('k_floor=1.0\\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.25),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(33, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('sub_diff=7', xy=(45, 0.736), xytext=(45, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('seed=77+\\nsub_diff=8', xy=(82, 0.764), xytext=(82, 0.90),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('seed=79+\\naug=5000', xy=(96, 0.851), xytext=(93, 0.95),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('sub_diff=6\\nR²=0.80', xy=(106, 0.804), xytext=(106, 0.90),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('GLOBAL BEST\\nR²=0.87', xy=(116, 0.869), xytext=(116, 0.96),\n            arrowprops=dict(arrowstyle='-&gt;', color='red'), fontsize=7, ha='center', color='red')\nax1.annotate('k_floor=1.5\\nR²=0.85', xy=(132, 0.846), xytext=(132, 0.93),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('rate_constants R² (raw)')\nax1.set_ylim(-0.02, 1.05)\nax1.set_xticks([1, 12, 24, 36, 48, 60, 72, 84, 96, 106, 108, 116, 120, 132, 136])\n\n# outlier count on secondary axis\nax2 = ax1.twinx()\nax2.plot(iters, outliers, 'o', color='black', markersize=3, alpha=0.4, label='outliers')\nax2.set_ylabel('outlier count')\nax2.tick_params(axis='y')\nax2.set_ylim(0, 70)\n\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)\n\nax1.set_title('UCB Exploration: Rate Constant Recovery (136 iterations)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Raw R² (bars) and outlier count (dots) across 136 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74), seed=77+sub_diff=8 (iter 82, 0.76), seed=79+aug=5000 (iter 96, 0.85), seed=77+sub_diff=6 (iter 106, 0.80), seed=77+sub_diff=6+aug=5500 (iter 116, 0.87). Training variance of ±0.2 R² is a dominant factor.\n\n\n\n\n\n\n\n\nAll 12 iterations achieved raw R² &lt; 0.07. The key discovery was that coeff_MLP_sub_norm=1.0 is essential — it corrects the MLP\\(_{\\text{sub}}\\) function shapes (c² becomes quadratic instead of linear) and enables MLP\\(_{\\text{node}}\\) to learn homeostasis.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n1 (iter 1–4)\nlr sweep\n0.044\nAll failed; MLP\\(_{\\text{node}}\\) dead, MLP\\(_{\\text{sub}}\\) c² linear\n\n\n2 (iter 5–8)\nsub_norm=1.0\n0.067\nMLP\\(_{\\text{sub}}\\) normalization is the single most effective change\n\n\n3 (iter 9–12)\ncombine best\n0.061\nMLP\\(_{\\text{node}}\\) activated; lr_node=0.005 hurts\n\n\n\n\n\n\nThe coeff_k_floor=1.0 penalty at iteration 14 produced a 10× improvement in R² (0.06 → 0.51) by preventing outlier \\(\\log k\\) values from drifting below the true minimum. Longer training then pushed R² to 0.69.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n4 (iter 13–16)\nk_floor=1.0\n0.508\nBreakthrough — R² jumped 10×\n\n\n5 (iter 17–20)\naug=3000\n0.642\nLonger training + k_floor synergistic\n\n\n6 (iter 21–24)\naug=4000\n0.690\nFirst to reach 0.69; \\(\\alpha = 0.85\\), 16 outliers\n\n\n\nBest result — Iteration 116: seed=77, coeff_MLP_sub_diff=6, lr_sub=0.001, data_augmentation_loop=5500, coeff_k_floor=1.0, coeff_MLP_sub_norm=1.0\n\n\n\nMetric\nValue\n\n\n\n\nRaw R²\n0.869\n\n\nOutliers\n10 / 256 (3.9%)\n\n\nSlope\n0.98\n\n\n\\(\\alpha\\)\n0.93\n\n\n\nNote: Optimal sub_diff and aug are seed-dependent: sub_diff=6 for seed=77 (R²=0.87), sub_diff=7 for seed=42 (R²=0.74), sub_diff=8 for seed=79 (R²=0.85). seed=77 benefits from aug=5500, seed=79 from aug=5000. Training has intrinsic variance of $$0.2–0.3 R² even with identical configuration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline). R² = 0.044, 43 outliers. Before k_floor and sub_norm — predicted \\(\\log k\\) values scatter widely with no correlation to ground truth.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The k_floor=1.0 penalty prevents outlier \\(\\log k\\) from drifting below the true minimum, producing a 10\\(\\times\\) R² jump.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Iteration 116 (best). R² = 0.869, 10 outliers. seed=77 + sub_diff=6 + aug=5500 yields the tightest clustering around the diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Iteration 1 (baseline). Top-left: ground-truth \\(dc/dt\\); top-right: GNN prediction; bottom-left: residual; bottom-right: scatter. The untrained GNN produces near-zero predictions — no reaction dynamics captured.\n\n\n\n\n\n\n\n\n\nFigure 8: Iteration 116 (best, seed=77, sub_diff=6, aug=5500). Same layout. The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure. The residual reveals the missing homeostatic signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Iteration 1 (baseline). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.42 — under-scaled. MLP\\(_{\\text{node}}\\): flat at zero.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Iteration 116 (best R²). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.93 — close to the true scale. MLP\\(_{\\text{node}}\\): still flat at zero across all 136 iterations — homeostasis not learned.\n\n\n\n\n\n\n\n\n\nTwelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling lr_sub from 0.0005 to 0.001 broke through to R² = 0.726.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n7 (iter 25–28)\naug=5000, seed, batch_size\n0.652\naug=5000 hurt R² but \\(\\alpha=0.95\\) (best ever)\n\n\n8 (iter 29–32)\nL1=0, sub_norm=2.0, lr_k\n0.619\nsub_norm=2.0: fewest outliers (14), best slope (0.99)\n\n\n9 (iter 33–36)\nsub_norm=2.0+aug=3500, recurrent, lr_sub=0.001, aug=3500\n0.726\nlr_sub=0.001 broke the plateau\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstrategies = {\n    'No k_floor\\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],\n    'k_floor\\nsub_diff=5\\nseed=42': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],\n    'sub_diff=7\\nseed=42': [0.736, 0.696, 0.655, 0.662, 0.560, 0.701, 0.718, 0.658, 0.720, 0.574],\n    'seed=77\\nsub_diff=6': [0.804, 0.550, 0.869, 0.657, 0.581, 0.670, 0.512, 0.635],\n    'seed=77\\nsub_diff=7': [0.764, 0.780, 0.643, 0.812, 0.734, 0.733, 0.782, 0.680, 0.509, 0.675, 0.846],\n    'seed=79\\nsub_diff=8': [0.748, 0.822, 0.592, 0.619, 0.851, 0.720, 0.609, 0.634, 0.637],\n    'seed=77\\nsub_diff=8': [0.748, 0.661, 0.764, 0.619, 0.704, 0.742, 0.754, 0.509],\n    'k_floor=1.5': [0.704, 0.674, 0.846, 0.635, 0.512, 0.677, 0.692],\n}\n\nfig, ax = plt.subplots(figsize=(16, 5))\npositions = []\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#d35400', '#e84393', '#9b59b6', '#95a5a6']\nfor i, (label, vals) in enumerate(strategies.items()):\n    x = np.random.normal(i, 0.08, len(vals))\n    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3, color=colors[i], edgecolors='none')\n    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)\n    positions.append(i)\n\nax.set_xticks(positions)\nax.set_xticklabels(list(strategies.keys()), fontsize=8)\nax.set_ylabel('raw R²', fontsize=12)\nax.set_ylim(-0.05, 0.95)\nax.axhline(y=0.869, color='#f39c12', linestyle='--', alpha=0.3, label='Peak R² = 0.869')\nax.grid(axis='y', alpha=0.3)\nax.legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: Impact of key strategies on R². Each dot is one iteration. seed=77+sub_diff=6+aug=5500 achieved the global best R²=0.869 (iter 116). Training has extreme intrinsic variance of ±0.2–0.3 R².\n\n\n\n\n\n\n\n\nBlock 4 first tested combinations with lr_sub=0.001 and revealed seed sensitivity ($$0.2 R²). Then sub_diff=7 (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n10 (iter 37–40)\nsub_norm=2.0, lr_sub=0.002, lr_node=0.002, L1=0\n0.662\nAll combinations with lr_sub=0.001 hurt R² vs baseline\n\n\n11 (iter 41–44)\nseed=123, aug=4500, sub_diff=3, lr_k=0.007\n0.690\naug=4500 most stable (\\(\\alpha = 0.94\\)); seed=123 crashed to R²=0.49\n\n\n12 (iter 45–48)\nsub_diff=7, lr_k=0.004, hidden_dim=128, batch_size=16\n0.736\nsub_diff=7 new best; wider/deeper MLP and larger batch hurt\n\n\n\nIteration 45 (sub_diff=7, aug=4500) achieved R² = 0.736 with \\(\\alpha = 0.90\\) and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46–48 confirmed that lr_k=0.004 is too slow, hidden_dim_sub=128 allows degenerate solutions, and batch_size=16 degrades convergence.\n\n\n\nBlock 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n13 (iter 49–52)\naug=5000, sub_diff=8, seed=123, n_layers=4\n0.696\naug=5000 hurts (confirmed twice); sub_diff=7 improves seed robustness ($$0.08 vs $$0.24)\n\n\n14 (iter 53–56)\naug=4250, sub_diff=6, seed=123+aug=4000, lr_sub=0.0015\n0.701\nAll variations hurt; sub_diff=6 too weak, lr_sub=0.0015 too high\n\n\n15 (iter 57–60)\nseed=123+aug=3500, seed=99, L1=0.5, sub_norm=0.5\n0.718\nseed=99 promising (R²=0.72, \\(\\alpha=0.92\\)); sub_norm=0.5 confirmed essential principle\n\n\n\nTight bounds confirmed: aug=4500 (not 4000, 4250, or 5000), sub_diff=7 (not 5, 6, or 8), lr_sub=0.001 (not 0.0015 or 0.002), lr_k=0.005 (not 0.004 or 0.007). sub_diff=7 improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with sub_diff=5) to 0.08.\n\n\n\nBlock 6 explored new seeds and k_floor variations, then discovered that training has high intrinsic variance — an exact replica of the best configuration (iter 45) achieved only R²=0.66 instead of 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n16 (iter 61–64)\nseed=7, seed=99+aug=4250, lr_node=0.0005, k_floor=1.5\n0.704\nk_floor=1.5 promising (R²=0.70, \\(\\alpha=0.92\\)); seed=99 very sensitive to aug\n\n\n17 (iter 65–68)\nk_floor=1.25, k_floor=1.5+seed=99, aug=4750, sub_diff=6+k_floor=1.5\n0.674\nk_floor response is non-monotonic: 1.25 &lt; 1.0 and 1.5; aug=4750 hurts\n\n\n18 (iter 69–72)\nExact replica of iter 45, lr_k=0.0045, hidden_dim_node=32, sub_norm=1.5\n0.658\nHIGH VARIANCE: replica got R²=0.66 vs original 0.74; MLP\\(_{\\text{sub}}\\) c² failure mode\n\n\n\nThe high variance discovery is significant: identical configurations can produce R² values differing by 0.08, meaning many earlier “improvements” may have been within noise. This makes seed selection and reproducibility central concerns.\n\n\n\nBlock 7 explored new seeds and found that seed=77 is a “golden seed” — achieving R²=0.748 (iter 80) and then R²=0.764 with sub_diff=8 (iter 82), surpassing the previous best.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n19 (iter 73–76)\nseed=123, lr_sub=0.0012, sub_diff=9, aug=4000\n0.722\nseed=123 improved to R²=0.72 (vs 0.66 earlier); all other variations hurt\n\n\n20 (iter 77–80)\nseed=7, k_floor=1.5, n_layers=4, seed=77\n0.748\nseed=77 new best seed (R²=0.748, 12 outliers); n_layers=4 confirmed harmful\n\n\n21 (iter 81–84)\nReplicate seed=77, seed=77+sub_diff=8, seed=78, aug=5000\n0.764\nseed=77+sub_diff=8 = new global best; replica got R²=0.66 (variance); seed=78 poor (R²=0.39)\n\n\n\nThe key finding: optimal sub_diff is seed-dependent. sub_diff=8 hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76). Adjacent seeds (78, 79) do not share seed=77’s properties — “golden seeds” are rare and unpredictable.\n\n\n\nBlock 8 discovered seed=79 as a new golden seed (R²=0.748, iter 87) and then broke the R²=0.85 barrier when combined with aug=5000 (R²=0.851, iter 96) — refuting the established principle that aug&gt;4500 always hurts.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n22 (iter 85–88)\nreplicate, seed=76, seed=79, seed=42+sub_diff=8\n0.748\nseed=79 is a golden seed (R²=0.75); seed=42+sub_diff=8 works with lr_sub=0.001 (R²=0.72, refutes earlier principle)\n\n\n23 (iter 89–92)\nreplicate seed=79, sub_diff=9, seed=80, seed=77+sub_diff=9\n0.822\nseed=79 replicate broke 0.80 barrier (R²=0.82); sub_diff=9 hurts all golden seeds (3× confirmed)\n\n\n24 (iter 93–96)\nreplicate, sub_diff=7, seed=81, seed=79+aug=5000\n0.851\nNEW GLOBAL BEST R²=0.851; aug=5000 helps seed=79 (principle refuted for this seed)\n\n\n\nThe block revealed extreme training variance ($$0.2 R²): the same seed=79+sub_diff=8 config gave R²=0.82 (iter 89) and R²=0.59 (iter 93). The aug=5000 result (iter 96, R²=0.851, 11 outliers, \\(\\alpha\\)=0.94, slope=0.99) was the strongest single run at the time.\n\n\n\nBlock 9 explored aug boundaries for golden seeds and discovered that sub_diff=6 is optimal for seed=77 — achieving R²=0.804 (iter 106), the second-best result at the time.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n25 (iter 97–100)\nreplicate seed=79, aug=5500, seed=42+aug=5000, seed=79+sub_diff=7\n0.742\naug=5500 hurts seed=79 (R²=0.61); sub_diff=7 catastrophic for seed=79 (R²=0.47)\n\n\n26 (iter 101–104)\nseed=77+aug=4500, replicate seed=79, seed=42+aug=5000, seed=77+sub_diff=7\n0.764\nseed=77 consistent R²≈0.75; seed=79 extreme variance (0.85→0.63)\n\n\n27 (iter 105–108)\nreplicate, seed=77+sub_diff=6, seed=42+sub_diff=7, seed=77+sub_diff=9\n0.804\nsub_diff=6 is optimal for seed=77; sub_diff=9 confirmed harmful (3× refuted)\n\n\n\nThe key insight: optimal sub_diff is not just seed-dependent but follows a pattern — lower values (6) work best for seed=77 while higher values (8) suit seed=79. aug=5500 was found to hurt seed=79 but would later prove essential for seed=77.\n\n\n\nBlock 10 tested sub_diff=6 cross-seed transfer and then achieved the global best R²=0.869 at iteration 116 with seed=77+sub_diff=6+aug=5500.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n28 (iter 109–112)\nreplicate iter 106, seed=77+sub_diff=5, seed=79+sub_diff=6, seed=42+sub_diff=6\n0.655\nExtreme variance: replica of R²=0.80 got R²=0.55; sub_diff=6 doesn’t transfer to seed=79/42\n\n\n29 (iter 113–116)\nreplicate seed=79, seed=77+sub_diff=7+aug=5000, seed=79+sub_diff=7, seed=77+sub_diff=6+aug=5500\n0.869\nNEW GLOBAL BEST R²=0.869 (iter 116); aug=5500 helps seed=77 (refutes “aug&gt;4500 hurts”)\n\n\n30 (iter 117–120)\nreplicate iter 116, aug=5750, sub_diff=5, sub_diff=7+aug=5500\n0.812\naug=5750 hurts (R²=0.52); sub_diff=7+aug=5500 also strong (R²=0.81)\n\n\n\nIteration 116 (R²=0.869, 10 outliers, \\(\\alpha\\)=0.93) established the current global best. The aug=5500 finding refuted the principle that longer training always hurts — it is seed-dependent. However, aug=5750 overshoots even for seed=77.\n\n\n\nBlock 11 systematically tested robustness of the top configurations and discovered that k_floor=1.5 combined with sub_diff=7 can achieve R²=0.846 (iter 132, second-best overall).\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n31 (iter 121–124)\nreplicate sub_diff=7, aug=5250, seed=42+sub_diff=7, seed=77+sub_diff=8\n0.734\nsub_diff=7 more robust than sub_diff=6 (variance ~0.08 vs ~0.21); sub_diff=8 hurts seed=77\n\n\n32 (iter 125–128)\nreplicate, aug=5750, seed=79+sub_diff=7, seed=77+sub_diff=6\n0.782\naug=5750 hurts sub_diff=7 too; sub_diff=7 doesn’t transfer to seed=79\n\n\n33 (iter 129–132)\nworst replicate, aug=5250, seed=42+sub_diff=6, seed=77+sub_diff=7+k_floor=1.5\n0.846\n2ND BEST R²=0.846 with k_floor=1.5; extreme variance persists (R²=0.51–0.85)\n\n\n\nThe sub_diff=7 configuration proved more robust than sub_diff=6 for seed=77: R² range 0.73–0.81 (variance ~0.08) vs 0.55–0.87 (variance ~0.21). The trade-off is clear — sub_diff=6 has a higher ceiling but lower floor.\n\n\n\nBlock 12 tested whether k_floor=1.5 reliably improves R² and found that variance dominates configuration differences.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n34 (iter 133–136)\nreplicate iter 132, sub_diff=6+k_floor=1.5, seed=42+k_floor=1.5, k_floor=2.0\n0.692\nReplicate R²=0.64 vs 0.85 (variance ~0.21); sub_diff=6 requires k_floor=1.0; k_floor=2.0 not catastrophic\n\n\n\nThe main conclusion: with $$0.2–0.3 R² training variance, configuration differences below ~0.15 R² are indistinguishable from noise.\n\n\n\n\n\n\n\n\n\nFigure 12: UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the k_floor=1.0 breakthrough (node 14) and converged on the aug=4000 regime. The tree has since grown to 136 nodes; the overall best is node 116 (R² = 0.869).\n\n\n\n\n\n\nThe LLM’s persistent memory has accumulated these validated principles after 136 iterations:\n\ncoeff_MLP_sub_norm=1.0 is essential — enables correct MLP shapes: c² becomes quadratic, MLP\\(_{\\text{node}}\\) activates. sub_norm=0.5 and sub_norm=1.5 both hurt (iter 60, 72).\ncoeff_k_floor=1.0–1.5 is the optimal range — R² jumped from 0.06 to 0.51 (10× improvement) with k_floor=1.0. k_floor=1.5+sub_diff=7 achieved R²=0.85 (iter 132, 2nd best). Response is non-monotonic: k_floor=1.25 &lt; both 1.0 and 1.5 (iter 65). k_floor response is sub_diff-dependent: sub_diff=6 requires k_floor=1.0 (iter 134).\nOptimal aug is seed-dependent — aug=5500 is optimal for seed=77 (R²=0.87, iter 116), aug=5000 for seed=79 (R²=0.85, iter 96), aug=4500 for seed=42. aug=5750 hurts all configs (iter 118, 126). aug=5500 is the hard ceiling.\nlr_k=0.005 is optimal — lower (0.003, 0.004, 0.0045) too slow, higher (0.007, 0.01) destabilizes (confirmed across 6 iterations)\nlr_sub=0.001 is optimal — 2× increase from 0.0005 broke the R² plateau; lr_sub=0.002 too high (iter 38); lr_sub=0.0015 hurts (iter 56); lr_sub=0.0012 hurts (iter 74)\nlr_node=0.001 is optimal — lr_node=0.005 destabilizes (iter 11); lr_node=0.0005 hurts (iter 63)\nOptimal sub_diff is seed-specific — sub_diff=6 for seed=77 (R²=0.87, iter 116), sub_diff=7 for seed=42 (R²=0.74, iter 45), sub_diff=8 for seed=79 (R²=0.85, iter 96). sub_diff=9 hurts all golden seeds (3× confirmed). sub_diff=5 underperforms (iter 119).\n**Training has extreme intrinsic variance ($\\(0.2--0.3 R²)** — exact replicas: iter 132 R²=0.85 vs iter 133 R²=0.64 (\\)\\(=0.21); iter 106 R²=0.80 vs iter 109 R²=0.55 (\\)$=0.25). Configuration differences below ~0.15 R² are indistinguishable from noise.\nseed=77 is the best seed — R²=0.869 with sub_diff=6+aug=5500 (iter 116, global best). sub_diff=7 more robust (variance ~0.08) but lower ceiling. seed=79 second best (R²=0.851, iter 96) but higher variance. Adjacent seeds (76, 78, 80, 81) are poor — “golden seeds” are rare.\nbatch_size=8 is optimal — batch_size=16 hurts R² (iter 48)\nDefault MLP architecture is optimal — hidden_dim_sub=128 allows degenerate solutions (iter 47); hidden_dim_node=32 significantly worse (iter 71); n_layers_sub=4 hurts (iter 52, 79)\ncoeff_MLP_node_L1=1.0 is optimal — L1=0.0 + long training harmful (iter 22, 29, 40); L1=0.5 hurts (iter 59)\nsub_diff=7 is the most robust setting — variance ~0.08 R² for seed=77 (range 0.73–0.81) vs sub_diff=6’s ~0.21 (range 0.55–0.87). Does not transfer well to seed=42 or seed=79.\nMLP\\(_{\\text{node}}\\) remains flat across all 136 iterations — homeostasis \\(-\\lambda(c - c^{\\text{base}})\\) is never learned, regardless of configuration\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nEvidence\n\n\n\n\nRecurrent training breaks degeneracy\nNo R² improvement, 3.5× slower (iter 13, 34)\n\n\nSmaller MLP improves k recovery\nWorst R² = 0.011 (iter 15)\n\n\nHigher lr_node activates MLP\\(_{\\text{node}}\\)\nlr_node=0.005 hurts (iter 11); 0.002 no effect (iter 26, 39)\n\n\nDifferent seed breaks degeneracy\nSame MLP\\(_{\\text{node}}\\) flatness (iter 27); R² variance $$0.2 (iter 41)\n\n\nStronger monotonicity (sub_diff=10) helps\nR² dropped to 0.41 (iter 32)\n\n\nWeaker monotonicity (sub_diff=3) helps\nR² dropped to 0.61 (iter 43)\n\n\nSmaller batch size helps convergence\nR² dropped (iter 28)\n\n\naug=5000 continues to improve\nConfirmed 3×: iter 25 (0.65), iter 49 (0.70), iter 84 (0.69)\n\n\nlr_sub=0.002 improves over 0.001\nR² dropped from 0.73 to 0.52 (iter 38)\n\n\nsub_norm=2.0 improves R²\nImproves \\(\\alpha\\) but hurts R² (iter 30, 37)\n\n\nCombining lr_sub=0.001 with other changes\nAll combinations hurt R² vs baseline (iter 37–40)\n\n\nWider MLP\\(_{\\text{sub}}\\) (hidden_dim=128) helps\nR² dropped to 0.56 — allows degenerate solutions (iter 47)\n\n\nLarger batch_size=16 stabilizes gradients\nR² dropped to 0.56 (iter 48)\n\n\nsub_diff=8 hurts all seeds\nHurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76)\n\n\nsub_diff=6 is better than 7\nR² dropped from 0.74 to 0.56 (iter 54)\n\n\nDeeper MLP\\(_{\\text{sub}}\\) (n_layers=4) helps\nR² dropped to 0.55 (iter 52); confirmed iter 79 (R²=0.47)\n\n\nlr_sub=0.0015 (intermediate) helps\nR² dropped from 0.74 to 0.60 (iter 56)\n\n\nlr_k=0.004 gives finer convergence\nR² dropped to 0.48 — too slow (iter 46)\n\n\nsub_norm=0.5 helps\nR² dropped from 0.74 to 0.60 (iter 60)\n\n\nIntermediate k_floor=1.25 is optimal\nR²=0.56, worse than both 1.0 and 1.5 — non-monotonic (iter 65)\n\n\naug=4750 within safe range\nR² dropped to 0.66 despite \\(\\alpha\\)=0.96 (iter 67)\n\n\nsub_norm=1.5 better than 1.0\nR²=0.55 vs 0.66 (iter 72)\n\n\nhidden_dim_node=32 simpler is better\nR²=0.47, significantly worse (iter 71)\n\n\nlr_sub=0.0012 helps MLP\\(_{\\text{sub}}\\)\nR²=0.58 vs 0.66 (iter 74)\n\n\nseed=42 is the best seed\nseed=79 achieved R²=0.851 (iter 96), seed=77 R²=0.764 (iter 82)\n\n\nAdjacent seeds share properties\nseed=76 R²=0.51 (iter 86), seed=78 R²=0.39 (iter 83), seed=80 R²=0.57 (iter 91), seed=81 R²=0.55 (iter 95) — all poor\n\n\nk_floor=1.5 is reproducible\nSame config gave R²=0.70 (iter 64) and R²=0.51 (iter 78)\n\n\nsub_diff=8 hurts seed=42\nRefuted: iter 88 got R²=0.72 with seed=42+sub_diff=8 (key: lr_sub=0.001 not 0.0005)\n\n\nsub_diff=9 forces c² quadratic\nHurt all golden seeds: seed=79 0.75→0.64 (iter 90), seed=77 0.76→0.70 (iter 92), 3× confirmed\n\n\naug=5000 always hurts R²\nRefuted for seed=79: R²=0.851 with aug=5000 (iter 96); then aug=5500 helps seed=77 to R²=0.869 (iter 116)\n\n\naug=5500 hurts all seeds\nRefuted: seed=77+sub_diff=6+aug=5500 gave R²=0.869 (iter 116, global best)\n\n\nsub_diff=6 transfers across seeds\nsub_diff=6 doesn’t help seed=79 (R²=0.65, iter 111) or seed=42 (R²=0.59, iter 112)\n\n\nsub_diff=7 transfers to all seeds\nseed=42 R²=0.57 (iter 123), seed=79 R²=0.54 (iter 127)\n\n\nsub_diff=8 works for seed=77\nR²=0.51 (iter 124) vs sub_diff=6 R²=0.87 and sub_diff=7 R²=0.81\n\n\naug=5750 tolerates any sub_diff\nHurts both sub_diff=6 (iter 118, R²=0.52) and sub_diff=7 (iter 126, R²=0.68)\n\n\nk_floor=1.5 reduces variance\nSame config: iter 132 R²=0.85 vs iter 133 R²=0.64 (variance ~0.21 persists)\n\n\nk_floor=1.5 helps sub_diff=6\nR²=0.51 (iter 134) vs k_floor=1.0 R²=0.87 (iter 116)\n\n\nk_floor=2.0 is catastrophically worse\nR²=0.69 (iter 136), within variance range of k_floor=1.5\n\n\n\n\n\n\n\nIs R² \\(\\geq\\) 0.90 achievable? After 136 iterations, the best R²=0.869 (seed=77+sub_diff=6+aug=5500). Training variance of $$0.2–0.3 means a lucky run could reach 0.90+, but systematic improvement requires reducing variance.\nCan variance be reduced? The dominant bottleneck is now stochastic variance ($$0.2–0.3 R²), not hyperparameter tuning. Ensemble averaging, longer training with learning rate scheduling, or gradient accumulation could help.\nAre there more “golden seeds”? Only seeds 77, 79, and 42 are confirmed productive. Adjacent seeds (76, 78, 80, 81) are all poor. What makes a seed “golden” remains unknown.\nWhy is the k_floor response non-monotonic? k_floor=1.25 is worse than both 1.0 and 1.5 (iter 65). The mechanism is unknown.\nIs the remaining 13% error from identifiability issues? Multiple \\(k\\) combinations may produce similar \\(dc/dt\\), setting a fundamental ceiling on single-run recovery."
  },
  {
    "objectID": "results.html#why-homeostasis-is-not-learned",
    "href": "results.html#why-homeostasis-is-not-learned",
    "title": "Results",
    "section": "Why Homeostasis Is Not Learned",
    "text": "Why Homeostasis Is Not Learned\nAcross all 136 iterations, MLP\\(_{\\text{node}}\\) remains flat — the homeostasis function \\(-\\lambda_t(c_i - c_i^{\\text{baseline}})\\) is never recovered. This is not a hyperparameter issue. It is a structural limitation of single-step (\\(t \\to t+1\\)) training.\n\nThe scale mismatch problem\nThe GNN predicts \\(dc/dt\\) at each time step:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\prod_k \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{reaction}}\n\\]\nThe reaction term dominates the instantaneous \\(dc/dt\\) — it drives the fast oscillatory dynamics. Homeostasis acts as a slow restoring force that only manifests over many time steps: it prevents concentrations from drifting away from baseline. In a single-step loss \\(\\| \\hat{y}_{t+1} - y_{t+1} \\|^2\\), the gradient signal from homeostasis is negligible compared to the reaction term. The optimizer has no reason to learn it.\n\n\nHomeostasis is an integral problem\nHomeostasis determines the long-term trajectory envelope, not the instantaneous derivative. Its effect accumulates over time:\n\\[\nc_i(t + T) \\approx c_i(t) + \\int_t^{t+T} \\left[ -\\lambda_i (c_i - c_i^{\\text{base}}) + \\text{reaction terms} \\right] d\\tau\n\\]\nA model trained on \\(t \\to t+1\\) can achieve low loss by fitting the dominant reaction signal and ignoring the small homeostatic correction. Over many steps this error accumulates — but the single-step loss never sees it.\n\n\nProposed approach: two-phase training\nTo recover homeostasis, we propose a two-phase training scheme:\n\nPhase 1 — Reaction recovery (\\(t \\to t+1\\)): Train as currently done. Recover \\(k_j\\), MLP\\(_{\\text{sub}}\\), and the stoichiometric structure. Freeze these parameters.\nPhase 2 — Homeostasis recovery (recurrent, \\(t \\to t+T\\)): With the reaction term frozen, train only MLP\\(_{\\text{node}}\\) and embeddings \\(a_i\\) using multi-step rollout. The recurrent loss forces the model to match trajectories over \\(T\\) steps, where the homeostatic drift becomes visible.\n\nThis separates the two learning problems by time scale: fast reactions are learned from instantaneous gradients, slow homeostasis from trajectory matching. The key insight is that recurrent training is not needed for \\(k_j\\) recovery (it was tried and failed — iter 13, 34), but may be essential specifically for homeostasis once the reaction parameters are frozen.\n\n\nAlternative approach: residual-based supervision\nThe kinograph residual (Figure 8, bottom-left) directly reveals what the single-step model cannot learn. After Phase 1, we can roll out the learned model autoregressively:\n\\[\n\\hat{c}_i(t+1) = \\hat{c}_i(t) + \\Delta t \\cdot \\left[ \\sum_j S_{ij} \\cdot k_j \\prod_k \\text{MLP}_{\\text{sub}}(\\hat{c}_k, |S_{kj}|) \\right]\n\\]\nThe rollout trajectory \\(\\hat{c}(t)\\) drifts from the observation \\(c(t)\\) because the missing homeostatic restoring force is not applied at each step. The accumulated residual\n\\[\nr_i(t) = c_i(t) - \\hat{c}_i(t)\n\\]\nis precisely the integrated effect of the missing slow terms — homeostasis, external sources, and degradation. This residual provides a direct supervision signal for Phase 2 without requiring backpropagation through time: we can compute a per-step target\n\\[\n\\frac{r_i(t+1) - r_i(t)}{\\Delta t} \\approx \\text{MLP}_{\\text{node}}(c_i(t), a_i)\n\\]\nand train MLP\\(_{\\text{node}}\\) with a standard single-step loss on this derived target. This avoids the instabilities of recurrent training while still capturing the long-term dynamics that the reaction-only model structurally cannot predict.\n\n\nDirect evidence: the one-step derivative residual is the homeostasis signal\nThe one-step derivative diagnostic feeds the true concentration at each timestep and records the GNN’s predicted \\(dc/dt\\). Since MLP\\(_{\\text{node}} \\approx 0\\) across all 136 iterations, the prediction is purely the reaction term:\n\\[\n\\widehat{\\frac{dc_i}{dt}} = \\underbrace{0}_{\\text{MLP}_{\\text{node}}} + \\sum_j S_{ij} \\cdot k_j \\prod_k \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\n\\]\nThe residual between prediction and ground truth is therefore:\n\\[\n\\widehat{\\frac{dc_i}{dt}} - \\frac{dc_i}{dt} = -\\text{MLP}_{\\text{node}}^{\\text{true}}(c_i, a_i) = \\lambda_i(c_i - c_i^{\\text{base}})\n\\]\nThis is the negative of the unlearned homeostasis term, directly observable per metabolite over time.\n\n\n\n\n\n\nFigure 13: One-step derivative residual traces (Iteration 96, best model). Each trace shows \\(\\widehat{dc/dt} - dc/dt\\) for a single metabolite — the signal that single-step training structurally cannot capture. The smooth, slow-varying shape is consistent with homeostatic regulation \\(\\lambda(c - c^{\\text{base}})\\). Metabolites with the highest MAE (36, 99, 10) correspond to the strongest homeostatic restoring forces, not GNN prediction errors.\n\n\n\nThe derivative residual traces confirm three properties expected of the homeostasis signal:\n\nSmooth and slow-varying — the traces lack the fast oscillatory structure of the reaction term, consistent with a restoring force that operates on longer timescales.\nNon-zero mean per metabolite — each metabolite has a persistent bias, reflecting the time-averaged homeostatic drive \\(\\lambda_i(c_i - c_i^{\\text{base}})\\).\nMetabolite-specific amplitude — the MAE varies by metabolite (0.02 to 0.14), suggesting heterogeneous homeostatic strength \\(\\lambda_i\\) across the network.\n\nThese residual traces provide a ready-made supervision signal for Phase 2: rather than requiring backpropagation through time, MLP\\(_{\\text{node}}\\) can be trained directly on \\(-(\\widehat{dc/dt} - dc/dt)\\) at each timestep, using the frozen Phase 1 reaction model.\nThis approach is an instance of the Universal Differential Equations framework (Rackauckas et al., 2021), where a partially known ODE is augmented with a neural network that learns the missing terms from data. Here, the known part is the reaction dynamics \\(\\sum_j S_{ij} k_j \\prod_k \\text{MLP}_{\\text{sub}}\\), and the unknown part is the homeostatic correction learned by MLP\\(_{\\text{node}}\\). The residual-based supervision strategy is also related to PDE-Refiner (Lippe et al., NeurIPS 2023), which uses iterative refinement on rollout residuals to recover low-amplitude dynamics that single-pass neural solvers miss — analogous to the small homeostatic signal masked by dominant reactions.\n\n\nRelated work\nThe difficulty of learning slow dynamics from single-step training is well established in the literature:\n\nTeacher forcing and long-term dependencies. Williams & Zipser (1989) introduced teacher forcing for RNNs: feeding ground-truth inputs at each step. Bengio et al. (1994) showed that gradient-based learning of long-term dependencies is fundamentally difficult because short-term gradient contributions dominate, exactly the mechanism that prevents our GNN from learning homeostasis.\nScheduled sampling. Bengio et al. (2015) proposed a curriculum strategy that gradually transitions from teacher forcing (single-step) to free-running (multi-step) prediction, reducing the train–inference discrepancy. Our two-phase proposal is conceptually related: Phase 1 is teacher-forced, Phase 2 uses multi-step rollout.\nMultiple shooting for Neural ODEs. Massaroli et al. (2021) introduced differentiable multiple shooting layers that parallelize trajectory integration. Turan & Jäschke (2021) showed that standard Neural ODE fitting on oscillatory data produces “flattened” trajectories — multiple shooting recovers the true dynamics by breaking long horizons into segments. This directly addresses the failure mode we observe.\nStiff Neural ODEs. Kim et al. (2021) showed that learning neural ODEs for systems with widely separated time scales (stiff systems) requires proper output scaling and stabilized gradients — the fast dynamics otherwise dominate training, exactly as in our reaction-vs-homeostasis scale mismatch.\nMulti-scale separation in dynamical systems. Fenichel (1979) established geometric singular perturbation theory: for systems with fast and slow time scales, the fast subsystem is solved first, then the dynamics are reduced to a slow manifold. Our two-phase training mirrors this decomposition — Phase 1 recovers the fast reaction dynamics, Phase 2 learns the slow homeostatic manifold.\nLatent timescales in Neural ODEs. Gupta et al. (2024) showed that training trajectory length directly controls the timescales a Neural ODE can recover: longer trajectories are needed to capture slower dynamics. This supports the need for multi-step rollout in Phase 2.\n\nThe scale mismatch / integral argument is essentially the same observation that Bengio (1994) made for RNNs and Kim et al. (2021) made for stiff ODEs. The two-phase proposal mirrors Fenichel’s fast-then-slow decomposition. The novelty here is applying this reasoning to GNN-based metabolic network recovery, where the bipartite graph structure and the identifiability of rate constants \\(k_j\\) add domain-specific constraints."
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#metabolic-networks-as-bipartite-graphs",
    "href": "model.html#metabolic-networks-as-bipartite-graphs",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#the-stoichiometric-matrix",
    "href": "model.html#the-stoichiometric-matrix",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Stoichiometric Matrix",
    "text": "The Stoichiometric Matrix\nThe stoichiometric matrix \\(\\mathbf{S}\\) is an \\((n_{\\text{metabolites}} \\times n_{\\text{reactions}})\\) matrix where entry \\(S_{ij}\\) indicates how metabolite \\(i\\) participates in reaction \\(j\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix}\n-1 & 0 & \\cdots \\\\\n-1 & +1 & \\cdots \\\\\n+2 & -1 & \\cdots \\\\\n+1 & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\]\nProperties of S:\n\nSparse: most entries are zero (each reaction involves only 2-6 metabolites)\nInteger-valued: entries are typically in \\(\\{-2, -1, 0, +1, +2\\}\\)\nMass conservation: column sums should be zero for balanced reactions\n\nExample:\nConsider the diagram above with 2 reactions:\n\\[\n\\begin{aligned}\n\\text{R1}: \\quad & \\text{Glucose} + \\text{ATP} \\longrightarrow 2\\,\\text{Pyruvate} + \\text{ADP} \\\\\n\\text{R2}: \\quad & \\text{Pyruvate} \\longrightarrow \\text{ATP}\n\\end{aligned}\n\\]\nThe corresponding stoichiometric matrix is:\n\\[\n\\begin{array}{c|cc}\n& \\text{R1} & \\text{R2} \\\\\n\\hline\n\\text{Glucose} & -1 & 0 \\\\\n\\text{ATP} & -1 & +1 \\\\\n\\text{Pyruvate} & +2 & -1 \\\\\n\\text{ADP} & +1 & 0\n\\end{array}\n\\]"
  },
  {
    "objectID": "model.html#reaction-kinetics",
    "href": "model.html#reaction-kinetics",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Reaction Kinetics",
    "text": "Reaction Kinetics\n\nMass-Action Law\nThe rate of a chemical reaction is proportional to the product of the concentrations of its substrates, each raised to the power of its stoichiometric coefficient. For a reaction \\(j\\) with substrates \\(k\\):\n\\[v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nThis is the law of mass action (Guldberg & Waage, 1864): a reaction proceeds faster when its substrates are more abundant. The exponent \\(|S_{kj}|\\) reflects how many molecules of substrate \\(k\\) are consumed — a reaction requiring 2 molecules of ATP depends quadratically on ATP concentration (\\(c_{\\text{ATP}}^2\\)), while a reaction consuming 1 molecule depends linearly (\\(c_{\\text{ATP}}^1\\)).\n\n\nSubstrate Aggregation\nEach reaction consumes multiple substrates. The aggregation rule determines how substrate contributions combine to form the reaction rate:\n\n\n\n\n\n\n\n\nAggregation\nFormula\nPhysical meaning\n\n\n\n\nMultiplicative (product)\n\\(v_j = k_j \\cdot \\prod_k c_k^{\\|S_{kj}\\|}\\)\nTrue mass-action: all substrates must be present simultaneously. Rate drops to zero if any substrate is absent.\n\n\nAdditive (sum)\n\\(v_j = k_j \\cdot \\sum_k c_k^{\\|S_{kj}\\|}\\)\nApproximate: substrates contribute independently. Useful when reactions are not strictly mass-action (e.g., enzyme-mediated).\n\n\n\nMultiplicative aggregation is the physically correct form for mass-action kinetics — it encodes the requirement that a reaction can only proceed if all its substrates are available. It also produces richer dynamics: multiplicative coupling between metabolites creates nonlinear feedback loops that can sustain oscillations.\nAdditive aggregation is a simplification where each substrate contributes independently to the rate. It cannot produce sustained oscillations from autocatalytic cycles because it lacks the multiplicative coupling needed for positive feedback."
  },
  {
    "objectID": "model.html#model-evolution",
    "href": "model.html#model-evolution",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Model Evolution",
    "text": "Model Evolution\n\n1. Pure Reaction\nPure reaction dynamics without homeostasis, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nPure reaction dynamics: concentrations evolve under additive mass-action kinetics without homeostasis. Most metabolites equilibrate within ~100 time steps.\n\n\n\n\n2. Homeostasis\nWith homeostatic regulation pulling concentrations toward baseline, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}}_{\\text{reactions}}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.01\n\n\n\\(c^{\\text{baseline}}\\)\n5.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nHomeostatic regulation: concentrations are pulled toward a baseline (\\(c^{\\text{baseline}} = 5.0\\)) by a linear restoring force. Dynamics converge to steady state.\n\n\n\n\n3. Oscillatory\nAutocatalytic cycles with mass-action kinetics (multiplicative aggregation) for sustained oscillations:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nWith multiplicative aggregation, autocatalytic cycles create positive feedback: in \\(A + B \\to 2B\\), the rate \\(v = k \\cdot c_A \\cdot c_B\\) increases with both \\(A\\) and \\(B\\), so producing more \\(B\\) accelerates the reaction — a nonlinear feedback loop that sustains oscillations when cycles are closed (\\(A \\to B \\to C \\to A\\)).\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nAggregation\nProduct (multiplicative)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.5}, 10^{-1}]\\)\n\n\nInitial concentrations\n[1.0, 9.0]\n\n\nFlux limiting\nDisabled\n\n\n\n\n\n\nOscillatory dynamics (activity rank 20): autocatalytic 3-cycles with multiplicative aggregation produce sustained oscillations. The wide rate constant range \\([10^{-2.5}, 10^{-1}]\\) means ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly."
  },
  {
    "objectID": "model.html#activity-rank",
    "href": "model.html#activity-rank",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Activity Rank",
    "text": "Activity Rank\nTo quantify the complexity of concentration dynamics, we compute the activity rank using singular value decomposition (SVD) of the concentration matrix \\(\\mathbf{C} \\in \\mathbb{R}^{T \\times n}\\) (time frames × metabolites).\nThe rank at 99% variance is the number of singular values needed to capture 99% of the total variance:\n\\[\\text{rank}_{99} = \\min \\left\\{ k : \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.99 \\right\\}\\]\nInterpretation:\n\nLow rank (1-5): concentrations are highly correlated, dynamics are simple (equilibration or uniform decay)\nHigh rank (&gt;20): metabolites evolve independently with rich, complex dynamics\n\n\nIncreasing Activity Rank\nWith the baseline oscillatory config (\\(k_j \\in [10^{-2.5}, 10^{-1}]\\), 256 reactions), the activity rank is 20. Activity rank is controlled by the number of actively contributing reactions per metabolite:\n\n\n\nConfig\nReactions\n\\(k_j\\) range\nActivity Rank\n\n\n\n\nBaseline (rank_20)\n256\n\\([10^{-2.5}, 10^{-1}]\\)\n20\n\n\nNarrow \\(k\\) (rank_50)\n256\n\\([10^{-2.0}, 10^{-1}]\\)\n47\n\n\nMore reactions (rank_70)\n512\n\\([10^{-2.0}, 10^{-1}]\\)\n70\n\n\n\nThe most impactful change is narrowing the rate constant range from \\([10^{-2.5}, 10^{-1}]\\) to \\([10^{-2.0}, 10^{-1}]\\). With the wider range, ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly. Removing these inert reactions more than doubles the activity rank (20 → 47). Doubling the number of reactions further increases overlap per metabolite (47 → 70).\n\n\nGNN Training Dataset\nFor GNN training and rate constant recovery, we use the rank_50 config (256 reactions, activity rank 47) — complex enough to test the inverse problem while keeping it tractable:\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.0}, 10^{-1}]\\)\n\n\nAll other parameters\nSame as oscillatory above\n\n\n\n\n\n\nOscillatory dynamics with narrowed rate constants (activity rank 47). Eliminating slow inert reactions produces richer dynamics. This is the primary dataset for GNN training."
  },
  {
    "objectID": "model.html#summary-the-full-model",
    "href": "model.html#summary-the-full-model",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Summary: The Full Model",
    "text": "Summary: The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate follows mass-action kinetics:\n\\[\nv_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\n\\]"
  },
  {
    "objectID": "model.html#the-inverse-problem",
    "href": "model.html#the-inverse-problem",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nGiven observed concentration time series \\(\\mathbf{C}(t)\\) and the bipartite graph structure, the goal is to recover the model components that generated the dynamics. A graph neural network (GNN) operates on the bipartite metabolite–reaction graph and learns two functions and a set of scalar parameters:\n\nWhat the GNN Learns\nThe forward model is:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\prod_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nThe GNN replaces the known functions with learnable MLPs, and the rate constants with learnable parameters. It must recover all three simultaneously from concentration data alone:\n1. Substrate function \\(f_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\)\nA neural network that learns how each substrate concentration contributes to the reaction rate. The ground-truth function is the mass-action power law \\(c_k^{|S_{kj}|}\\), but the GNN does not know this — it must discover the functional form from data:\n\\[\n\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|) \\;\\overset{?}{\\approx}\\; c_k^{|S_{kj}|}\n\\]\nThis is a function discovery problem: the MLP receives concentration and stoichiometric coefficient as inputs and must learn to output the correct power-law relationship. Since the stoichiometric coefficients are typically 1 or 2, the MLP must learn to distinguish between linear (\\(c^1\\)) and quadratic (\\(c^2\\)) dependencies.\n2. Homeostasis function \\(f_{\\text{node}} \\to \\text{MLP}_{\\text{node}}(c_i)\\)\nA neural network that learns the per-metabolite self-regulation term. The ground truth is a linear function \\(-\\lambda_{\\text{type}(i)} (c_i - c_i^{\\text{baseline}})\\) with small magnitude, but the GNN must discover this from data:\n\\[\n\\text{MLP}_{\\text{node}}(c_i) \\;\\overset{?}{\\approx}\\; -\\lambda_{\\text{type}(i)} \\cdot (c_i - c_i^{\\text{baseline}})\n\\]\nThis function captures how each metabolite is regulated independently of reactions — pulling concentrations back toward a baseline level. The challenge is that homeostatic forces are small compared to reaction rates, so \\(\\text{MLP}_{\\text{node}}\\) must learn a subtle signal without absorbing information that belongs to the reaction terms.\n3. Rate constants \\(k_j\\) (256 learnable scalars)\nPer-reaction rate constants learned in log-space. Unlike the MLPs, these are not functions but a vector of 256 scalar parameters — one per reaction — that scale the reaction fluxes.\n\n\nIdentifiability Challenges\nThe three components interact and can compensate for each other:\n\nScale ambiguity: \\(k_j \\cdot \\text{MLP}_{\\text{sub}}\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). Without anchoring, the MLP can absorb a global scale factor and shift all \\(k\\) values. Regularization (\\(\\texttt{coeff\\_k\\_center}\\)) anchors \\(\\text{mean}(\\log k)\\) to the known range center.\nFunction compensation: If \\(\\text{MLP}_{\\text{sub}}\\) learns a wrong functional form (e.g., \\(c^{1.5}\\) instead of \\(c^2\\)), the rate constants can partially compensate by adjusting their values. This leads to degenerate solutions with high prediction accuracy but poor parameter recovery.\nHomeostasis absorption: \\(\\text{MLP}_{\\text{node}}\\) can grow large and absorb dynamics that should be explained by the reaction terms, masking the true rate constants.\n\n\n\nLearning Modes\n\n\n\n\n\n\n\n\n\nMode\nS matrix\nPrimary metric\nChallenge\n\n\n\n\nS learning\nLearnable\nstoichiometry \\(R^2\\)\nRecovering integer coefficients and sparsity\n\n\nS given\nFrozen from GT\nrate constants \\(R^2\\)\nDisentangling \\(k\\), \\(\\text{MLP}_{\\text{sub}}\\), \\(\\text{MLP}_{\\text{node}}\\)"
  },
  {
    "objectID": "gnn-llm-memory.html",
    "href": "gnn-llm-memory.html",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#the-exploration-loop",
    "href": "gnn-llm-memory.html#the-exploration-loop",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#training-scheme",
    "href": "gnn-llm-memory.html#training-scheme",
    "title": "GNN-LLM-Memory",
    "section": "Training Scheme",
    "text": "Training Scheme\nThe GNN is trained by minimizing the prediction error on \\(dc/dt\\):\n\\[\n\\mathcal{L} = \\sum_{\\text{frames}} \\left\\| \\frac{dc}{dt}_{\\text{pred}} - \\frac{dc}{dt}_{\\text{GT}} \\right\\|_2 + \\mathcal{R}\n\\]\nwhere \\(\\mathcal{R}\\) is the sum of regularization terms described below.\n\nSeparate Learning Rates\nEach model component has its own learning rate to control the balance between parameter groups:\n\n\n\n\n\n\n\n\n\nComponent\nConfig key\nControls\nTypical range\n\n\n\n\nRate constants \\(k_j\\)\nlearning_rate_k\nHow fast k values are updated\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{node}}\\) (homeostasis)\nlearning_rate_node\nHomeostasis function learning speed\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{sub}}\\) (substrate)\nlearning_rate_sub\nSubstrate function learning speed\n1E-4 to 1E-2\n\n\n\nThe learning rate balance is critical:\n\nlr_k too high: \\(k\\) values overshoot, oscillate, or converge to wrong values\nlr_k too low: \\(k\\) barely moves, MLPs compensate\nlr_node/lr_sub imbalance: one function absorbs capacity meant for the other\n\n\n\nRegularization Terms\nThe total regularization \\(\\mathcal{R}\\) is the sum of the following penalties:\n\nMLP\\(_{\\text{sub}}\\) Monotonicity (coeff_MLP_sub_diff)\nMLP\\(_{\\text{sub}}\\) learns \\(c^s\\) which should be monotonically increasing in concentration. This penalty samples concentration pairs \\((c, c+\\delta)\\) and penalizes cases where the output decreases:\n\\[\n\\mathcal{R}_{\\text{sub\\_diff}} = \\left\\| \\text{ReLU}\\left(\\|\\text{MLP}_{\\text{sub}}(c)\\| - \\|\\text{MLP}_{\\text{sub}}(c+\\delta)\\|\\right) \\right\\|_2 \\cdot \\lambda_{\\text{sub\\_diff}}\n\\]\nWithout this constraint, MLP\\(_{\\text{sub}}\\) can develop non-physical local minima that don’t match the true power law behavior.\n\n\nMLP\\(_{\\text{node}}\\) L1 (coeff_MLP_node_L1)\nPenalizes large MLP\\(_{\\text{node}}\\) output to keep homeostasis values small relative to reaction terms:\n\\[\n\\mathcal{R}_{\\text{node\\_L1}} = \\text{mean}\\left(|\\text{MLP}_{\\text{node}}(c_i, a_i)|\\right) \\cdot \\lambda_{\\text{node\\_L1}}\n\\]\nMLP\\(_{\\text{node}}\\) is initialized to zero output so homeostasis starts inactive. Combined with a reduced learning rate, this prevents MLP\\(_{\\text{node}}\\) from dominating the dynamics. The true homeostatic \\(\\lambda\\) values are small (0.001–0.002), so MLP\\(_{\\text{node}}\\) output should remain small.\n\n\nMLP\\(_{\\text{sub}}\\) Normalization (coeff_MLP_sub_norm)\nBreaks the scale ambiguity between \\(k\\) and MLP\\(_{\\text{sub}}\\) at the source. The product \\(k_j \\cdot \\text{MLP}_{\\text{sub}}(c)\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). This penalty enforces that MLP\\(_{\\text{sub}}\\) outputs 1 at the reference point \\(c=1, |s|=1\\), where the true value \\(c^s = 1^1 = 1\\):\n\\[\n\\mathcal{R}_{\\text{sub\\_norm}} = \\left(\\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\| - 1\\right)^2 \\cdot \\lambda_{\\text{sub\\_norm}}\n\\]\nSince \\(c^s = 1\\) at \\(c=1\\) for any stoichiometry \\(s\\), this pins the MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) to 1 and prevents \\(k\\) from absorbing a global factor.\n\n\nRate Constant Floor (coeff_k_floor)\nPrevents \\(\\log_{10} k_j\\) from drifting far below the physically plausible range. Without this, some reactions develop outlier values (e.g. \\(\\log k = -4\\) when the true range is \\([-2, -1]\\)), which distorts the \\(R^2\\) even when most reactions are well-recovered:\n\\[\n\\mathcal{R}_{\\text{k\\_floor}} = \\sum_j \\text{ReLU}\\left(\\tau - \\log_{10} k_j\\right)^2 \\cdot \\lambda_{\\text{k\\_floor}}\n\\]\nwhere \\(\\tau\\) is the configurable threshold (k_floor_threshold, default \\(-3\\)). Only \\(\\log k\\) values below \\(\\tau\\) are penalized.\n\n\n\nScalar Correction\nEven without the normalization regularization, a post-hoc scalar correction is applied when evaluating rate constants. The MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) is measured by evaluating MLP\\(_{\\text{sub}}\\) at the reference point:\n\\[\n\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\n\\]\nWith multiplicative aggregation, each reaction rate absorbs \\(\\alpha^{n_j}\\) where \\(n_j\\) is the number of substrates. The corrected rate constants are:\n\\[\n\\log_{10} k_j^{\\text{corrected}} = \\log_{10} k_j^{\\text{learned}} + n_j \\cdot \\log_{10} \\alpha\n\\]\nThe reported rate_constants_R2 is computed on these corrected values against the identity line \\(y=x\\).\n\n\nSummary of Hyperparameters\n\n\n\n\n\n\n\n\n\nParameter\nConfig key\nDescription\nTypical range\n\n\n\n\nLearning rate \\(k\\)\nlearning_rate_k\nRate constants update speed\n1E-4 to 1E-2\n\n\nLearning rate node\nlearning_rate_node\nMLP\\(_{\\text{node}}\\) update speed\n1E-4 to 1E-2\n\n\nLearning rate sub\nlearning_rate_sub\nMLP\\(_{\\text{sub}}\\) update speed\n1E-4 to 1E-2\n\n\nBatch size\nbatch_size\nTime frames per gradient step\n4 to 32\n\n\nTraining iterations\ndata_augmentation_loop\nMultiplier for iterations per epoch\n100 to 5000\n\n\nEpochs\nn_epochs\nNumber of training epochs\n1 to 5\n\n\nTime step\ntime_step\nSteps per gradient update (1 = single-step, &gt;1 = recurrent rollout)\n1 to 8\n\n\nRecurrent training\nrecurrent_training\nEnable multi-step rollout training\ntrue / false\n\n\nMLP\\(_{\\text{sub}}\\) monotonicity\ncoeff_MLP_sub_diff\nPenalize non-increasing MLP\\(_{\\text{sub}}\\)\n0 to 500\n\n\nMLP\\(_{\\text{node}}\\) L1\ncoeff_MLP_node_L1\nPenalize large homeostasis output\n0 to 10\n\n\nMLP\\(_{\\text{sub}}\\) normalization\ncoeff_MLP_sub_norm\nPin MLP\\(_{\\text{sub}}(c{=}1, |s|{=}1)\\) to 1\n0 to 10\n\n\nRate constant floor\ncoeff_k_floor\nPenalize \\(\\log k\\) below threshold\n0 to 10\n\n\nFloor threshold\nk_floor_threshold\nThreshold for k floor penalty\n\\(-3\\) (default)\n\n\nMLP\\(_{\\text{sub}}\\) hidden dim\nhidden_dim_sub\nHidden layer width for substrate MLP\n16 to 128\n\n\nMLP\\(_{\\text{sub}}\\) layers\nn_layers_sub\nNumber of layers for substrate MLP\n2 to 5\n\n\nMLP\\(_{\\text{node}}\\) hidden dim\nhidden_dim_node\nHidden layer width for homeostasis MLP\n16 to 128\n\n\nMLP\\(_{\\text{node}}\\) layers\nn_layers_node\nNumber of layers for homeostasis MLP\n2 to 5"
  },
  {
    "objectID": "gnn-llm-memory.html#metrics",
    "href": "gnn-llm-memory.html#metrics",
    "title": "GNN-LLM-Memory",
    "section": "Metrics",
    "text": "Metrics\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood value\n\n\n\n\nrate_constants_R2\nR² between learned and true rate constants \\(k\\) (after scalar correction, excluding outliers)\n&gt; 0.9\n\n\nn_outliers\nNumber of reactions with \\(|\\Delta \\log_{10} k| &gt; 0.3\\) (factor of 2 error in \\(k\\)-space)\n&lt; 25\n\n\nslope\nSlope of linear fit between learned and true \\(\\log_{10} k\\). Slope \\(&lt; 1\\) means the learned range is compressed.\n\\(\\approx 1.0\\)\n\n\ntest_R2\nR² on held-out test frames (rollout prediction)\n&gt; 0.9\n\n\ntest_pearson\nPearson correlation on test frames\n&gt; 0.95\n\n\nfinal_loss\nFinal prediction loss (MSE on \\(dc/dt\\))\nLower is better\n\n\nalpha\nMLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\\). Ideal value is 1.0 — indicates MLP\\(_{\\text{sub}}\\) has learned the correct scale. Deviations from 1 indicate residual scale ambiguity that the scalar correction must compensate for.\n\\(\\approx 1.0\\)\n\n\n\n\nDiagnostic Interpretation\nThe relationship between rate_constants_R2 and test_pearson diagnoses whether the model found the true rate constants or a degenerate solution:\n\n\n\n\n\n\n\n\ntest_pearson\n\\(R^2\\)\nDiagnosis\n\n\n\n\n&gt; 0.95\n&gt; 0.9\nHealthy — good dynamics from correct \\(k\\)\n\n\n&gt; 0.95\n0.3–0.9\nDegenerate — good dynamics from wrong \\(k\\) (MLPs compensate)\n\n\n&gt; 0.95\n&lt; 0.3\nSeverely degenerate — MLPs absorb all dynamics\n\n\n&lt; 0.5\n&gt; 0.9\nGood \\(k\\), poor rollout — rate constants correct but MLP errors compound during integration\n\n\n&lt; 0.5\n&lt; 0.5\nFailed — both dynamics and \\(k\\) poor\n\n\n\nThe current oscillatory regime experiments show the good \\(k\\), poor rollout pattern: \\(R^2 \\approx 0.93\\) but Pearson \\(\\approx 0.05\\). The MLP\\(_{\\text{sub}}\\) scale compression (\\(\\alpha \\approx 0.4\\)) and MLP\\(_{\\text{node}}\\) failure (flat at zero) produce errors that accumulate during multi-step rollout, even though the rate constants themselves are well-recovered."
  },
  {
    "objectID": "gnn-llm-memory.html#ucb-tree-search",
    "href": "gnn-llm-memory.html#ucb-tree-search",
    "title": "GNN-LLM-Memory",
    "section": "UCB Tree Search",
    "text": "UCB Tree Search\nThe LLM selects parent configurations to mutate using an Upper Confidence Bound (UCB) strategy that balances exploitation of high-performing branches with exploration of under-visited regions:\n\\[\n\\text{UCB}(i) = \\bar{X}_i + c \\cdot \\sqrt{\\frac{\\ln N}{n_i}}\n\\]\nwhere \\(\\bar{X}_i\\) is the mean reward of node \\(i\\), \\(N\\) is the total number of visits, \\(n_i\\) is the number of visits to node \\(i\\), and \\(c\\) is the exploration constant.\n4 parallel slots run per batch with diversified roles:\n\n\n\n\n\n\n\n\nSlot\nRole\nDescription\n\n\n\n\n0\nexploit\nHighest UCB node, conservative mutation\n\n\n1\nexploit\n2nd highest UCB, or same parent different param\n\n\n2\nexplore\nUnder-visited node, or new parameter dimension\n\n\n3\nprinciple-test\nTest or challenge one established principle from memory"
  },
  {
    "objectID": "application.html",
    "href": "application.html",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#motivation",
    "href": "application.html#motivation",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "href": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Chronic Jugular Microdialysis in Freely Moving Mice",
    "text": "Chronic Jugular Microdialysis in Freely Moving Mice\nNardin et al. developed a workflow combining chronic jugular microdialysis with chemical isotope labeling LC-MS to continuously measure bloodborne metabolites in freely moving mice:\n\n\n\nProperty\nValue\n\n\n\n\nCompounds measured\n~123 high-quality amines and small peptides\n\n\nSampling cadence\n7.5 minutes\n\n\nRecording duration\n~8 hours per session\n\n\nTime frames\n~64 per session\n\n\nAnimals\n3 mice, implants patent for &gt;7 days\n\n\n\nThe data captures real metabolic dynamics: purine turnover correlating with movement, delayed histamine/5-HIAA changes, coordinated amino-acid dynamics, and state-dependent metabolic shifts between locomotion and rest.\n\nLow-Rank Physiological State Space\nA key finding: 10 principal components explain 73% of the variance across all measured compounds. The first component alone (rPC1) captures 35.5% and is strongly correlated with locomotion (Spearman \\(r = -0.59\\), peaking at 7.5 min lag).\nThis low-rank structure is consistent with what we observe in our simulations:\n\n\n\n\n\n\n\n\n\nNardin et al.\nMetabolismGraph simulations\n\n\n\n\nn_metabolites\n~123\n100\n\n\nRank at 70% variance\n~10\n~5–15 (estimated)\n\n\nRank at 99% variance\n—\n24–50\n\n\nDominant mode\nLocomotion-driven amino acid metabolism\nAutocatalytic cycle oscillations\n\n\n\nThe paper concludes that “endocrine and metabolic control may operate over a compact set of latent variables” — the same low-rank assumption that underlies our activity rank analysis via SVD."
  },
  {
    "objectID": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "href": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Applying MetabolismGraph to Microdialysis Data",
    "text": "Applying MetabolismGraph to Microdialysis Data\n\nWhat changes\nMoving from synthetic to real data introduces several differences:\n1. The stoichiometric matrix S is partially known\nIn simulation, \\(\\mathbf{S}\\) is either generated (ground truth) or learned from data. For real metabolomics, the reaction network connecting measured compounds can be partially reconstructed from biochemical databases (KEGG, Recon3D, HMDB). However:\n\nNot all reactions are known\nNot all participants in a reaction are measured (the 123 compounds are a subset of the full metabolome)\nSome measured compounds participate in multiple pathways\n\nThis suggests an intermediate mode between “S given” and “S learning”: initialize S from database knowledge, then refine with data.\n2. Observation is incomplete\nThe microdialysis probe measures only the free (unbound) fraction of molecules below the 6 kDa molecular weight cutoff. Many metabolites, enzymes, and signaling molecules are invisible. The GNN must learn dynamics from a partial observation of the full system — analogous to learning neural dynamics from a subset of recorded neurons.\n3. External drivers exist\nThe synthetic model is autonomous: \\(dc/dt\\) depends only on current concentrations and the network. Real metabolic dynamics are driven by external inputs — feeding, locomotion, circadian rhythms, stress — that are not part of the reaction network. The paper shows locomotion is a dominant driver (rPC1).\nThis maps to extensions of the current model:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot v_j}_{\\text{reactions}} + \\underbrace{f_{\\text{ext}}(x_{\\text{locomotion}}, t)}_{\\text{external drive}}\n\\]\nThe circadian_amplitude and circadian_period parameters already in the simulation config are a first step toward modeling time-varying external inputs.\n4. Kinetics may not be mass-action\nEnzyme-mediated reactions follow Michaelis-Menten or Hill kinetics rather than pure mass-action. The flexible MLP\\(_{\\text{sub}}\\) architecture is well-suited here — it can discover the true functional form without assuming \\(c^{|S|}\\) a priori. The additive aggregation mode may be more appropriate for enzyme-mediated reactions where substrates contribute more independently.\n\n\nWhat stays the same\nThe core framework transfers directly:\n\nBipartite graph structure: metabolites connected to reactions through stoichiometric edges\nGNN message passing: substrate concentrations aggregated per reaction, then distributed back to metabolites\nFunction discovery: MLP\\(_{\\text{sub}}\\) learns the concentration-to-rate mapping, MLP\\(_{\\text{node}}\\) learns homeostatic regulation\nRate constant recovery: per-reaction \\(k_j\\) learned in log-space\nIdentifiability challenges: scale ambiguity, function compensation, and homeostasis absorption are equally present (and harder to diagnose without ground truth)\n\n\n\nProposed workflow\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    A[Microdialysis time series&lt;br/&gt;123 compounds × 64 frames] --&gt; B[Construct bipartite graph&lt;br/&gt;from KEGG/Recon3D]\n    B --&gt; C[Initialize S from&lt;br/&gt;database stoichiometry]\n    C --&gt; D[Train GNN&lt;br/&gt;predict dc/dt]\n    D --&gt; E{Evaluate}\n    E --&gt;|test R² &gt; 0.9| F[Extract learned parameters]\n    E --&gt;|test R² &lt; 0.9| G[Refine: unfreeze S,&lt;br/&gt;add external inputs]\n    G --&gt; D\n    F --&gt; H[Rate constants k_j]\n    F --&gt; I[Learned kinetics&lt;br/&gt;MLP_sub shape]\n    F --&gt; J[Homeostatic regulation&lt;br/&gt;MLP_node per metabolite type]\n\n\n\n\n\n\nStep 1. Build the metabolite-reaction bipartite graph from KEGG pathway maps for the 123 identified compounds. Estimate the number of reactions linking measured metabolites.\nStep 2. Initialize \\(\\mathbf{S}\\) from known stoichiometry (S given mode). Train the GNN to predict \\(dc/dt\\) from concentration snapshots, learning \\(k_j\\), MLP\\(_{\\text{sub}}\\), and MLP\\(_{\\text{node}}\\).\nStep 3. Evaluate prediction quality on held-out time frames. If prediction R\\(^2\\) is high but the learned MLP\\(_{\\text{sub}}\\) does not match expected kinetics (mass-action or Michaelis-Menten), investigate degeneracy.\nStep 4. Optionally unfreeze \\(\\mathbf{S}\\) to discover missing reactions or correct database errors (S learning mode). Compare learned stoichiometry against biochemical databases.\nStep 5. Incorporate locomotion and other behavioral covariates as external inputs to account for the dominant non-metabolic drivers of concentration change."
  },
  {
    "objectID": "application.html#challenges-and-open-questions",
    "href": "application.html#challenges-and-open-questions",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Challenges and Open Questions",
    "text": "Challenges and Open Questions\nTemporal resolution. The 7.5-minute sampling cadence gives ~64 frames per session. Our simulations use 2880 frames. With fewer time points, the GNN has less data to disentangle rate constants from MLP functions, exacerbating identifiability issues. Multi-session data across the 7+ days of implant patency could help.\nMissing metabolites. The 123 measured compounds are a fraction of the full metabolome. Reactions involving unmeasured substrates will appear as unexplained variance. The MLP\\(_{\\text{node}}\\) may absorb some of this as apparent “homeostasis.”\nNon-stationarity. Real metabolic dynamics are non-stationary — circadian rhythms, feeding cycles, and adaptation shift the baseline over hours. The current model assumes fixed \\(c^{\\text{baseline}}\\) and \\(\\lambda\\). Time-varying homeostatic parameters may be needed.\nValidation without ground truth. In simulation, we validate against known \\(k_j\\) values (rate constants R\\(^2\\)). With real data, validation must rely on: (1) held-out prediction accuracy, (2) consistency with known biochemistry, (3) perturbation experiments (the paper notes microdialysis can also deliver molecules), and (4) cross-animal reproducibility."
  },
  {
    "objectID": "application.html#references",
    "href": "application.html#references",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "References",
    "text": "References\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\nSee the current results for rate constant recovery on an oscillatory regime with 100 metabolites and 256 reactions.\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\nSee the current results for rate constant recovery on an oscillatory regime with 100 metabolites and 256 reactions.\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#the-full-model",
    "href": "index.html#the-full-model",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Full Model",
    "text": "The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate \\(v_j\\) depends on aggregation type:\n\n\n\n\n\n\n\nAggregation\nRate \\(v_j\\)\n\n\n\n\nAdditive\n\\(v_j = k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\nMultiplicative\n\\(v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\n\nSee Model for detailed equations, diagrams, and model configurations."
  },
  {
    "objectID": "index.html#the-inverse-problem",
    "href": "index.html#the-inverse-problem",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe forward model describes how concentrations evolve given all parameters. In practice, the parameters themselves are unknown. The inverse problem is to recover them from observed dynamics.\nGiven:\n\nConcentration trajectories \\(\\{c_i(t)\\}_{i=1}^{n}\\) measured over time\nStoichiometric matrix \\(\\mathbf{S}\\) (known from biochemistry)\n\nTo learn:\n\nSubstrate function \\(\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\) — discovers the mass-action power law \\(c_k^{|S_{kj}|}\\)\nHomeostasis function \\(\\text{MLP}_{\\text{node}}(c_i)\\) — discovers per-metabolite regulation \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\nRate constants \\(k_j\\) — per-reaction speed scalars\n\nThis is challenging because the system is high-dimensional (\\(n\\) metabolites, \\(m\\) reactions), the mapping from parameters to dynamics is nonlinear, and multiple parameter combinations can produce similar trajectories (identifiability). Classical optimization approaches struggle with this combinatorial landscape.\nWe address this by casting the inverse problem as a Graph Neural Network learning task. The metabolic network is naturally a bipartite graph (metabolites \\(\\leftrightarrow\\) reactions), and we replace the unknown functions with learnable MLPs that operate on this graph structure. The GNN is trained end-to-end by minimizing the prediction error on \\(dc/dt\\), recovering the rate constants and homeostatic functions simultaneously. An LLM-driven closed-loop exploration engine systematically searches the hyperparameter space — see GNN-LLM-Memory for the training scheme, regularization terms, and exploration loop.\n\nGNN Parameterization\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\prod_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nwhere:\n\n\\(a_i \\in \\mathbb{R}^d\\) is a learnable embedding for metabolite \\(i\\)\n\\(k_j\\) are learnable rate constants\n\\(\\prod\\) denotes multiplicative aggregation (mass-action kinetics)\n\n\n\nLearnable Parameters\n\n\n\n\n\n\n\n\nParameter\nType\nPurpose\n\n\n\n\n\\(a_i\\)\nEmbedding vectors\nPer-metabolite identity\n\n\n\\(k_j\\)\nScalars\nPer-reaction rate constants\n\n\n\\(\\text{MLP}_{\\text{node}}\\)\nNeural network\nLearns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\n\n\n\\(\\text{MLP}_{\\text{sub}}\\)\nNeural network\nLearns \\(c_k^{|S_{kj}|}\\)"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "Citation",
    "text": "Citation\nIf you use MetabolismGraph in your research, please cite:\n@software{metabolismgraph2025,\n  author = {Allier, Cédric},\n  title = {MetabolismGraph: Learning Metabolism Dynamics with GNNs},\n  year = {2026},\n  url = {https://github.com/allierc/MetabolismGraph}\n}"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This page was generated by Claude and may contain inaccuracies in author lists, publication details, or descriptions. Please verify citations before use."
  },
  {
    "objectID": "references.html#context",
    "href": "references.html#context",
    "title": "References",
    "section": "Context",
    "text": "Context\nMetabolismGraph sits at the intersection of several active research areas: graph neural networks for physical and biological systems, inverse problems in systems biology, neural differential equations, and LLM-driven scientific exploration. The framework uses message-passing GNNs on bipartite metabolite-reaction graphs to solve the inverse problem of recovering kinetic parameters from concentration dynamics, with an LLM-based closed-loop exploration engine for hyperparameter optimization.\nBelow we collect key references organized by topic. Where available, we provide arXiv or DOI links."
  },
  {
    "objectID": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "href": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "title": "References",
    "section": "Graph Neural Networks for Physical and Biological Systems",
    "text": "Graph Neural Networks for Physical and Biological Systems\n\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., & Battaglia, P. W. (2020). Learning to simulate complex physics with graph networks. ICML 2020. arXiv:2002.09405 — Graph network-based simulators (GNS) for learning physical dynamics from particle-based representations. Demonstrates that GNNs can learn accurate forward simulators for complex physical systems.\nKipf, T., Fetaya, E., Wang, K.-C., Welling, M., & Zemel, R. (2018). Neural relational inference for interacting systems. ICML 2018. arXiv:1802.04687 — Learns interaction graphs from observed trajectories using variational autoencoders on graph structures. Closely related to our inverse-problem setting where the goal is to recover network structure from dynamics.\nCranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., & Ho, S. (2020). Discovering symbolic models from deep learning with inductive biases. NeurIPS 2020. arXiv:2006.11287 — Combines GNNs with symbolic regression to extract interpretable physical laws from learned representations. Relevant to our approach of recovering interpretable kinetic parameters from learned MLP functions."
  },
  {
    "objectID": "references.html#neural-differential-equations-and-scientific-ml",
    "href": "references.html#neural-differential-equations-and-scientific-ml",
    "title": "References",
    "section": "Neural Differential Equations and Scientific ML",
    "text": "Neural Differential Equations and Scientific ML\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural ordinary differential equations. NeurIPS 2018. arXiv:1806.07366 — Foundational work on continuous-depth neural networks parameterized as ODEs, enabling gradient-based learning of dynamical systems.\nRackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R., Skinner, D., Ramadhan, A., & Edelman, A. (2020). Universal differential equations for scientific machine learning. arXiv:2001.04385 — Framework combining differential equations with neural networks for scientific modeling. The UDE approach of embedding learnable components within known differential equation structure is conceptually similar to how MetabolismGraph embeds learnable MLPs within the known stoichiometric framework."
  },
  {
    "objectID": "references.html#symbolic-regression-for-network-dynamics",
    "href": "references.html#symbolic-regression-for-network-dynamics",
    "title": "References",
    "section": "Symbolic Regression for Network Dynamics",
    "text": "Symbolic Regression for Network Dynamics\n\nYu, Z., Ding, J., & Li, Y. (2025). Discovering network dynamics with neural symbolic regression. Nature Computational Science. DOI:10.1038/s43588-025-00893-8 — ND2: neural symbolic regression that discovers governing equations of network dynamics directly from data. Applied to gene regulatory networks, the method corrects the classical Hill equation model by replacing per-neighbor nonlinear terms with a logistic function applied to the aggregate neighbor sum (see comparison below).\n\n\nComparison with MetabolismGraph\nYu et al.’s corrected gene regulation model (their Eq. 2) and MetabolismGraph share the same general ODE structure — self-dynamics plus interaction dynamics — but differ in how neighbor contributions are aggregated:\nYu et al. — Gene regulation (ND2 corrected):\n\\[\\frac{dx_i}{dt} = \\underbrace{s_i - \\gamma_i x_i}_{\\text{self-dynamics}} + \\underbrace{\\beta \\, \\tilde{S}\\!\\left(\\sum_j A_{ij}\\, x_j\\right)}_{\\text{interaction}}\\]\nwhere \\(\\tilde{S}(x) = (1 + e^{-x})^{-1}\\) is the logistic function. The nonlinearity acts on the sum of weighted neighbor states — no per-edge rate constants, no multiplicative aggregation.\nMetabolismGraph — Metabolic kinetics:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i(c_i - c_i^{\\text{baseline}})}_{\\text{self-dynamics}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}}_{\\text{interaction}}\\]\nKey differences:\n\n\n\n\n\n\n\n\n\nYu et al. (gene regulation)\nMetabolismGraph (metabolism)\n\n\n\n\nGraph\nMonopartite (gene → gene)\nBipartite (metabolite ↔︎ reaction)\n\n\nAggregation\n\\(\\tilde{S}(\\sum_j A_{ij} x_j)\\) — logistic of sum\n\\(\\sum_j S_{ij} k_j \\prod_k c_k^{s_{kj}}\\) — sum of products\n\n\nRate constants\nSingle global \\(\\beta\\)\nPer-reaction \\(k_j\\) (256 parameters)\n\n\nNonlinearity\nBounded logistic \\(\\tilde{S} \\in [0,1]\\)\nUnbounded power law \\(c^s\\)\n\n\nHigher-order\nImplicit: \\(\\partial \\dot{x}_i / \\partial x_j\\) depends on all neighbors via \\(\\tilde{S}\\)\nExplicit: mass-action products couple substrates within each reaction\n\n\n\nThe gene regulation model has no per-reaction aggregation step — it sums all neighbor states into a single scalar, then applies a saturating nonlinearity. MetabolismGraph instead computes a separate rate for each reaction (multiplicative aggregation of substrate concentrations), then sums the stoichiometric contributions. This reflects a fundamental difference between gene regulation (bounded transcriptional response) and metabolism (unbounded mass-action kinetics)."
  },
  {
    "objectID": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "href": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "title": "References",
    "section": "In Vivo Metabolomics and Physiological State Spaces",
    "text": "In Vivo Metabolomics and Physiological State Spaces\n\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974 — Chronic jugular microdialysis paired with LC-MS measures ~123 bloodborne compounds at 7.5-min cadence in freely moving mice. PCA reveals a low-rank physiological manifold: 10 components explain 73% of variance, with rPC1 aligned to locomotion. Provides the real-world metabolomic time series that MetabolismGraph’s inverse problem framework is designed to analyze. See Application for how the GNN framework maps to this data."
  },
  {
    "objectID": "references.html#llm-driven-scientific-discovery",
    "href": "references.html#llm-driven-scientific-discovery",
    "title": "References",
    "section": "LLM-Driven Scientific Discovery",
    "text": "LLM-Driven Scientific Discovery\n\nRomera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468-475. DOI:10.1038/s41586-023-06924-6 — FunSearch: uses LLMs to discover new mathematical constructions through evolutionary program search. Pioneering demonstration that LLMs can make genuine scientific contributions when embedded in a search loop.\nNovikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration. Google DeepMind. — Extends the LLM-driven exploration paradigm to broader scientific and algorithmic discovery tasks. The closed-loop LLM exploration engine in MetabolismGraph draws inspiration from this line of work.\nLu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024). The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv:2408.06292 — End-to-end autonomous research agent that generates hypotheses, runs experiments, and writes papers."
  }
]