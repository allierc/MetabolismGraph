[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine ran 68 iterations (3 blocks of 24, with 4 parallel slots) on the oscillatory simulation regime: 100 metabolites, 256 reactions, 2880 time frames, mass-action kinetics with stoichiometry frozen from ground truth. The goal was to recover the 256 rate constants \\(k_j\\) by optimizing training hyperparameters through UCB tree search.\nThe best rate constant recovery reached \\(R^2 = 0.58\\) (shift-corrected), achieved with the following optimal configuration:\n\n\n\nParameter\nOptimal value\n\n\n\n\nlr_k\n0.005\n\n\nlr_node\n0.001\n\n\nlr_sub\n0.0005\n\n\nbatch_size\n8\n\n\nn_epochs\n1\n\n\ndata_augmentation_loop\n2000\n\n\ncoeff_k_center\n5.0\n\n\ncoeff_MLP_sub_diff\n3–5\n\n\ncoeff_MLP_node_L1\n0.5–1.0\n\n\n\n\n\nSharp optima. Several hyperparameters have narrow optimal ranges. lr_k = 0.005 is a sharp optimum — both 0.004 and 0.006 degrade \\(R^2\\) by 10\\(\\times\\). Similarly, coeff_k_center = 5.0 is strictly optimal; values of 4.0 or 6.0 cause significant regression.\nScale ambiguity regularization matters. The coeff_k_center regularization anchoring \\(\\text{mean}(\\log k)\\) to the ground-truth range center was essential. Without it, the learned rate constants absorb a multiplicative offset into the MLP\\(_\\text{sub}\\) output, making the scatter plot slope deviate from 1.\nRegularization interactions. coeff_MLP_sub_diff and coeff_MLP_node_L1 interact: the combination (diff=5, node_L1=0.5) and (diff=3, node_L1=1.0) both work, but (diff=3, node_L1=0.5) fails. Reducing coeff_MLP_node_L1 below 0.5 causes the MLP\\(_\\text{sub}\\) to learn an inverse (negative correlation) function.\nNon-reproducibility. Results are highly sensitive to random seed. The same configuration can yield \\(R^2\\) ranging from 0.09 to 0.57 across different seeds. Working seeds identified: 45, 48, 49, 50, 52, 55. This suggests the optimization landscape has many local minima.\nMLP\\(_\\text{node}\\) limitation. Across all runs, the MLP\\(_\\text{node}\\) (homeostasis) did not learn the true homeostatic function \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\). It consistently shows a flat or weakly decreasing pattern, suggesting the architecture may need modification to better capture per-metabolite homeostatic regulation.\n\n\n\n\nCan \\(R^2\\) be pushed above 0.6 with architectural changes?\nIs the non-reproducibility inherent to the optimization landscape or addressable with better initialization?\nCan the MLP\\(_\\text{node}\\) architecture be modified to learn homeostasis (e.g., explicit linear bias term)?\nWill unfreezing the stoichiometry matrix \\(\\mathbf{S}\\) improve or degrade rate constant recovery?"
  },
  {
    "objectID": "results.html#oscillatory-simulation-s-given-mode",
    "href": "results.html#oscillatory-simulation-s-given-mode",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine ran 68 iterations (3 blocks of 24, with 4 parallel slots) on the oscillatory simulation regime: 100 metabolites, 256 reactions, 2880 time frames, mass-action kinetics with stoichiometry frozen from ground truth. The goal was to recover the 256 rate constants \\(k_j\\) by optimizing training hyperparameters through UCB tree search.\nThe best rate constant recovery reached \\(R^2 = 0.58\\) (shift-corrected), achieved with the following optimal configuration:\n\n\n\nParameter\nOptimal value\n\n\n\n\nlr_k\n0.005\n\n\nlr_node\n0.001\n\n\nlr_sub\n0.0005\n\n\nbatch_size\n8\n\n\nn_epochs\n1\n\n\ndata_augmentation_loop\n2000\n\n\ncoeff_k_center\n5.0\n\n\ncoeff_MLP_sub_diff\n3–5\n\n\ncoeff_MLP_node_L1\n0.5–1.0\n\n\n\n\n\nSharp optima. Several hyperparameters have narrow optimal ranges. lr_k = 0.005 is a sharp optimum — both 0.004 and 0.006 degrade \\(R^2\\) by 10\\(\\times\\). Similarly, coeff_k_center = 5.0 is strictly optimal; values of 4.0 or 6.0 cause significant regression.\nScale ambiguity regularization matters. The coeff_k_center regularization anchoring \\(\\text{mean}(\\log k)\\) to the ground-truth range center was essential. Without it, the learned rate constants absorb a multiplicative offset into the MLP\\(_\\text{sub}\\) output, making the scatter plot slope deviate from 1.\nRegularization interactions. coeff_MLP_sub_diff and coeff_MLP_node_L1 interact: the combination (diff=5, node_L1=0.5) and (diff=3, node_L1=1.0) both work, but (diff=3, node_L1=0.5) fails. Reducing coeff_MLP_node_L1 below 0.5 causes the MLP\\(_\\text{sub}\\) to learn an inverse (negative correlation) function.\nNon-reproducibility. Results are highly sensitive to random seed. The same configuration can yield \\(R^2\\) ranging from 0.09 to 0.57 across different seeds. Working seeds identified: 45, 48, 49, 50, 52, 55. This suggests the optimization landscape has many local minima.\nMLP\\(_\\text{node}\\) limitation. Across all runs, the MLP\\(_\\text{node}\\) (homeostasis) did not learn the true homeostatic function \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\). It consistently shows a flat or weakly decreasing pattern, suggesting the architecture may need modification to better capture per-metabolite homeostatic regulation.\n\n\n\n\nCan \\(R^2\\) be pushed above 0.6 with architectural changes?\nIs the non-reproducibility inherent to the optimization landscape or addressable with better initialization?\nCan the MLP\\(_\\text{node}\\) architecture be modified to learn homeostasis (e.g., explicit linear bias term)?\nWill unfreezing the stoichiometry matrix \\(\\mathbf{S}\\) improve or degrade rate constant recovery?"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph Metabolites\n        A[Glucose]\n        B[ATP]\n        C[Pyruvate]\n        D[ADP]\n    end\n\n    subgraph Reactions\n        R1((R1))\n        R2((R2))\n    end\n\n    A --&gt;|\"-1\"| R1\n    B --&gt;|\"-1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"-1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#metabolic-networks-as-bipartite-graphs",
    "href": "model.html#metabolic-networks-as-bipartite-graphs",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph Metabolites\n        A[Glucose]\n        B[ATP]\n        C[Pyruvate]\n        D[ADP]\n    end\n\n    subgraph Reactions\n        R1((R1))\n        R2((R2))\n    end\n\n    A --&gt;|\"-1\"| R1\n    B --&gt;|\"-1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"-1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#the-stoichiometric-matrix",
    "href": "model.html#the-stoichiometric-matrix",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Stoichiometric Matrix",
    "text": "The Stoichiometric Matrix\nThe stoichiometric matrix \\(\\mathbf{S}\\) is an \\((n_{\\text{metabolites}} \\times n_{\\text{reactions}})\\) matrix where entry \\(S_{ij}\\) indicates how metabolite \\(i\\) participates in reaction \\(j\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix}\n-1 & 0 & \\cdots \\\\\n-1 & +1 & \\cdots \\\\\n+2 & -1 & \\cdots \\\\\n+1 & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\]\nProperties of S:\n\nSparse: most entries are zero (each reaction involves only 2-6 metabolites)\nInteger-valued: entries are typically in \\(\\{-2, -1, 0, +1, +2\\}\\)\nMass conservation: column sums should be zero for balanced reactions\n\nExample:\nConsider the diagram above with 2 reactions:\n\\[\n\\begin{aligned}\n\\text{R1}: \\quad & \\text{Glucose} + \\text{ATP} \\longrightarrow 2\\,\\text{Pyruvate} + \\text{ADP} \\\\\n\\text{R2}: \\quad & \\text{Pyruvate} \\longrightarrow \\text{ATP}\n\\end{aligned}\n\\]\nThe corresponding stoichiometric matrix is:\n\\[\n\\begin{array}{c|cc}\n& \\text{R1} & \\text{R2} \\\\\n\\hline\n\\text{Glucose} & -1 & 0 \\\\\n\\text{ATP} & -1 & +1 \\\\\n\\text{Pyruvate} & +2 & -1 \\\\\n\\text{ADP} & +1 & 0\n\\end{array}\n\\]"
  },
  {
    "objectID": "model.html#model-evolution",
    "href": "model.html#model-evolution",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Model Evolution",
    "text": "Model Evolution\n\n1. Pure Reaction\nPure reaction dynamics without homeostasis, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{s_{kj}}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\n2. Homeostasis\nWith homeostatic regulation pulling concentrations toward baseline, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{s_{kj}}}_{\\text{reactions}}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.01\n\n\n\\(c^{\\text{baseline}}\\)\n5.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\n3. Oscillatory\nAutocatalytic cycles with mass-action kinetics (multiplicative aggregation) for sustained oscillations:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}\\]\nExample cycle: \\(A + B \\to 2B, \\quad B + C \\to 2C, \\quad C + A \\to 2A\\)\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nAggregation\nProduct (multiplicative)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.5}, 10^{-1}]\\)\n\n\nInitial concentrations\n[1.0, 9.0]\n\n\nFlux limiting\nDisabled"
  },
  {
    "objectID": "model.html#activity-rank",
    "href": "model.html#activity-rank",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Activity Rank",
    "text": "Activity Rank\nTo quantify the complexity of concentration dynamics, we compute the activity rank using singular value decomposition (SVD) of the concentration matrix \\(\\mathbf{C} \\in \\mathbb{R}^{T \\times n}\\) (time frames × metabolites).\nThe rank at 99% variance is the number of singular values needed to capture 99% of the total variance:\n\\[\\text{rank}_{99} = \\min \\left\\{ k : \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.99 \\right\\}\\]\nInterpretation:\n\nLow rank (1-5): concentrations are highly correlated, dynamics are simple (equilibration or uniform decay)\nHigh rank (&gt;20): metabolites evolve independently with rich, complex dynamics\n\n\nIncreasing Activity Rank\nWith the baseline oscillatory config (\\(k_j \\in [10^{-2.5}, 10^{-1}]\\), 256 reactions), the activity rank is 24. Systematic exploration of simulation parameters revealed that activity rank is controlled by the number of actively contributing reactions per metabolite. Three parameters have a significant effect:\n\n\n\n\n\n\n\n\nChange\nActivity Rank\nEffect\n\n\n\n\nBaseline\n24\nReference\n\n\nNarrow \\(k\\) range: \\([10^{-2.0}, 10^{-1}]\\)\n50\nEliminates slow inert reactions\n\n\nDouble reactions: 512\n47\nMore overlapping cycles per metabolite\n\n\nZero homeostasis: \\(\\lambda = 0\\)\n37\nRemoves linear damping\n\n\n\nThe most impactful single change is narrowing the rate constant range from \\([10^{-2.5}, 10^{-1}]\\) to \\([10^{-2.0}, 10^{-1}]\\). With the wider range, ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly to the dynamics. Removing these inert reactions doubles the activity rank.\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.0}, 10^{-1}]\\)\n\n\nAll other parameters\nSame as oscillatory baseline"
  },
  {
    "objectID": "model.html#summary-the-full-model",
    "href": "model.html#summary-the-full-model",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Summary: The Full Model",
    "text": "Summary: The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate follows mass-action kinetics:\n\\[\nv_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}\n\\]"
  },
  {
    "objectID": "gnn-llm-memory.html",
    "href": "gnn-llm-memory.html",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#the-exploration-loop",
    "href": "gnn-llm-memory.html#the-exploration-loop",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#training-scheme",
    "href": "gnn-llm-memory.html#training-scheme",
    "title": "GNN-LLM-Memory",
    "section": "Training Scheme",
    "text": "Training Scheme\nThe GNN is trained by minimizing the prediction error on \\(dc/dt\\):\n\\[\n\\mathcal{L} = \\sum_{\\text{frames}} \\left\\| \\frac{dc}{dt}_{\\text{pred}} - \\frac{dc}{dt}_{\\text{GT}} \\right\\|_2 + \\mathcal{R}\n\\]\nwhere \\(\\mathcal{R}\\) is the sum of regularization terms described below.\n\nSeparate Learning Rates\nEach model component has its own learning rate to control the balance between parameter groups:\n\n\n\n\n\n\n\n\n\nComponent\nConfig key\nControls\nTypical range\n\n\n\n\nRate constants \\(k_j\\)\nlearning_rate_k\nHow fast k values are updated\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{node}}\\) (homeostasis)\nlearning_rate_node\nHomeostasis function learning speed\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{sub}}\\) (substrate)\nlearning_rate_sub\nSubstrate function learning speed\n1E-4 to 1E-2\n\n\n\nThe learning rate balance is critical:\n\nlr_k too high: \\(k\\) values overshoot, oscillate, or converge to wrong values\nlr_k too low: \\(k\\) barely moves, MLPs compensate\nlr_node/lr_sub imbalance: one function absorbs capacity meant for the other\n\n\n\nRegularization Terms\nThe total regularization \\(\\mathcal{R}\\) is the sum of the following penalties:\n\nMLP\\(_{\\text{sub}}\\) Monotonicity (coeff_MLP_sub_diff)\nMLP\\(_{\\text{sub}}\\) learns \\(c^s\\) which should be monotonically increasing in concentration. This penalty samples concentration pairs \\((c, c+\\delta)\\) and penalizes cases where the output decreases:\n\\[\n\\mathcal{R}_{\\text{sub\\_diff}} = \\left\\| \\text{ReLU}\\left(\\|\\text{MLP}_{\\text{sub}}(c)\\| - \\|\\text{MLP}_{\\text{sub}}(c+\\delta)\\|\\right) \\right\\|_2 \\cdot \\lambda_{\\text{sub\\_diff}}\n\\]\nWithout this constraint, MLP\\(_{\\text{sub}}\\) can develop non-physical local minima that don’t match the true power law behavior.\n\n\nMLP\\(_{\\text{node}}\\) L1 (coeff_MLP_node_L1)\nPenalizes large MLP\\(_{\\text{node}}\\) output to keep homeostasis values small relative to reaction terms:\n\\[\n\\mathcal{R}_{\\text{node\\_L1}} = \\text{mean}\\left(|\\text{MLP}_{\\text{node}}(c_i, a_i)|\\right) \\cdot \\lambda_{\\text{node\\_L1}}\n\\]\nMLP\\(_{\\text{node}}\\) is initialized to zero output so homeostasis starts inactive. Combined with a reduced learning rate, this prevents MLP\\(_{\\text{node}}\\) from dominating the dynamics. The true homeostatic \\(\\lambda\\) values are small (0.001–0.002), so MLP\\(_{\\text{node}}\\) output should remain small.\n\n\n\\(k\\) Center (coeff_k_center)\nBreaks the scale ambiguity between \\(k\\) and MLP\\(_{\\text{sub}}\\). The product \\(k_j \\cdot \\text{MLP}_{\\text{sub}}(c)\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). This penalty anchors the mean of \\(\\log_{10}(k)\\) to the center of the known range:\n\\[\n\\mathcal{R}_{k\\_center} = \\left(\\text{mean}(\\log_{10} k) - \\frac{\\log_{10} k_{\\min} + \\log_{10} k_{\\max}}{2}\\right)^2 \\cdot \\lambda_{k\\_center}\n\\]\nWithout this, MLP\\(_{\\text{sub}}\\) can absorb a global scale factor and shift all \\(k\\) values, producing high correlation (high \\(R^2_{\\text{shifted}}\\)) but poor absolute recovery (low \\(R^2\\)).\n\n\n\nSummary of Hyperparameters\n\n\n\n\n\n\n\n\n\nParameter\nConfig key\nDescription\nTypical range\n\n\n\n\nLearning rate \\(k\\)\nlearning_rate_k\nRate constants update speed\n1E-4 to 1E-2\n\n\nLearning rate node\nlearning_rate_node\nMLP\\(_{\\text{node}}\\) update speed\n1E-4 to 1E-2\n\n\nLearning rate sub\nlearning_rate_sub\nMLP\\(_{\\text{sub}}\\) update speed\n1E-4 to 1E-2\n\n\nBatch size\nbatch_size\nTime frames per gradient step\n4 to 32\n\n\nTraining iterations\ndata_augmentation_loop\nMultiplier for iterations per epoch\n100 to 5000\n\n\nMLP\\(_{\\text{sub}}\\) monotonicity\ncoeff_MLP_sub_diff\nPenalize non-increasing MLP\\(_{\\text{sub}}\\)\n0 to 500\n\n\nMLP\\(_{\\text{node}}\\) L1\ncoeff_MLP_node_L1\nPenalize large homeostasis output\n0 to 10\n\n\n\\(k\\) center\ncoeff_k_center\nAnchor mean(\\(\\log k\\)) to GT range\n0 to 10"
  },
  {
    "objectID": "gnn-llm-memory.html#metrics",
    "href": "gnn-llm-memory.html#metrics",
    "title": "GNN-LLM-Memory",
    "section": "Metrics",
    "text": "Metrics\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood value\n\n\n\n\nrate_constants_R2\nR² between learned and true rate constants \\(k\\)\n&gt; 0.9\n\n\nrate_constants_R2_shifted\nR² after removing mean offset — measures correlation independent of global scale\n&gt; 0.9\n\n\ntest_R2\nR² on held-out test frames\n&gt; 0.9\n\n\ntest_pearson\nPearson correlation on test frames\n&gt; 0.95\n\n\nfinal_loss\nFinal prediction loss (MSE on \\(dc/dt\\))\nLower is better\n\n\n\n\nDiagnostic Interpretation\n\n\n\n\\(R^2\\)\n\\(R^2_{\\text{shifted}}\\)\nDiagnosis\n\n\n\n\nHigh\nHigh\nCorrect recovery\n\n\nLow\nHigh\nScale ambiguity — increase coeff_k_center\n\n\nLow\nLow\nPoor correlation — adjust learning rates\n\n\n\n\n\n\n\n\n\n\n\n\ntest_pearson\n\\(R^2\\)\nGap\nDiagnosis\n\n\n\n\n&gt; 0.95\n&gt; 0.9\n&lt; 0.1\nHealthy — good dynamics from correct \\(k\\)\n\n\n&gt; 0.95\n0.3–0.9\n0.1–0.7\nDegenerate — good dynamics from wrong \\(k\\)\n\n\n&gt; 0.95\n&lt; 0.3\n&gt; 0.7\nSeverely degenerate — MLPs compensating\n\n\n&lt; 0.5\n&lt; 0.5\n~0\nFailed — both dynamics and \\(k\\) poor"
  },
  {
    "objectID": "gnn-llm-memory.html#ucb-tree-search",
    "href": "gnn-llm-memory.html#ucb-tree-search",
    "title": "GNN-LLM-Memory",
    "section": "UCB Tree Search",
    "text": "UCB Tree Search\nThe LLM selects parent configurations to mutate using an Upper Confidence Bound (UCB) strategy that balances exploitation of high-performing branches with exploration of under-visited regions:\n\\[\n\\text{UCB}(i) = \\bar{X}_i + c \\cdot \\sqrt{\\frac{\\ln N}{n_i}}\n\\]\nwhere \\(\\bar{X}_i\\) is the mean reward of node \\(i\\), \\(N\\) is the total number of visits, \\(n_i\\) is the number of visits to node \\(i\\), and \\(c\\) is the exploration constant.\n4 parallel slots run per batch with diversified roles:\n\n\n\n\n\n\n\n\nSlot\nRole\nDescription\n\n\n\n\n0\nexploit\nHighest UCB node, conservative mutation\n\n\n1\nexploit\n2nd highest UCB, or same parent different param\n\n\n2\nexplore\nUnder-visited node, or new parameter dimension\n\n\n3\nprinciple-test\nTest or challenge one established principle from memory"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MetabolismGraph: Learning Stoichiometric Networks with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nHomeostatic parameters \\(\\lambda_i\\), \\(c_i^{\\text{baseline}}\\) — metabolite regulation"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MetabolismGraph: Learning Stoichiometric Networks with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nHomeostatic parameters \\(\\lambda_i\\), \\(c_i^{\\text{baseline}}\\) — metabolite regulation"
  },
  {
    "objectID": "index.html#the-full-model",
    "href": "index.html#the-full-model",
    "title": "MetabolismGraph: Learning Stoichiometric Networks with Graph Neural Networks",
    "section": "The Full Model",
    "text": "The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate \\(v_j\\) depends on aggregation type:\n\n\n\n\n\n\n\nAggregation\nRate \\(v_j\\)\n\n\n\n\nAdditive\n\\(v_j = k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{s_{kj}}\\)\n\n\nMultiplicative\n\\(v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}\\)\n\n\n\nSee Model for detailed equations, diagrams, and model configurations."
  },
  {
    "objectID": "index.html#the-inverse-problem",
    "href": "index.html#the-inverse-problem",
    "title": "MetabolismGraph: Learning Stoichiometric Networks with Graph Neural Networks",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe forward model describes how concentrations evolve given all parameters. In practice, the parameters themselves are unknown. The inverse problem is to recover them from observed dynamics.\nGiven:\n\nConcentration trajectories \\(\\{c_i(t)\\}_{i=1}^{n}\\) measured over time\nStoichiometric matrix \\(\\mathbf{S}\\) (known from biochemistry)\n\nTo learn:\n\nRate constants \\(k_j\\) — how fast each reaction proceeds\nHomeostatic parameters \\(\\lambda_i\\), \\(c_i^{\\text{baseline}}\\) — per-metabolite regulatory strength and target concentration\n\nThis is challenging because the system is high-dimensional (\\(n\\) metabolites, \\(m\\) reactions), the mapping from parameters to dynamics is nonlinear, and multiple parameter combinations can produce similar trajectories (identifiability). Classical optimization approaches struggle with this combinatorial landscape.\nWe address this by casting the inverse problem as a Graph Neural Network learning task. The metabolic network is naturally a bipartite graph (metabolites \\(\\leftrightarrow\\) reactions), and we replace the unknown functions with learnable MLPs that operate on this graph structure. The GNN is trained end-to-end by minimizing the prediction error on \\(dc/dt\\), recovering the rate constants and homeostatic functions simultaneously. An LLM-driven closed-loop exploration engine systematically searches the hyperparameter space — see GNN-LLM-Memory for the training scheme, regularization terms, and exploration loop.\n\nGNN Parameterization\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot k_j \\cdot \\text{aggr}_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, s_{kj})}_{\\text{learns } c_k^{s_{kj}}}\n\\]\nwhere:\n\n\\(a_i \\in \\mathbb{R}^d\\) is a learnable embedding for metabolite \\(i\\)\n\\(k_j\\) are learnable rate constants\n\\(\\text{aggr}\\) is sum (additive) or product (multiplicative)\n\n\n\nLearnable Parameters\n\n\n\n\n\n\n\n\nParameter\nType\nPurpose\n\n\n\n\n\\(a_i\\)\nEmbedding vectors\nPer-metabolite identity\n\n\n\\(k_j\\)\nScalars\nPer-reaction rate constants\n\n\n\\(\\text{MLP}_{\\text{node}}\\)\nNeural network\nLearns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\n\n\n\\(\\text{MLP}_{\\text{sub}}\\)\nNeural network\nLearns \\(c_k^{s_{kj}}\\)\n\n\n\n\n\nConfiguration\ngraph_model:\n  aggr_type: add         # sum (additive) or mul (multiplicative)\n  embedding_dim: 2       # dimension of metabolite embeddings a_i\n  hidden_dim: 32\n\ntraining:\n  learning_rate_start: 0.001\n  freeze_stoichiometry: true   # S given mode\n  training_single_type: false  # learn per-metabolite embeddings"
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "MetabolismGraph: Learning Stoichiometric Networks with Graph Neural Networks",
    "section": "Key Features",
    "text": "Key Features\n\nBipartite graph representation: metabolites and reactions form a bipartite graph, with stoichiometric coefficients on edges\nLearnable rate constants: per-reaction \\(k_j\\) learned via gradient descent\nLearnable homeostasis: MLP learns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\) per metabolite\nFlexible aggregation: additive (sum) or multiplicative (product) for different dynamics\nMetabolite embeddings: learnable vectors \\(a_i\\) capture per-metabolite identity"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "MetabolismGraph: Learning Stoichiometric Networks with Graph Neural Networks",
    "section": "Citation",
    "text": "Citation\nIf you use MetabolismGraph in your research, please cite:\n@software{metabolismgraph2025,\n  author = {Allier, Cédric},\n  title = {MetabolismGraph: Learning Stoichiometric Networks with GNNs},\n  year = {2025},\n  url = {https://github.com/allierc/MetabolismGraph}\n}"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This page was generated by Claude and may contain inaccuracies in author lists, publication details, or descriptions. Please verify citations before use."
  },
  {
    "objectID": "references.html#context",
    "href": "references.html#context",
    "title": "References",
    "section": "Context",
    "text": "Context\nMetabolismGraph sits at the intersection of several active research areas: graph neural networks for physical and biological systems, inverse problems in systems biology, neural differential equations, and LLM-driven scientific exploration. The framework uses message-passing GNNs on bipartite metabolite-reaction graphs to solve the inverse problem of recovering kinetic parameters from concentration dynamics, with an LLM-based closed-loop exploration engine for hyperparameter optimization.\nBelow we collect key references organized by topic. Where available, we provide arXiv or DOI links."
  },
  {
    "objectID": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "href": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "title": "References",
    "section": "Graph Neural Networks for Physical and Biological Systems",
    "text": "Graph Neural Networks for Physical and Biological Systems\n\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., & Battaglia, P. W. (2020). Learning to simulate complex physics with graph networks. ICML 2020. arXiv:2002.09405 — Graph network-based simulators (GNS) for learning physical dynamics from particle-based representations. Demonstrates that GNNs can learn accurate forward simulators for complex physical systems.\nKipf, T., Fetaya, E., Wang, K.-C., Welling, M., & Zemel, R. (2018). Neural relational inference for interacting systems. ICML 2018. arXiv:1802.04687 — Learns interaction graphs from observed trajectories using variational autoencoders on graph structures. Closely related to our inverse-problem setting where the goal is to recover network structure from dynamics.\nCranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., & Ho, S. (2020). Discovering symbolic models from deep learning with inductive biases. NeurIPS 2020. arXiv:2006.11287 — Combines GNNs with symbolic regression to extract interpretable physical laws from learned representations. Relevant to our approach of recovering interpretable kinetic parameters from learned MLP functions."
  },
  {
    "objectID": "references.html#neural-differential-equations-and-scientific-ml",
    "href": "references.html#neural-differential-equations-and-scientific-ml",
    "title": "References",
    "section": "Neural Differential Equations and Scientific ML",
    "text": "Neural Differential Equations and Scientific ML\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural ordinary differential equations. NeurIPS 2018. arXiv:1806.07366 — Foundational work on continuous-depth neural networks parameterized as ODEs, enabling gradient-based learning of dynamical systems.\nRackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R., Skinner, D., Ramadhan, A., & Edelman, A. (2020). Universal differential equations for scientific machine learning. arXiv:2001.04385 — Framework combining differential equations with neural networks for scientific modeling. The UDE approach of embedding learnable components within known differential equation structure is conceptually similar to how MetabolismGraph embeds learnable MLPs within the known stoichiometric framework."
  },
  {
    "objectID": "references.html#llm-driven-scientific-discovery",
    "href": "references.html#llm-driven-scientific-discovery",
    "title": "References",
    "section": "LLM-Driven Scientific Discovery",
    "text": "LLM-Driven Scientific Discovery\n\nRomera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468-475. DOI:10.1038/s41586-023-06924-6 — FunSearch: uses LLMs to discover new mathematical constructions through evolutionary program search. Pioneering demonstration that LLMs can make genuine scientific contributions when embedded in a search loop.\nNovikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration. Google DeepMind. — Extends the LLM-driven exploration paradigm to broader scientific and algorithmic discovery tasks. The closed-loop LLM exploration engine in MetabolismGraph draws inspiration from this line of work.\nLu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024). The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv:2408.06292 — End-to-end autonomous research agent that generates hypotheses, runs experiments, and writes papers."
  },
  {
    "objectID": "references.html#symbolic-regression-for-network-dynamics",
    "href": "references.html#symbolic-regression-for-network-dynamics",
    "title": "References",
    "section": "Symbolic Regression for Network Dynamics",
    "text": "Symbolic Regression for Network Dynamics\n\nYu, Z., Ding, J., & Li, Y. (2025). Discovering network dynamics with neural symbolic regression. Nature Computational Science. DOI:10.1038/s43588-025-00893-8 — ND2: neural symbolic regression that discovers governing equations of network dynamics directly from data. Applied to gene regulatory networks, the method corrects the classical Hill equation model by replacing per-neighbor nonlinear terms with a logistic function applied to the aggregate neighbor sum (see comparison below).\n\n\nComparison with MetabolismGraph\nYu et al.’s corrected gene regulation model (their Eq. 2) and MetabolismGraph share the same general ODE structure — self-dynamics plus interaction dynamics — but differ in how neighbor contributions are aggregated:\nYu et al. — Gene regulation (ND2 corrected):\n\\[\\frac{dx_i}{dt} = \\underbrace{s_i - \\gamma_i x_i}_{\\text{self-dynamics}} + \\underbrace{\\beta \\, \\tilde{S}\\!\\left(\\sum_j A_{ij}\\, x_j\\right)}_{\\text{interaction}}\\]\nwhere \\(\\tilde{S}(x) = (1 + e^{-x})^{-1}\\) is the logistic function. The nonlinearity acts on the sum of weighted neighbor states — no per-edge rate constants, no multiplicative aggregation.\nMetabolismGraph — Metabolic kinetics:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i(c_i - c_i^{\\text{baseline}})}_{\\text{self-dynamics}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}}_{\\text{interaction}}\\]\nKey differences:\n\n\n\n\n\n\n\n\n\nYu et al. (gene regulation)\nMetabolismGraph (metabolism)\n\n\n\n\nGraph\nMonopartite (gene → gene)\nBipartite (metabolite ↔︎ reaction)\n\n\nAggregation\n\\(\\tilde{S}(\\sum_j A_{ij} x_j)\\) — logistic of sum\n\\(\\sum_j S_{ij} k_j \\prod_k c_k^{s_{kj}}\\) — sum of products\n\n\nRate constants\nSingle global \\(\\beta\\)\nPer-reaction \\(k_j\\) (256 parameters)\n\n\nNonlinearity\nBounded logistic \\(\\tilde{S} \\in [0,1]\\)\nUnbounded power law \\(c^s\\)\n\n\nHigher-order\nImplicit: \\(\\partial \\dot{x}_i / \\partial x_j\\) depends on all neighbors via \\(\\tilde{S}\\)\nExplicit: mass-action products couple substrates within each reaction\n\n\n\nThe gene regulation model has no per-reaction aggregation step — it sums all neighbor states into a single scalar, then applies a saturating nonlinearity. MetabolismGraph instead computes a separate rate for each reaction (multiplicative aggregation of substrate concentrations), then sums the stoichiometric contributions. This reflects a fundamental difference between gene regulation (bounded transcriptional response) and metabolism (unbounded mass-action kinetics)."
  },
  {
    "objectID": "model.html#the-inverse-problem",
    "href": "model.html#the-inverse-problem",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nGiven observed concentration time series \\(\\mathbf{C}(t)\\) and the bipartite graph structure, the goal is to recover the model parameters that generated the dynamics. A graph neural network (GNN) operates on the bipartite metabolite–reaction graph and learns three components:\n\nLearned Components\n1. Substrate function — \\(\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\)\nAn MLP that maps each substrate concentration \\(c_k\\) and its stoichiometric coefficient \\(|S_{kj}|\\) to the substrate’s contribution to the reaction rate. In the ground truth, this function computes the power-law \\(c_k^{|S_{kj}|}\\). The learned MLP should recover this mapping:\n\\[\n\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|) \\approx c_k^{|S_{kj}|}\n\\]\n2. Homeostasis function — \\(\\text{MLP}_{\\text{node}}(c_i)\\)\nAn MLP that maps each metabolite’s concentration to a per-metabolite correction term, capturing homeostatic regulation. In the ground truth, this is a linear function with small magnitude:\n\\[\n\\text{MLP}_{\\text{node}}(c_i) \\approx -\\lambda_{\\text{type}(i)} \\cdot (c_i - c_i^{\\text{baseline}})\n\\]\n3. Rate constants — \\(k_j\\)\nA vector of 256 learnable per-reaction rate constants (in log-space). These scale the reaction fluxes and must be recovered up to the correct global scale.\n\n\nGNN Forward Pass\nThe GNN assembles these learned components into the full dynamics prediction on the bipartite graph:\n\\[\n\\widehat{\\frac{dc_i}{dt}} = \\text{MLP}_{\\text{node}}(c_i) + \\sum_{j=1}^{m} S_{ij} \\cdot k_j \\cdot \\underset{k \\in \\text{sub}(j)}{\\text{aggr}} \\; \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\n\\]\nwhere \\(\\text{aggr}\\) is either sum (additive) or product (multiplicative, for mass-action kinetics). The model is trained by minimizing the prediction error on \\(dc/dt\\) computed from the observed concentration trajectories.\n\n\nLearning Modes\n\n\n\n\n\n\n\n\n\nMode\nS matrix\nPrimary metric\nChallenge\n\n\n\n\nS learning\nLearnable\nstoichiometry \\(R^2\\)\nRecovering integer coefficients and sparsity\n\n\nS given\nFrozen from GT\nrate constants \\(R^2\\)\nDisentangling \\(k\\), \\(\\text{MLP}_{\\text{sub}}\\), \\(\\text{MLP}_{\\text{node}}\\)\n\n\n\nA key challenge is scale ambiguity: the product \\(k_j \\cdot \\text{MLP}_{\\text{sub}}\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). Regularization (\\(\\texttt{coeff\\_k\\_center}\\)) anchors the global scale of \\(k\\) to break this degeneracy."
  }
]