[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the high-rank oscillatory regime (activity rank $$50): 100 metabolites, 256 reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n16 iterations completed across 2 blocks. The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 48, confirming a high-rank regime where most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). The matrix is sparse with autocatalytic cycle motifs.\n\n\n\n\n\n\n\n\n\nThe best rate constant recovery reached \\(R^2 = 0.54\\) (iteration 5), with:\n\n\n\nParameter\nValue\n\n\n\n\nlr_k\n0.001\n\n\nlr_node\n0.001\n\n\nlr_sub\n0.0005\n\n\nbatch_size\n8\n\n\nn_epochs\n1\n\n\ndata_augmentation_loop\n1000\n\n\ncoeff_k_center\n5.0\n\n\ncoeff_MLP_sub_diff\n5\n\n\ncoeff_MLP_node_L1\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Iteration 5 (best, \\(R^2 = 0.54\\)): Scatter plot of learned vs true \\(\\log_{10}(k_j)\\) for 256 reactions. The main cluster follows the diagonal but ~40 outlier reactions collapse to \\(\\log k \\approx -7\\), far from the true range \\([-2, -1]\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline, \\(R^2 = 0.09\\)): With lr_k=0.005 (from prior low-rank exploration), the rate constants barely separate. This established that the high-rank regime requires lower lr_k.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Best result (iter 5) — Left: MLP\\(_\\text{sub}\\) (substrate function) learns \\(c^1\\) well (solid blue vs dashed GT) but deviates from \\(c^2\\) (solid orange vs dashed). Right: MLP\\(_\\text{node}\\) (homeostasis) stays flat at zero (solid lines) despite the ground truth showing linear decreasing functions (dashed). This MLP\\(_\\text{node}\\) learning failure persists across all 16 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Kinograph comparison (iter 5): Ground truth (left) vs GNN prediction (right). Despite recovering rate constants (\\(R^2 = 0.54\\)), the dynamics prediction is poor (Pearson = 0.016). The GNN produces saturated flat bands, indicating the learned model does not capture the oscillatory dynamics. The residuals (bottom left) and scatter (bottom right) confirm the mismatch.\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 17))\nr2 = [0.094, 0.478, 0.081, 0.363, 0.537, 0.059, 0.417, 0.040,\n      0.291, 0.124, 0.469, 0.062, 0.523, 0.251, 0.335, 0.428]\n\nlabels = [\n    'baseline\\nlr_k=0.005',\n    'lr_k=0.002',\n    'lr_node=0.002',\n    'lr_sub=0.001',\n    'lr_k=0.001\\n(BEST)',\n    'lr_k=0.0005',\n    'lr_sub=0.001',\n    'lr_k=0.003',\n    'aug=1500',\n    'lr_node=0.002',\n    'L1_node=0.5',\n    'lr_k=0.0015',\n    'L1_node=0.0',\n    'recurrent\\nts=4',\n    'seed=56\\n(replication)',\n    'k_center=10',\n]\n\ncolors = []\nfor r in r2:\n    if r &gt;= 0.5:\n        colors.append('#2ecc71')\n    elif r &gt;= 0.3:\n        colors.append('#f39c12')\n    else:\n        colors.append('#e74c3c')\n\nfig, ax = plt.subplots(figsize=(14, 5))\nbars = ax.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5)\nax.axhline(y=0.537, color='#2ecc71', linestyle='--', alpha=0.5, label='Best R² = 0.54')\nax.axvspan(0.5, 4.5, alpha=0.05, color='blue', label='Batch 1')\nax.axvspan(4.5, 8.5, alpha=0.05, color='orange', label='Batch 2')\nax.axvspan(8.5, 12.5, alpha=0.05, color='green', label='Batch 3')\nax.axvspan(12.5, 16.5, alpha=0.05, color='purple', label='Batch 4')\n\nfor i, (it, v, lab) in enumerate(zip(iters, r2, labels)):\n    ax.text(it, v + 0.015, f'{v:.2f}', ha='center', va='bottom', fontsize=7, fontweight='bold')\n    ax.text(it, -0.06, lab, ha='center', va='top', fontsize=5.5, rotation=0)\n\nax.set_xlabel('Iteration')\nax.set_ylabel('rate_constants_R²')\nax.set_ylim(-0.15, 0.65)\nax.set_xticks(iters)\nax.legend(loc='upper right', fontsize=8)\nax.set_title('UCB Exploration: Rate Constant Recovery (High-Rank Regime)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Rate constants R² across 16 iterations. Color: green (R² ≥ 0.5), orange (0.3–0.5), red (&lt; 0.3). Dashed line marks the best result.\n\n\n\n\n\n\n\n\n\n\nlr_k = 0.001 is an extremely narrow optimum for the rate constant learning rate. Even a 50% perturbation destroys performance:\n\n\n\nlr_k\n\\(R^2\\)\nStatus\n\n\n\n\n0.0005\n0.06\nfailed\n\n\n0.001\n0.54\nbest\n\n\n0.0015\n0.06\nfailed\n\n\n0.002\n0.48\npartial\n\n\n0.003\n0.04\nfailed\n\n\n0.005\n0.09\nfailed\n\n\n\nThis sharp peak suggests the optimization landscape has a narrow valley: too high and \\(k\\) values overshoot, too low and the MLPs compensate before \\(k\\) can converge.\n\n\n\nAcross all 16 iterations, MLP\\(_\\text{node}\\) (the homeostatic regulation function) stays flat at zero (Figure 5, right panel). This persists regardless of:\n\nHigher lr_node (0.002): no effect\nReduced L1 penalty (coeff_MLP_node_L1 = 0.5): no effect\nRemoved L1 penalty (coeff_MLP_node_L1 = 0.0): no effect\n\nThe homeostatic terms (\\(\\lambda \\sim 0.001\\)–\\(0.002\\)) are small relative to the reaction rate terms (\\(k \\sim 0.01\\)–\\(0.1\\)), so the gradient signal flowing to MLP\\(_\\text{node}\\) may be insufficient to escape the zero initialization.\n\n\n\nApproximately 40 out of 256 reactions consistently learn incorrect \\(\\log_{10}(k)\\) values (at \\(-6\\) to \\(-8\\), far from the true range \\([-2, -1]\\)). These outliers are visible in Figure 3 and are not reduced by:\n\nStronger coeff_k_center (10.0 vs 5.0)\nLonger training (data_augmentation_loop = 1500)\n\nThey may correspond to reactions whose substrate concentrations provide insufficient gradient signal, or reactions involved in particular network motifs where the loss landscape has degenerate solutions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Iter 13 (\\(R^2 = 0.52\\), L1_node=0): Similar pattern to best result. Removing MLP\\(_\\text{node}\\) L1 penalty entirely did not help homeostasis learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Iter 14 (\\(R^2 = 0.25\\), recurrent, ts=4): Multi-step rollout training with time_step=4 worsened recovery. More outlier reactions and weaker main cluster.\n\n\n\n\n\n\nMulti-step rollout training (recurrent_training=true, time_step=4) was tested to break the degeneracy between correct and incorrect \\(k\\) values. With lr_k=0.001, it worsened \\(R^2\\) from 0.54 to 0.25. The 4-step integration amplified gradients too aggressively.\nA gentler approach (time_step=2, lr_k=0.0007) is currently being tested.\n\n\n\nThe same configuration yields \\(R^2\\) from 0.34 to 0.54 depending on the random seed (\\(\\Delta R^2 \\sim 0.2\\)). This implies the optimization landscape has many local minima, and results must be interpreted with caution.\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstrategies = {\n    'lr_k': [(0.005, 0.094), (0.002, 0.478), (0.001, 0.537), (0.0005, 0.059), (0.003, 0.040), (0.0015, 0.062)],\n    'lr_node': [(0.001, 0.537), (0.002, 0.081), (0.002, 0.124)],\n    'lr_sub': [(0.0005, 0.537), (0.001, 0.363), (0.001, 0.417)],\n    'coeff_node_L1': [(1.0, 0.537), (0.5, 0.469), (0.0, 0.523)],\n    'coeff_k_center': [(5.0, 0.537), (10.0, 0.428)],\n    'data_aug_loop': [(1000, 0.537), (1500, 0.291)],\n    'recurrent (ts)': [(1, 0.537), (4, 0.251)],\n    'seed only': [(46, 0.537), (56, 0.335)],\n}\n\nfig, axes = plt.subplots(2, 4, figsize=(14, 6))\naxes = axes.flatten()\n\nfor idx, (param, points) in enumerate(strategies.items()):\n    ax = axes[idx]\n    xs = [p[0] for p in points]\n    ys = [p[1] for p in points]\n    colors = ['#2ecc71' if y &gt;= 0.5 else '#f39c12' if y &gt;= 0.3 else '#e74c3c' for y in ys]\n    ax.scatter(xs, ys, c=colors, s=60, edgecolors='black', linewidth=0.5, zorder=3)\n    ax.axhline(y=0.537, color='gray', linestyle='--', alpha=0.3)\n    ax.set_title(param, fontsize=9)\n    ax.set_ylabel('R²', fontsize=8)\n    ax.set_ylim(-0.05, 0.65)\n    ax.tick_params(labelsize=7)\n    if param == 'lr_k':\n        ax.set_xscale('log')\n\nplt.suptitle('Parameter Sensitivity Analysis', fontsize=12, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Parameter sensitivity analysis across 16 iterations. Each panel shows R² vs one parameter dimension. Green: R² ≥ 0.5, orange: 0.3–0.5, red: &lt; 0.3.\n\n\n\n\n\n\n\n\nThe exploration is entering batch 5 (iterations 17–20) with four new strategies:\n\n\n\n\n\n\n\n\nSlot\nStrategy\nMutation\n\n\n\n\n0\nexploit\nlr_node: 0.001 \\(\\to\\) 0.005, L1_node=0 (force MLP\\(_\\text{node}\\) activation)\n\n\n1\nexploit\nrecurrent_training=true, time_step=2, lr_k=0.0007 (gentler rollout)\n\n\n2\nexplore\nseed replication (seed=60, baseline config)\n\n\n3\nprinciple-test\ncoeff_MLP_sub_diff: 5 \\(\\to\\) 10 (stronger monotonicity), L1_node=0\n\n\n\n\n\n\nThe following parameter dimensions have not yet been tested and represent the next frontier:\n\nMLP architecture: hidden_dim_sub, n_layers_sub, hidden_dim_node, n_layers_node (all at default 64/3). Smaller MLPs could act as implicit regularization.\nBatch size: All runs used batch_size=8. Larger batches (16, 32) may give smoother gradients.\nn_epochs &gt; 1: Multi-epoch training has not been tested in the high-rank regime.\n\n\n\n\n\nMLP\\(_\\text{node}\\) learning failure: Is the homeostatic signal too weak relative to reaction terms, or is there an architectural barrier? Would lr_node = 0.005–0.01 finally activate it?\nOutlier reactions: What structural property do the ~40 outlier reactions share? Are they in specific network motifs?\nRecurrent training: Can a gentler rollout (time_step=2, reduced lr_k) break the degeneracy without destabilizing training?\nSeed dependence: Is the \\(\\Delta R^2 \\sim 0.2\\) variance inherent to the loss landscape, or can better initialization reduce it?"
  },
  {
    "objectID": "results.html#high-rank-oscillatory-regime-rate-constant-recovery",
    "href": "results.html#high-rank-oscillatory-regime-rate-constant-recovery",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the high-rank oscillatory regime (activity rank $$50): 100 metabolites, 256 reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n16 iterations completed across 2 blocks. The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 48, confirming a high-rank regime where most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). The matrix is sparse with autocatalytic cycle motifs.\n\n\n\n\n\n\n\n\n\nThe best rate constant recovery reached \\(R^2 = 0.54\\) (iteration 5), with:\n\n\n\nParameter\nValue\n\n\n\n\nlr_k\n0.001\n\n\nlr_node\n0.001\n\n\nlr_sub\n0.0005\n\n\nbatch_size\n8\n\n\nn_epochs\n1\n\n\ndata_augmentation_loop\n1000\n\n\ncoeff_k_center\n5.0\n\n\ncoeff_MLP_sub_diff\n5\n\n\ncoeff_MLP_node_L1\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Iteration 5 (best, \\(R^2 = 0.54\\)): Scatter plot of learned vs true \\(\\log_{10}(k_j)\\) for 256 reactions. The main cluster follows the diagonal but ~40 outlier reactions collapse to \\(\\log k \\approx -7\\), far from the true range \\([-2, -1]\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline, \\(R^2 = 0.09\\)): With lr_k=0.005 (from prior low-rank exploration), the rate constants barely separate. This established that the high-rank regime requires lower lr_k.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Best result (iter 5) — Left: MLP\\(_\\text{sub}\\) (substrate function) learns \\(c^1\\) well (solid blue vs dashed GT) but deviates from \\(c^2\\) (solid orange vs dashed). Right: MLP\\(_\\text{node}\\) (homeostasis) stays flat at zero (solid lines) despite the ground truth showing linear decreasing functions (dashed). This MLP\\(_\\text{node}\\) learning failure persists across all 16 iterations.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Kinograph comparison (iter 5): Ground truth (left) vs GNN prediction (right). Despite recovering rate constants (\\(R^2 = 0.54\\)), the dynamics prediction is poor (Pearson = 0.016). The GNN produces saturated flat bands, indicating the learned model does not capture the oscillatory dynamics. The residuals (bottom left) and scatter (bottom right) confirm the mismatch.\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 17))\nr2 = [0.094, 0.478, 0.081, 0.363, 0.537, 0.059, 0.417, 0.040,\n      0.291, 0.124, 0.469, 0.062, 0.523, 0.251, 0.335, 0.428]\n\nlabels = [\n    'baseline\\nlr_k=0.005',\n    'lr_k=0.002',\n    'lr_node=0.002',\n    'lr_sub=0.001',\n    'lr_k=0.001\\n(BEST)',\n    'lr_k=0.0005',\n    'lr_sub=0.001',\n    'lr_k=0.003',\n    'aug=1500',\n    'lr_node=0.002',\n    'L1_node=0.5',\n    'lr_k=0.0015',\n    'L1_node=0.0',\n    'recurrent\\nts=4',\n    'seed=56\\n(replication)',\n    'k_center=10',\n]\n\ncolors = []\nfor r in r2:\n    if r &gt;= 0.5:\n        colors.append('#2ecc71')\n    elif r &gt;= 0.3:\n        colors.append('#f39c12')\n    else:\n        colors.append('#e74c3c')\n\nfig, ax = plt.subplots(figsize=(14, 5))\nbars = ax.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5)\nax.axhline(y=0.537, color='#2ecc71', linestyle='--', alpha=0.5, label='Best R² = 0.54')\nax.axvspan(0.5, 4.5, alpha=0.05, color='blue', label='Batch 1')\nax.axvspan(4.5, 8.5, alpha=0.05, color='orange', label='Batch 2')\nax.axvspan(8.5, 12.5, alpha=0.05, color='green', label='Batch 3')\nax.axvspan(12.5, 16.5, alpha=0.05, color='purple', label='Batch 4')\n\nfor i, (it, v, lab) in enumerate(zip(iters, r2, labels)):\n    ax.text(it, v + 0.015, f'{v:.2f}', ha='center', va='bottom', fontsize=7, fontweight='bold')\n    ax.text(it, -0.06, lab, ha='center', va='top', fontsize=5.5, rotation=0)\n\nax.set_xlabel('Iteration')\nax.set_ylabel('rate_constants_R²')\nax.set_ylim(-0.15, 0.65)\nax.set_xticks(iters)\nax.legend(loc='upper right', fontsize=8)\nax.set_title('UCB Exploration: Rate Constant Recovery (High-Rank Regime)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Rate constants R² across 16 iterations. Color: green (R² ≥ 0.5), orange (0.3–0.5), red (&lt; 0.3). Dashed line marks the best result.\n\n\n\n\n\n\n\n\n\n\nlr_k = 0.001 is an extremely narrow optimum for the rate constant learning rate. Even a 50% perturbation destroys performance:\n\n\n\nlr_k\n\\(R^2\\)\nStatus\n\n\n\n\n0.0005\n0.06\nfailed\n\n\n0.001\n0.54\nbest\n\n\n0.0015\n0.06\nfailed\n\n\n0.002\n0.48\npartial\n\n\n0.003\n0.04\nfailed\n\n\n0.005\n0.09\nfailed\n\n\n\nThis sharp peak suggests the optimization landscape has a narrow valley: too high and \\(k\\) values overshoot, too low and the MLPs compensate before \\(k\\) can converge.\n\n\n\nAcross all 16 iterations, MLP\\(_\\text{node}\\) (the homeostatic regulation function) stays flat at zero (Figure 5, right panel). This persists regardless of:\n\nHigher lr_node (0.002): no effect\nReduced L1 penalty (coeff_MLP_node_L1 = 0.5): no effect\nRemoved L1 penalty (coeff_MLP_node_L1 = 0.0): no effect\n\nThe homeostatic terms (\\(\\lambda \\sim 0.001\\)–\\(0.002\\)) are small relative to the reaction rate terms (\\(k \\sim 0.01\\)–\\(0.1\\)), so the gradient signal flowing to MLP\\(_\\text{node}\\) may be insufficient to escape the zero initialization.\n\n\n\nApproximately 40 out of 256 reactions consistently learn incorrect \\(\\log_{10}(k)\\) values (at \\(-6\\) to \\(-8\\), far from the true range \\([-2, -1]\\)). These outliers are visible in Figure 3 and are not reduced by:\n\nStronger coeff_k_center (10.0 vs 5.0)\nLonger training (data_augmentation_loop = 1500)\n\nThey may correspond to reactions whose substrate concentrations provide insufficient gradient signal, or reactions involved in particular network motifs where the loss landscape has degenerate solutions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Iter 13 (\\(R^2 = 0.52\\), L1_node=0): Similar pattern to best result. Removing MLP\\(_\\text{node}\\) L1 penalty entirely did not help homeostasis learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Iter 14 (\\(R^2 = 0.25\\), recurrent, ts=4): Multi-step rollout training with time_step=4 worsened recovery. More outlier reactions and weaker main cluster.\n\n\n\n\n\n\nMulti-step rollout training (recurrent_training=true, time_step=4) was tested to break the degeneracy between correct and incorrect \\(k\\) values. With lr_k=0.001, it worsened \\(R^2\\) from 0.54 to 0.25. The 4-step integration amplified gradients too aggressively.\nA gentler approach (time_step=2, lr_k=0.0007) is currently being tested.\n\n\n\nThe same configuration yields \\(R^2\\) from 0.34 to 0.54 depending on the random seed (\\(\\Delta R^2 \\sim 0.2\\)). This implies the optimization landscape has many local minima, and results must be interpreted with caution.\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstrategies = {\n    'lr_k': [(0.005, 0.094), (0.002, 0.478), (0.001, 0.537), (0.0005, 0.059), (0.003, 0.040), (0.0015, 0.062)],\n    'lr_node': [(0.001, 0.537), (0.002, 0.081), (0.002, 0.124)],\n    'lr_sub': [(0.0005, 0.537), (0.001, 0.363), (0.001, 0.417)],\n    'coeff_node_L1': [(1.0, 0.537), (0.5, 0.469), (0.0, 0.523)],\n    'coeff_k_center': [(5.0, 0.537), (10.0, 0.428)],\n    'data_aug_loop': [(1000, 0.537), (1500, 0.291)],\n    'recurrent (ts)': [(1, 0.537), (4, 0.251)],\n    'seed only': [(46, 0.537), (56, 0.335)],\n}\n\nfig, axes = plt.subplots(2, 4, figsize=(14, 6))\naxes = axes.flatten()\n\nfor idx, (param, points) in enumerate(strategies.items()):\n    ax = axes[idx]\n    xs = [p[0] for p in points]\n    ys = [p[1] for p in points]\n    colors = ['#2ecc71' if y &gt;= 0.5 else '#f39c12' if y &gt;= 0.3 else '#e74c3c' for y in ys]\n    ax.scatter(xs, ys, c=colors, s=60, edgecolors='black', linewidth=0.5, zorder=3)\n    ax.axhline(y=0.537, color='gray', linestyle='--', alpha=0.3)\n    ax.set_title(param, fontsize=9)\n    ax.set_ylabel('R²', fontsize=8)\n    ax.set_ylim(-0.05, 0.65)\n    ax.tick_params(labelsize=7)\n    if param == 'lr_k':\n        ax.set_xscale('log')\n\nplt.suptitle('Parameter Sensitivity Analysis', fontsize=12, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Parameter sensitivity analysis across 16 iterations. Each panel shows R² vs one parameter dimension. Green: R² ≥ 0.5, orange: 0.3–0.5, red: &lt; 0.3.\n\n\n\n\n\n\n\n\nThe exploration is entering batch 5 (iterations 17–20) with four new strategies:\n\n\n\n\n\n\n\n\nSlot\nStrategy\nMutation\n\n\n\n\n0\nexploit\nlr_node: 0.001 \\(\\to\\) 0.005, L1_node=0 (force MLP\\(_\\text{node}\\) activation)\n\n\n1\nexploit\nrecurrent_training=true, time_step=2, lr_k=0.0007 (gentler rollout)\n\n\n2\nexplore\nseed replication (seed=60, baseline config)\n\n\n3\nprinciple-test\ncoeff_MLP_sub_diff: 5 \\(\\to\\) 10 (stronger monotonicity), L1_node=0\n\n\n\n\n\n\nThe following parameter dimensions have not yet been tested and represent the next frontier:\n\nMLP architecture: hidden_dim_sub, n_layers_sub, hidden_dim_node, n_layers_node (all at default 64/3). Smaller MLPs could act as implicit regularization.\nBatch size: All runs used batch_size=8. Larger batches (16, 32) may give smoother gradients.\nn_epochs &gt; 1: Multi-epoch training has not been tested in the high-rank regime.\n\n\n\n\n\nMLP\\(_\\text{node}\\) learning failure: Is the homeostatic signal too weak relative to reaction terms, or is there an architectural barrier? Would lr_node = 0.005–0.01 finally activate it?\nOutlier reactions: What structural property do the ~40 outlier reactions share? Are they in specific network motifs?\nRecurrent training: Can a gentler rollout (time_step=2, reduced lr_k) break the degeneracy without destabilizing training?\nSeed dependence: Is the \\(\\Delta R^2 \\sim 0.2\\) variance inherent to the loss landscape, or can better initialization reduce it?"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#metabolic-networks-as-bipartite-graphs",
    "href": "model.html#metabolic-networks-as-bipartite-graphs",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#the-stoichiometric-matrix",
    "href": "model.html#the-stoichiometric-matrix",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Stoichiometric Matrix",
    "text": "The Stoichiometric Matrix\nThe stoichiometric matrix \\(\\mathbf{S}\\) is an \\((n_{\\text{metabolites}} \\times n_{\\text{reactions}})\\) matrix where entry \\(S_{ij}\\) indicates how metabolite \\(i\\) participates in reaction \\(j\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix}\n-1 & 0 & \\cdots \\\\\n-1 & +1 & \\cdots \\\\\n+2 & -1 & \\cdots \\\\\n+1 & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\]\nProperties of S:\n\nSparse: most entries are zero (each reaction involves only 2-6 metabolites)\nInteger-valued: entries are typically in \\(\\{-2, -1, 0, +1, +2\\}\\)\nMass conservation: column sums should be zero for balanced reactions\n\nExample:\nConsider the diagram above with 2 reactions:\n\\[\n\\begin{aligned}\n\\text{R1}: \\quad & \\text{Glucose} + \\text{ATP} \\longrightarrow 2\\,\\text{Pyruvate} + \\text{ADP} \\\\\n\\text{R2}: \\quad & \\text{Pyruvate} \\longrightarrow \\text{ATP}\n\\end{aligned}\n\\]\nThe corresponding stoichiometric matrix is:\n\\[\n\\begin{array}{c|cc}\n& \\text{R1} & \\text{R2} \\\\\n\\hline\n\\text{Glucose} & -1 & 0 \\\\\n\\text{ATP} & -1 & +1 \\\\\n\\text{Pyruvate} & +2 & -1 \\\\\n\\text{ADP} & +1 & 0\n\\end{array}\n\\]"
  },
  {
    "objectID": "model.html#reaction-kinetics",
    "href": "model.html#reaction-kinetics",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Reaction Kinetics",
    "text": "Reaction Kinetics\n\nMass-Action Law\nThe rate of a chemical reaction is proportional to the product of the concentrations of its substrates, each raised to the power of its stoichiometric coefficient. For a reaction \\(j\\) with substrates \\(k\\):\n\\[v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nThis is the law of mass action (Guldberg & Waage, 1864): a reaction proceeds faster when its substrates are more abundant. The exponent \\(|S_{kj}|\\) reflects how many molecules of substrate \\(k\\) are consumed — a reaction requiring 2 molecules of ATP depends quadratically on ATP concentration (\\(c_{\\text{ATP}}^2\\)), while a reaction consuming 1 molecule depends linearly (\\(c_{\\text{ATP}}^1\\)).\n\n\nSubstrate Aggregation\nEach reaction consumes multiple substrates. The aggregation rule determines how substrate contributions combine to form the reaction rate:\n\n\n\n\n\n\n\n\nAggregation\nFormula\nPhysical meaning\n\n\n\n\nMultiplicative (product)\n\\(v_j = k_j \\cdot \\prod_k c_k^{\\|S_{kj}\\|}\\)\nTrue mass-action: all substrates must be present simultaneously. Rate drops to zero if any substrate is absent.\n\n\nAdditive (sum)\n\\(v_j = k_j \\cdot \\sum_k c_k^{\\|S_{kj}\\|}\\)\nApproximate: substrates contribute independently. Useful when reactions are not strictly mass-action (e.g., enzyme-mediated).\n\n\n\nMultiplicative aggregation is the physically correct form for mass-action kinetics — it encodes the requirement that a reaction can only proceed if all its substrates are available. It also produces richer dynamics: multiplicative coupling between metabolites creates nonlinear feedback loops that can sustain oscillations.\nAdditive aggregation is a simplification where each substrate contributes independently to the rate. It cannot produce sustained oscillations from autocatalytic cycles because it lacks the multiplicative coupling needed for positive feedback."
  },
  {
    "objectID": "model.html#model-evolution",
    "href": "model.html#model-evolution",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Model Evolution",
    "text": "Model Evolution\n\n1. Pure Reaction\nPure reaction dynamics without homeostasis, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nPure reaction dynamics: concentrations evolve under additive mass-action kinetics without homeostasis. Most metabolites equilibrate within ~100 time steps.\n\n\n\n\n2. Homeostasis\nWith homeostatic regulation pulling concentrations toward baseline, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}}_{\\text{reactions}}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.01\n\n\n\\(c^{\\text{baseline}}\\)\n5.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nHomeostatic regulation: concentrations are pulled toward a baseline (\\(c^{\\text{baseline}} = 5.0\\)) by a linear restoring force. Dynamics converge to steady state.\n\n\n\n\n3. Oscillatory\nAutocatalytic cycles with mass-action kinetics (multiplicative aggregation) for sustained oscillations:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nWith multiplicative aggregation, autocatalytic cycles create positive feedback: in \\(A + B \\to 2B\\), the rate \\(v = k \\cdot c_A \\cdot c_B\\) increases with both \\(A\\) and \\(B\\), so producing more \\(B\\) accelerates the reaction — a nonlinear feedback loop that sustains oscillations when cycles are closed (\\(A \\to B \\to C \\to A\\)).\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n512\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nAggregation\nProduct (multiplicative)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.0}, 10^{-1}]\\)\n\n\nInitial concentrations\n[1.0, 9.0]\n\n\nFlux limiting\nDisabled\n\n\n\n\n\n\nOscillatory dynamics: autocatalytic 3-cycles with multiplicative aggregation produce sustained oscillations. 512 reactions, rate constant range \\([10^{-2.0}, 10^{-1}]\\), activity rank 68."
  },
  {
    "objectID": "model.html#activity-rank",
    "href": "model.html#activity-rank",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Activity Rank",
    "text": "Activity Rank\nTo quantify the complexity of concentration dynamics, we compute the activity rank using singular value decomposition (SVD) of the concentration matrix \\(\\mathbf{C} \\in \\mathbb{R}^{T \\times n}\\) (time frames × metabolites).\nThe rank at 99% variance is the number of singular values needed to capture 99% of the total variance:\n\\[\\text{rank}_{99} = \\min \\left\\{ k : \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.99 \\right\\}\\]\nInterpretation:\n\nLow rank (1-5): concentrations are highly correlated, dynamics are simple (equilibration or uniform decay)\nHigh rank (&gt;20): metabolites evolve independently with rich, complex dynamics\n\n\nActivity Rank and GNN Training Dataset\nThe oscillatory config above (512 reactions, \\(k_j \\in [10^{-2.0}, 10^{-1}]\\)) has activity rank 68. For GNN training, we use a reduced variant with 256 reactions (activity rank 47) — complex enough to test rate constant recovery while keeping the problem tractable:\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.0}, 10^{-1}]\\)\n\n\nAll other parameters\nSame as oscillatory above\n\n\n\n\n\n\nOscillatory dynamics with 256 reactions (activity rank 47). This is the primary dataset used for GNN training and rate constant recovery."
  },
  {
    "objectID": "model.html#summary-the-full-model",
    "href": "model.html#summary-the-full-model",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Summary: The Full Model",
    "text": "Summary: The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate follows mass-action kinetics:\n\\[\nv_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\n\\]"
  },
  {
    "objectID": "model.html#the-inverse-problem",
    "href": "model.html#the-inverse-problem",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nGiven observed concentration time series \\(\\mathbf{C}(t)\\) and the bipartite graph structure, the goal is to recover the model components that generated the dynamics. A graph neural network (GNN) operates on the bipartite metabolite–reaction graph and learns two functions and a set of scalar parameters:\n\nWhat the GNN Learns\nThe forward model is:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\underset{k \\in \\text{sub}(j)}{\\text{aggr}} \\; \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nThe GNN replaces the known functions with learnable MLPs, and the rate constants with learnable parameters. It must recover all three simultaneously from concentration data alone:\n1. Substrate function \\(f_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\)\nA neural network that learns how each substrate concentration contributes to the reaction rate. The ground-truth function is the mass-action power law \\(c_k^{|S_{kj}|}\\), but the GNN does not know this — it must discover the functional form from data:\n\\[\n\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|) \\;\\overset{?}{\\approx}\\; c_k^{|S_{kj}|}\n\\]\nThis is a function discovery problem: the MLP receives concentration and stoichiometric coefficient as inputs and must learn to output the correct power-law relationship. Since the stoichiometric coefficients are typically 1 or 2, the MLP must learn to distinguish between linear (\\(c^1\\)) and quadratic (\\(c^2\\)) dependencies.\n2. Homeostasis function \\(f_{\\text{node}} \\to \\text{MLP}_{\\text{node}}(c_i)\\)\nA neural network that learns the per-metabolite self-regulation term. The ground truth is a linear function \\(-\\lambda_{\\text{type}(i)} (c_i - c_i^{\\text{baseline}})\\) with small magnitude, but the GNN must discover this from data:\n\\[\n\\text{MLP}_{\\text{node}}(c_i) \\;\\overset{?}{\\approx}\\; -\\lambda_{\\text{type}(i)} \\cdot (c_i - c_i^{\\text{baseline}})\n\\]\nThis function captures how each metabolite is regulated independently of reactions — pulling concentrations back toward a baseline level. The challenge is that homeostatic forces are small compared to reaction rates, so \\(\\text{MLP}_{\\text{node}}\\) must learn a subtle signal without absorbing information that belongs to the reaction terms.\n3. Rate constants \\(k_j\\) (256 learnable scalars)\nPer-reaction rate constants learned in log-space. Unlike the MLPs, these are not functions but a vector of 256 scalar parameters — one per reaction — that scale the reaction fluxes.\n\n\nIdentifiability Challenges\nThe three components interact and can compensate for each other:\n\nScale ambiguity: \\(k_j \\cdot \\text{MLP}_{\\text{sub}}\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). Without anchoring, the MLP can absorb a global scale factor and shift all \\(k\\) values. Regularization (\\(\\texttt{coeff\\_k\\_center}\\)) anchors \\(\\text{mean}(\\log k)\\) to the known range center.\nFunction compensation: If \\(\\text{MLP}_{\\text{sub}}\\) learns a wrong functional form (e.g., \\(c^{1.5}\\) instead of \\(c^2\\)), the rate constants can partially compensate by adjusting their values. This leads to degenerate solutions with high prediction accuracy but poor parameter recovery.\nHomeostasis absorption: \\(\\text{MLP}_{\\text{node}}\\) can grow large and absorb dynamics that should be explained by the reaction terms, masking the true rate constants.\n\n\n\nLearning Modes\n\n\n\n\n\n\n\n\n\nMode\nS matrix\nPrimary metric\nChallenge\n\n\n\n\nS learning\nLearnable\nstoichiometry \\(R^2\\)\nRecovering integer coefficients and sparsity\n\n\nS given\nFrozen from GT\nrate constants \\(R^2\\)\nDisentangling \\(k\\), \\(\\text{MLP}_{\\text{sub}}\\), \\(\\text{MLP}_{\\text{node}}\\)"
  },
  {
    "objectID": "gnn-llm-memory.html",
    "href": "gnn-llm-memory.html",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#the-exploration-loop",
    "href": "gnn-llm-memory.html#the-exploration-loop",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#training-scheme",
    "href": "gnn-llm-memory.html#training-scheme",
    "title": "GNN-LLM-Memory",
    "section": "Training Scheme",
    "text": "Training Scheme\nThe GNN is trained by minimizing the prediction error on \\(dc/dt\\):\n\\[\n\\mathcal{L} = \\sum_{\\text{frames}} \\left\\| \\frac{dc}{dt}_{\\text{pred}} - \\frac{dc}{dt}_{\\text{GT}} \\right\\|_2 + \\mathcal{R}\n\\]\nwhere \\(\\mathcal{R}\\) is the sum of regularization terms described below.\n\nSeparate Learning Rates\nEach model component has its own learning rate to control the balance between parameter groups:\n\n\n\n\n\n\n\n\n\nComponent\nConfig key\nControls\nTypical range\n\n\n\n\nRate constants \\(k_j\\)\nlearning_rate_k\nHow fast k values are updated\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{node}}\\) (homeostasis)\nlearning_rate_node\nHomeostasis function learning speed\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{sub}}\\) (substrate)\nlearning_rate_sub\nSubstrate function learning speed\n1E-4 to 1E-2\n\n\n\nThe learning rate balance is critical:\n\nlr_k too high: \\(k\\) values overshoot, oscillate, or converge to wrong values\nlr_k too low: \\(k\\) barely moves, MLPs compensate\nlr_node/lr_sub imbalance: one function absorbs capacity meant for the other\n\n\n\nRegularization Terms\nThe total regularization \\(\\mathcal{R}\\) is the sum of the following penalties:\n\nMLP\\(_{\\text{sub}}\\) Monotonicity (coeff_MLP_sub_diff)\nMLP\\(_{\\text{sub}}\\) learns \\(c^s\\) which should be monotonically increasing in concentration. This penalty samples concentration pairs \\((c, c+\\delta)\\) and penalizes cases where the output decreases:\n\\[\n\\mathcal{R}_{\\text{sub\\_diff}} = \\left\\| \\text{ReLU}\\left(\\|\\text{MLP}_{\\text{sub}}(c)\\| - \\|\\text{MLP}_{\\text{sub}}(c+\\delta)\\|\\right) \\right\\|_2 \\cdot \\lambda_{\\text{sub\\_diff}}\n\\]\nWithout this constraint, MLP\\(_{\\text{sub}}\\) can develop non-physical local minima that don’t match the true power law behavior.\n\n\nMLP\\(_{\\text{node}}\\) L1 (coeff_MLP_node_L1)\nPenalizes large MLP\\(_{\\text{node}}\\) output to keep homeostasis values small relative to reaction terms:\n\\[\n\\mathcal{R}_{\\text{node\\_L1}} = \\text{mean}\\left(|\\text{MLP}_{\\text{node}}(c_i, a_i)|\\right) \\cdot \\lambda_{\\text{node\\_L1}}\n\\]\nMLP\\(_{\\text{node}}\\) is initialized to zero output so homeostasis starts inactive. Combined with a reduced learning rate, this prevents MLP\\(_{\\text{node}}\\) from dominating the dynamics. The true homeostatic \\(\\lambda\\) values are small (0.001–0.002), so MLP\\(_{\\text{node}}\\) output should remain small.\n\n\nMLP\\(_{\\text{sub}}\\) Normalization (coeff_MLP_sub_norm)\nBreaks the scale ambiguity between \\(k\\) and MLP\\(_{\\text{sub}}\\) at the source. The product \\(k_j \\cdot \\text{MLP}_{\\text{sub}}(c)\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). This penalty enforces that MLP\\(_{\\text{sub}}\\) outputs 1 at the reference point \\(c=1, |s|=1\\), where the true value \\(c^s = 1^1 = 1\\):\n\\[\n\\mathcal{R}_{\\text{sub\\_norm}} = \\left(\\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\| - 1\\right)^2 \\cdot \\lambda_{\\text{sub\\_norm}}\n\\]\nSince \\(c^s = 1\\) at \\(c=1\\) for any stoichiometry \\(s\\), this pins the MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) to 1 and prevents \\(k\\) from absorbing a global factor.\n\n\nRate Constant Floor (coeff_k_floor)\nPrevents \\(\\log_{10} k_j\\) from drifting far below the physically plausible range. Without this, some reactions develop outlier values (e.g. \\(\\log k = -4\\) when the true range is \\([-2, -1]\\)), which distorts the \\(R^2\\) even when most reactions are well-recovered:\n\\[\n\\mathcal{R}_{\\text{k\\_floor}} = \\sum_j \\text{ReLU}\\left(\\tau - \\log_{10} k_j\\right)^2 \\cdot \\lambda_{\\text{k\\_floor}}\n\\]\nwhere \\(\\tau\\) is the configurable threshold (k_floor_threshold, default \\(-3\\)). Only \\(\\log k\\) values below \\(\\tau\\) are penalized.\n\n\n\nScalar Correction\nEven without the normalization regularization, a post-hoc scalar correction is applied when evaluating rate constants. The MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) is measured by evaluating MLP\\(_{\\text{sub}}\\) at the reference point:\n\\[\n\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\n\\]\nWith multiplicative aggregation, each reaction rate absorbs \\(\\alpha^{n_j}\\) where \\(n_j\\) is the number of substrates. The corrected rate constants are:\n\\[\n\\log_{10} k_j^{\\text{corrected}} = \\log_{10} k_j^{\\text{learned}} + n_j \\cdot \\log_{10} \\alpha\n\\]\nThe reported rate_constants_R2 is computed on these corrected values against the identity line \\(y=x\\).\n\n\nSummary of Hyperparameters\n\n\n\n\n\n\n\n\n\nParameter\nConfig key\nDescription\nTypical range\n\n\n\n\nLearning rate \\(k\\)\nlearning_rate_k\nRate constants update speed\n1E-4 to 1E-2\n\n\nLearning rate node\nlearning_rate_node\nMLP\\(_{\\text{node}}\\) update speed\n1E-4 to 1E-2\n\n\nLearning rate sub\nlearning_rate_sub\nMLP\\(_{\\text{sub}}\\) update speed\n1E-4 to 1E-2\n\n\nBatch size\nbatch_size\nTime frames per gradient step\n4 to 32\n\n\nTraining iterations\ndata_augmentation_loop\nMultiplier for iterations per epoch\n100 to 5000\n\n\nMLP\\(_{\\text{sub}}\\) monotonicity\ncoeff_MLP_sub_diff\nPenalize non-increasing MLP\\(_{\\text{sub}}\\)\n0 to 500\n\n\nMLP\\(_{\\text{node}}\\) L1\ncoeff_MLP_node_L1\nPenalize large homeostasis output\n0 to 10\n\n\nMLP\\(_{\\text{sub}}\\) normalization\ncoeff_MLP_sub_norm\nPin MLP\\(_{\\text{sub}}(c{=}1, |s|{=}1)\\) to 1\n0 to 10\n\n\nRate constant floor\ncoeff_k_floor\nPenalize \\(\\log k\\) below threshold\n0 to 10\n\n\nFloor threshold\nk_floor_threshold\nThreshold for k floor penalty\n\\(-3\\) (default)"
  },
  {
    "objectID": "gnn-llm-memory.html#metrics",
    "href": "gnn-llm-memory.html#metrics",
    "title": "GNN-LLM-Memory",
    "section": "Metrics",
    "text": "Metrics\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood value\n\n\n\n\nrate_constants_R2\nR² between learned and true rate constants \\(k\\) (after scalar correction)\n&gt; 0.9\n\n\ntest_R2\nR² on held-out test frames\n&gt; 0.9\n\n\ntest_pearson\nPearson correlation on test frames\n&gt; 0.95\n\n\nfinal_loss\nFinal prediction loss (MSE on \\(dc/dt\\))\nLower is better\n\n\nalpha\nMLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\\). Ideal value is 1.0 — indicates MLP\\(_{\\text{sub}}\\) has learned the correct scale. Deviations from 1 indicate residual scale ambiguity that the scalar correction must compensate for.\n\\(\\approx 1.0\\)\n\n\n\n\nDiagnostic Interpretation\nThe degeneracy gap = test_pearson \\(-\\) rate_constants_R2 diagnoses whether the model found the true rate constants or a degenerate solution:\n\n\n\n\n\n\n\n\n\ntest_pearson\n\\(R^2\\)\nGap\nDiagnosis\n\n\n\n\n&gt; 0.95\n&gt; 0.9\n&lt; 0.1\nHealthy — good dynamics from correct \\(k\\)\n\n\n&gt; 0.95\n0.3–0.9\n0.1–0.7\nDegenerate — good dynamics from wrong \\(k\\)\n\n\n&gt; 0.95\n&lt; 0.3\n&gt; 0.7\nSeverely degenerate — MLPs compensating\n\n\n&lt; 0.5\n&lt; 0.5\n~0\nFailed — both dynamics and \\(k\\) poor"
  },
  {
    "objectID": "gnn-llm-memory.html#ucb-tree-search",
    "href": "gnn-llm-memory.html#ucb-tree-search",
    "title": "GNN-LLM-Memory",
    "section": "UCB Tree Search",
    "text": "UCB Tree Search\nThe LLM selects parent configurations to mutate using an Upper Confidence Bound (UCB) strategy that balances exploitation of high-performing branches with exploration of under-visited regions:\n\\[\n\\text{UCB}(i) = \\bar{X}_i + c \\cdot \\sqrt{\\frac{\\ln N}{n_i}}\n\\]\nwhere \\(\\bar{X}_i\\) is the mean reward of node \\(i\\), \\(N\\) is the total number of visits, \\(n_i\\) is the number of visits to node \\(i\\), and \\(c\\) is the exploration constant.\n4 parallel slots run per batch with diversified roles:\n\n\n\n\n\n\n\n\nSlot\nRole\nDescription\n\n\n\n\n0\nexploit\nHighest UCB node, conservative mutation\n\n\n1\nexploit\n2nd highest UCB, or same parent different param\n\n\n2\nexplore\nUnder-visited node, or new parameter dimension\n\n\n3\nprinciple-test\nTest or challenge one established principle from memory"
  },
  {
    "objectID": "application.html",
    "href": "application.html",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#motivation",
    "href": "application.html#motivation",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "href": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Chronic Jugular Microdialysis in Freely Moving Mice",
    "text": "Chronic Jugular Microdialysis in Freely Moving Mice\nNardin et al. developed a workflow combining chronic jugular microdialysis with chemical isotope labeling LC-MS to continuously measure bloodborne metabolites in freely moving mice:\n\n\n\nProperty\nValue\n\n\n\n\nCompounds measured\n~123 high-quality amines and small peptides\n\n\nSampling cadence\n7.5 minutes\n\n\nRecording duration\n~8 hours per session\n\n\nTime frames\n~64 per session\n\n\nAnimals\n3 mice, implants patent for &gt;7 days\n\n\n\nThe data captures real metabolic dynamics: purine turnover correlating with movement, delayed histamine/5-HIAA changes, coordinated amino-acid dynamics, and state-dependent metabolic shifts between locomotion and rest.\n\nLow-Rank Physiological State Space\nA key finding: 10 principal components explain 73% of the variance across all measured compounds. The first component alone (rPC1) captures 35.5% and is strongly correlated with locomotion (Spearman \\(r = -0.59\\), peaking at 7.5 min lag).\nThis low-rank structure is consistent with what we observe in our simulations:\n\n\n\n\n\n\n\n\n\nNardin et al.\nMetabolismGraph simulations\n\n\n\n\nn_metabolites\n~123\n100\n\n\nRank at 70% variance\n~10\n~5–15 (estimated)\n\n\nRank at 99% variance\n—\n24–50\n\n\nDominant mode\nLocomotion-driven amino acid metabolism\nAutocatalytic cycle oscillations\n\n\n\nThe paper concludes that “endocrine and metabolic control may operate over a compact set of latent variables” — the same low-rank assumption that underlies our activity rank analysis via SVD."
  },
  {
    "objectID": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "href": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Applying MetabolismGraph to Microdialysis Data",
    "text": "Applying MetabolismGraph to Microdialysis Data\n\nWhat changes\nMoving from synthetic to real data introduces several differences:\n1. The stoichiometric matrix S is partially known\nIn simulation, \\(\\mathbf{S}\\) is either generated (ground truth) or learned from data. For real metabolomics, the reaction network connecting measured compounds can be partially reconstructed from biochemical databases (KEGG, Recon3D, HMDB). However:\n\nNot all reactions are known\nNot all participants in a reaction are measured (the 123 compounds are a subset of the full metabolome)\nSome measured compounds participate in multiple pathways\n\nThis suggests an intermediate mode between “S given” and “S learning”: initialize S from database knowledge, then refine with data.\n2. Observation is incomplete\nThe microdialysis probe measures only the free (unbound) fraction of molecules below the 6 kDa molecular weight cutoff. Many metabolites, enzymes, and signaling molecules are invisible. The GNN must learn dynamics from a partial observation of the full system — analogous to learning neural dynamics from a subset of recorded neurons.\n3. External drivers exist\nThe synthetic model is autonomous: \\(dc/dt\\) depends only on current concentrations and the network. Real metabolic dynamics are driven by external inputs — feeding, locomotion, circadian rhythms, stress — that are not part of the reaction network. The paper shows locomotion is a dominant driver (rPC1).\nThis maps to extensions of the current model:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot v_j}_{\\text{reactions}} + \\underbrace{f_{\\text{ext}}(x_{\\text{locomotion}}, t)}_{\\text{external drive}}\n\\]\nThe circadian_amplitude and circadian_period parameters already in the simulation config are a first step toward modeling time-varying external inputs.\n4. Kinetics may not be mass-action\nEnzyme-mediated reactions follow Michaelis-Menten or Hill kinetics rather than pure mass-action. The flexible MLP\\(_{\\text{sub}}\\) architecture is well-suited here — it can discover the true functional form without assuming \\(c^{|S|}\\) a priori. The additive aggregation mode may be more appropriate for enzyme-mediated reactions where substrates contribute more independently.\n\n\nWhat stays the same\nThe core framework transfers directly:\n\nBipartite graph structure: metabolites connected to reactions through stoichiometric edges\nGNN message passing: substrate concentrations aggregated per reaction, then distributed back to metabolites\nFunction discovery: MLP\\(_{\\text{sub}}\\) learns the concentration-to-rate mapping, MLP\\(_{\\text{node}}\\) learns homeostatic regulation\nRate constant recovery: per-reaction \\(k_j\\) learned in log-space\nIdentifiability challenges: scale ambiguity, function compensation, and homeostasis absorption are equally present (and harder to diagnose without ground truth)\n\n\n\nProposed workflow\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    A[Microdialysis time series&lt;br/&gt;123 compounds × 64 frames] --&gt; B[Construct bipartite graph&lt;br/&gt;from KEGG/Recon3D]\n    B --&gt; C[Initialize S from&lt;br/&gt;database stoichiometry]\n    C --&gt; D[Train GNN&lt;br/&gt;predict dc/dt]\n    D --&gt; E{Evaluate}\n    E --&gt;|test R² &gt; 0.9| F[Extract learned parameters]\n    E --&gt;|test R² &lt; 0.9| G[Refine: unfreeze S,&lt;br/&gt;add external inputs]\n    G --&gt; D\n    F --&gt; H[Rate constants k_j]\n    F --&gt; I[Learned kinetics&lt;br/&gt;MLP_sub shape]\n    F --&gt; J[Homeostatic regulation&lt;br/&gt;MLP_node per metabolite type]\n\n\n\n\n\n\nStep 1. Build the metabolite-reaction bipartite graph from KEGG pathway maps for the 123 identified compounds. Estimate the number of reactions linking measured metabolites.\nStep 2. Initialize \\(\\mathbf{S}\\) from known stoichiometry (S given mode). Train the GNN to predict \\(dc/dt\\) from concentration snapshots, learning \\(k_j\\), MLP\\(_{\\text{sub}}\\), and MLP\\(_{\\text{node}}\\).\nStep 3. Evaluate prediction quality on held-out time frames. If prediction R\\(^2\\) is high but the learned MLP\\(_{\\text{sub}}\\) does not match expected kinetics (mass-action or Michaelis-Menten), investigate degeneracy.\nStep 4. Optionally unfreeze \\(\\mathbf{S}\\) to discover missing reactions or correct database errors (S learning mode). Compare learned stoichiometry against biochemical databases.\nStep 5. Incorporate locomotion and other behavioral covariates as external inputs to account for the dominant non-metabolic drivers of concentration change."
  },
  {
    "objectID": "application.html#challenges-and-open-questions",
    "href": "application.html#challenges-and-open-questions",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Challenges and Open Questions",
    "text": "Challenges and Open Questions\nTemporal resolution. The 7.5-minute sampling cadence gives ~64 frames per session. Our simulations use 2880 frames. With fewer time points, the GNN has less data to disentangle rate constants from MLP functions, exacerbating identifiability issues. Multi-session data across the 7+ days of implant patency could help.\nMissing metabolites. The 123 measured compounds are a fraction of the full metabolome. Reactions involving unmeasured substrates will appear as unexplained variance. The MLP\\(_{\\text{node}}\\) may absorb some of this as apparent “homeostasis.”\nNon-stationarity. Real metabolic dynamics are non-stationary — circadian rhythms, feeding cycles, and adaptation shift the baseline over hours. The current model assumes fixed \\(c^{\\text{baseline}}\\) and \\(\\lambda\\). Time-varying homeostatic parameters may be needed.\nValidation without ground truth. In simulation, we validate against known \\(k_j\\) values (rate constants R\\(^2\\)). With real data, validation must rely on: (1) held-out prediction accuracy, (2) consistency with known biochemistry, (3) perturbation experiments (the paper notes microdialysis can also deliver molecules), and (4) cross-animal reproducibility."
  },
  {
    "objectID": "application.html#references",
    "href": "application.html#references",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "References",
    "text": "References\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#the-full-model",
    "href": "index.html#the-full-model",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Full Model",
    "text": "The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate \\(v_j\\) depends on aggregation type:\n\n\n\n\n\n\n\nAggregation\nRate \\(v_j\\)\n\n\n\n\nAdditive\n\\(v_j = k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\nMultiplicative\n\\(v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\n\nSee Model for detailed equations, diagrams, and model configurations."
  },
  {
    "objectID": "index.html#the-inverse-problem",
    "href": "index.html#the-inverse-problem",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe forward model describes how concentrations evolve given all parameters. In practice, the parameters themselves are unknown. The inverse problem is to recover them from observed dynamics.\nGiven:\n\nConcentration trajectories \\(\\{c_i(t)\\}_{i=1}^{n}\\) measured over time\nStoichiometric matrix \\(\\mathbf{S}\\) (known from biochemistry)\n\nTo learn:\n\nSubstrate function \\(\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\) — discovers the mass-action power law \\(c_k^{|S_{kj}|}\\)\nHomeostasis function \\(\\text{MLP}_{\\text{node}}(c_i)\\) — discovers per-metabolite regulation \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\nRate constants \\(k_j\\) — per-reaction speed scalars\n\nThis is challenging because the system is high-dimensional (\\(n\\) metabolites, \\(m\\) reactions), the mapping from parameters to dynamics is nonlinear, and multiple parameter combinations can produce similar trajectories (identifiability). Classical optimization approaches struggle with this combinatorial landscape.\nWe address this by casting the inverse problem as a Graph Neural Network learning task. The metabolic network is naturally a bipartite graph (metabolites \\(\\leftrightarrow\\) reactions), and we replace the unknown functions with learnable MLPs that operate on this graph structure. The GNN is trained end-to-end by minimizing the prediction error on \\(dc/dt\\), recovering the rate constants and homeostatic functions simultaneously. An LLM-driven closed-loop exploration engine systematically searches the hyperparameter space — see GNN-LLM-Memory for the training scheme, regularization terms, and exploration loop.\n\nGNN Parameterization\n\n\n\n\n\nflowchart LR\n    C[\"cₖ\"] --&gt; SUB[\"MLP_sub&lt;br/&gt;(cₖ, |Sₖⱼ|)\"]\n    SUB --&gt; AGG[\"aggr&lt;br/&gt;Σ / Π\"]\n    AGG --&gt; K[\"× kⱼ\"]\n    K --&gt; S[\"× Sᵢⱼ&lt;br/&gt;Σⱼ\"]\n    NODE[\"MLP_node&lt;br/&gt;(cᵢ, aᵢ)\"] --&gt; OUT([\"dcᵢ/dt\"])\n    S --&gt; OUT\n\n    style C fill:#e1f5fe,stroke:#0277bd\n    style SUB fill:#f3e5f5,stroke:#7b1fa2\n    style AGG fill:#fff3e0,stroke:#ef6c00\n    style K fill:#fff3e0,stroke:#ef6c00\n    style S fill:#e8f5e9,stroke:#2e7d32\n    style NODE fill:#f3e5f5,stroke:#7b1fa2\n    style OUT fill:#fce4ec,stroke:#c62828\n\n\n\n\n\n\nSubstrate concentrations flow through MLP_sub (purple), are aggregated per-reaction and scaled by \\(k_j\\) (orange), then mixed via stoichiometry \\(S\\) (green). MLP_node (purple) adds homeostatic regulation. The output is \\(dc/dt\\) (red).\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\underset{k \\in \\text{sub}(j)}{\\text{aggr}} \\; \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nwhere:\n\n\\(a_i \\in \\mathbb{R}^d\\) is a learnable embedding for metabolite \\(i\\)\n\\(k_j\\) are learnable rate constants\n\\(\\text{aggr}\\) is sum (additive) or product (multiplicative)\n\n\n\nLearnable Parameters\n\n\n\n\n\n\n\n\nParameter\nType\nPurpose\n\n\n\n\n\\(a_i\\)\nEmbedding vectors\nPer-metabolite identity\n\n\n\\(k_j\\)\nScalars\nPer-reaction rate constants\n\n\n\\(\\text{MLP}_{\\text{node}}\\)\nNeural network\nLearns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\n\n\n\\(\\text{MLP}_{\\text{sub}}\\)\nNeural network\nLearns \\(c_k^{|S_{kj}|}\\)\n\n\n\n\n\nConfiguration\ngraph_model:\n  aggr_type: add         # sum (additive) or mul (multiplicative)\n  embedding_dim: 2       # dimension of metabolite embeddings a_i\n  hidden_dim: 32\n\ntraining:\n  learning_rate_start: 0.001\n  freeze_stoichiometry: true   # S given mode\n  training_single_type: false  # learn per-metabolite embeddings"
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "Key Features",
    "text": "Key Features\n\nBipartite graph representation: metabolites and reactions form a bipartite graph, with stoichiometric coefficients on edges\nLearnable rate constants: per-reaction \\(k_j\\) learned via gradient descent\nLearnable homeostasis: MLP learns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\) per metabolite\nFlexible aggregation: additive (sum) or multiplicative (product) for different dynamics\nMetabolite embeddings: learnable vectors \\(a_i\\) capture per-metabolite identity"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "Citation",
    "text": "Citation\nIf you use MetabolismGraph in your research, please cite:\n@software{metabolismgraph2025,\n  author = {Allier, Cédric},\n  title = {MetabolismGraph: Learning Metabolism Dynamics with GNNs},\n  year = {2026},\n  url = {https://github.com/allierc/MetabolismGraph}\n}"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This page was generated by Claude and may contain inaccuracies in author lists, publication details, or descriptions. Please verify citations before use."
  },
  {
    "objectID": "references.html#context",
    "href": "references.html#context",
    "title": "References",
    "section": "Context",
    "text": "Context\nMetabolismGraph sits at the intersection of several active research areas: graph neural networks for physical and biological systems, inverse problems in systems biology, neural differential equations, and LLM-driven scientific exploration. The framework uses message-passing GNNs on bipartite metabolite-reaction graphs to solve the inverse problem of recovering kinetic parameters from concentration dynamics, with an LLM-based closed-loop exploration engine for hyperparameter optimization.\nBelow we collect key references organized by topic. Where available, we provide arXiv or DOI links."
  },
  {
    "objectID": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "href": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "title": "References",
    "section": "Graph Neural Networks for Physical and Biological Systems",
    "text": "Graph Neural Networks for Physical and Biological Systems\n\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., & Battaglia, P. W. (2020). Learning to simulate complex physics with graph networks. ICML 2020. arXiv:2002.09405 — Graph network-based simulators (GNS) for learning physical dynamics from particle-based representations. Demonstrates that GNNs can learn accurate forward simulators for complex physical systems.\nKipf, T., Fetaya, E., Wang, K.-C., Welling, M., & Zemel, R. (2018). Neural relational inference for interacting systems. ICML 2018. arXiv:1802.04687 — Learns interaction graphs from observed trajectories using variational autoencoders on graph structures. Closely related to our inverse-problem setting where the goal is to recover network structure from dynamics.\nCranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., & Ho, S. (2020). Discovering symbolic models from deep learning with inductive biases. NeurIPS 2020. arXiv:2006.11287 — Combines GNNs with symbolic regression to extract interpretable physical laws from learned representations. Relevant to our approach of recovering interpretable kinetic parameters from learned MLP functions."
  },
  {
    "objectID": "references.html#neural-differential-equations-and-scientific-ml",
    "href": "references.html#neural-differential-equations-and-scientific-ml",
    "title": "References",
    "section": "Neural Differential Equations and Scientific ML",
    "text": "Neural Differential Equations and Scientific ML\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural ordinary differential equations. NeurIPS 2018. arXiv:1806.07366 — Foundational work on continuous-depth neural networks parameterized as ODEs, enabling gradient-based learning of dynamical systems.\nRackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R., Skinner, D., Ramadhan, A., & Edelman, A. (2020). Universal differential equations for scientific machine learning. arXiv:2001.04385 — Framework combining differential equations with neural networks for scientific modeling. The UDE approach of embedding learnable components within known differential equation structure is conceptually similar to how MetabolismGraph embeds learnable MLPs within the known stoichiometric framework."
  },
  {
    "objectID": "references.html#symbolic-regression-for-network-dynamics",
    "href": "references.html#symbolic-regression-for-network-dynamics",
    "title": "References",
    "section": "Symbolic Regression for Network Dynamics",
    "text": "Symbolic Regression for Network Dynamics\n\nYu, Z., Ding, J., & Li, Y. (2025). Discovering network dynamics with neural symbolic regression. Nature Computational Science. DOI:10.1038/s43588-025-00893-8 — ND2: neural symbolic regression that discovers governing equations of network dynamics directly from data. Applied to gene regulatory networks, the method corrects the classical Hill equation model by replacing per-neighbor nonlinear terms with a logistic function applied to the aggregate neighbor sum (see comparison below).\n\n\nComparison with MetabolismGraph\nYu et al.’s corrected gene regulation model (their Eq. 2) and MetabolismGraph share the same general ODE structure — self-dynamics plus interaction dynamics — but differ in how neighbor contributions are aggregated:\nYu et al. — Gene regulation (ND2 corrected):\n\\[\\frac{dx_i}{dt} = \\underbrace{s_i - \\gamma_i x_i}_{\\text{self-dynamics}} + \\underbrace{\\beta \\, \\tilde{S}\\!\\left(\\sum_j A_{ij}\\, x_j\\right)}_{\\text{interaction}}\\]\nwhere \\(\\tilde{S}(x) = (1 + e^{-x})^{-1}\\) is the logistic function. The nonlinearity acts on the sum of weighted neighbor states — no per-edge rate constants, no multiplicative aggregation.\nMetabolismGraph — Metabolic kinetics:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i(c_i - c_i^{\\text{baseline}})}_{\\text{self-dynamics}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}}_{\\text{interaction}}\\]\nKey differences:\n\n\n\n\n\n\n\n\n\nYu et al. (gene regulation)\nMetabolismGraph (metabolism)\n\n\n\n\nGraph\nMonopartite (gene → gene)\nBipartite (metabolite ↔︎ reaction)\n\n\nAggregation\n\\(\\tilde{S}(\\sum_j A_{ij} x_j)\\) — logistic of sum\n\\(\\sum_j S_{ij} k_j \\prod_k c_k^{s_{kj}}\\) — sum of products\n\n\nRate constants\nSingle global \\(\\beta\\)\nPer-reaction \\(k_j\\) (256 parameters)\n\n\nNonlinearity\nBounded logistic \\(\\tilde{S} \\in [0,1]\\)\nUnbounded power law \\(c^s\\)\n\n\nHigher-order\nImplicit: \\(\\partial \\dot{x}_i / \\partial x_j\\) depends on all neighbors via \\(\\tilde{S}\\)\nExplicit: mass-action products couple substrates within each reaction\n\n\n\nThe gene regulation model has no per-reaction aggregation step — it sums all neighbor states into a single scalar, then applies a saturating nonlinearity. MetabolismGraph instead computes a separate rate for each reaction (multiplicative aggregation of substrate concentrations), then sums the stoichiometric contributions. This reflects a fundamental difference between gene regulation (bounded transcriptional response) and metabolism (unbounded mass-action kinetics)."
  },
  {
    "objectID": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "href": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "title": "References",
    "section": "In Vivo Metabolomics and Physiological State Spaces",
    "text": "In Vivo Metabolomics and Physiological State Spaces\n\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974 — Chronic jugular microdialysis paired with LC-MS measures ~123 bloodborne compounds at 7.5-min cadence in freely moving mice. PCA reveals a low-rank physiological manifold: 10 components explain 73% of variance, with rPC1 aligned to locomotion. Provides the real-world metabolomic time series that MetabolismGraph’s inverse problem framework is designed to analyze. See Application for how the GNN framework maps to this data."
  },
  {
    "objectID": "references.html#llm-driven-scientific-discovery",
    "href": "references.html#llm-driven-scientific-discovery",
    "title": "References",
    "section": "LLM-Driven Scientific Discovery",
    "text": "LLM-Driven Scientific Discovery\n\nRomera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468-475. DOI:10.1038/s41586-023-06924-6 — FunSearch: uses LLMs to discover new mathematical constructions through evolutionary program search. Pioneering demonstration that LLMs can make genuine scientific contributions when embedded in a search loop.\nNovikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration. Google DeepMind. — Extends the LLM-driven exploration paradigm to broader scientific and algorithmic discovery tasks. The closed-loop LLM exploration engine in MetabolismGraph draws inspiration from this line of work.\nLu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024). The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv:2408.06292 — End-to-end autonomous research agent that generates hypotheses, runs experiments, and writes papers."
  }
]