[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the oscillatory regime (activity rank \\(\\sim50\\)): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j \\in [10^{-2}, 10^{-1}]\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n84 iterations completed across 7 blocks (21 batches). The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). 100% autocatalytic 3-cycles.\n\n\n\n\n\n\n\n\n\nThe primary metric is raw R² computed on all 256 reactions (after MLP\\(_{\\text{sub}}\\) scalar correction). The trimmed R² excludes outlier reactions (\\(|\\Delta \\log_{10} k| &gt; 0.3\\)) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 85))\nr2 = [\n    # Block 1 (Iter 1-12): Initial exploration\n    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep\n    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough\n    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation\n    # Block 2 (Iter 13-24): k_floor breakthrough\n    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough\n    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor\n    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best\n    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough\n    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns\n    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches\n    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH\n    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7\n    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub\n    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed\n    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST\n    # Block 5 (Iter 49-60): Confirming optimum\n    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests\n    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds\n    0.701, 0.718, 0.603, 0.603,  # Batch 15: seed=99 promising\n    # Block 6 (Iter 61-72): Variance discovery\n    0.688, 0.430, 0.616, 0.704,  # Batch 16: k_floor=1.5 promising\n    0.565, 0.674, 0.664, 0.609,  # Batch 17: k_floor non-monotonic\n    0.658, 0.639, 0.473, 0.550,  # Batch 18: HIGH VARIANCE discovered\n    # Block 7 (Iter 73-84): seed=77 breakthrough\n    0.722, 0.583, 0.682, 0.636,  # Batch 19: seed=123 improved\n    0.694, 0.515, 0.473, 0.748,  # Batch 20: seed=77 NEW BEST\n    0.661, 0.764, 0.387, 0.689,  # Batch 21: seed=77+sub_diff=8 PEAK\n]\noutliers = [\n    43, 47, 45, 53,\n    61, 36, 45, 32,\n    27, 38, 57, 30,\n    28, 33, 36, 33,\n    17, 24, 26, 35,\n    16, 29, 19, 24,\n    18, 18, 15, 17,\n    19, 14, 21, 21,\n    21, 21, 15, 22,  # Batch 9\n    16, 19, 20, 21,  # Batch 10\n    21, 16, 18, 19,  # Batch 11\n    15, 20, 23, 21,  # Batch 12\n    12, 21, 12, 25,  # Batch 13\n    21, 21, 18, 19,  # Batch 14\n    17, 17, 16, 20,  # Batch 15\n    17, 20, 17, 16,  # Batch 16\n    17, 14, 16, 14,  # Batch 17\n    19, 14, 22, 17,  # Batch 18\n    19, 16, 22, 20,  # Batch 19\n    16, 16, 32, 12,  # Batch 20\n    16, 15, 23, 18,  # Batch 21\n]\n\n# Color by block\nblock_colors = ['#3498db', '#2ecc71', '#e67e22', '#9b59b6', '#1abc9c', '#e84393', '#f39c12']\nblock_bounds = [12, 24, 36, 48, 60, 72, 84]\ncolors = []\nfor i in range(len(r2)):\n    for b, bound in enumerate(block_bounds):\n        if i &lt; bound:\n            colors.append(block_colors[b])\n            break\n\nfig, ax1 = plt.subplots(figsize=(16, 5))\n\nbars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)\nax1.axhline(y=0.764, color='#f39c12', linestyle='--', alpha=0.4, label='Peak R² = 0.764')\nax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')\n\n# Block separators and labels\nfor b in range(len(block_bounds) - 1):\n    ax1.axvline(x=block_bounds[b] + 0.5, color='gray', linestyle=':', alpha=0.4)\nblock_labels = ['Block 1', 'Block 2', 'Block 3', 'Block 4', 'Block 5', 'Block 6', 'Block 7']\nblock_centers = [6.5, 18.5, 30.5, 42.5, 54.5, 66.5, 78.5]\nfor i, (lbl, cx) in enumerate(zip(block_labels, block_centers)):\n    ax1.text(cx, 1.0, lbl, ha='center', fontsize=8, color=block_colors[i])\n\n# Annotate key events\nax1.annotate('k_floor=1.0\\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.25),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(35, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('sub_diff=7', xy=(45, 0.736), xytext=(45, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('seed=77+\\nsub_diff=8', xy=(82, 0.764), xytext=(82, 0.90),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('rate_constants R² (raw)')\nax1.set_ylim(-0.02, 1.05)\nax1.set_xticks([1, 12, 14, 21, 24, 35, 36, 45, 48, 60, 72, 80, 82, 84])\n\n# outlier count on secondary axis\nax2 = ax1.twinx()\nax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')\nax2.set_ylabel('outlier count', color='#e74c3c')\nax2.tick_params(axis='y', labelcolor='#e74c3c')\nax2.set_ylim(0, 70)\n\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)\n\nax1.set_title('UCB Exploration: Rate Constant Recovery (84 iterations)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Raw R² (bars) and outlier count (dots) across 84 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74), seed=77+sub_diff=8 (iter 82, 0.76). Training variance of ±0.08 R² is a dominant factor beyond Block 5.\n\n\n\n\n\n\n\n\nAll 12 iterations achieved raw R² &lt; 0.07. The key discovery was that coeff_MLP_sub_norm=1.0 is essential — it corrects the MLP\\(_{\\text{sub}}\\) function shapes (c² becomes quadratic instead of linear) and enables MLP\\(_{\\text{node}}\\) to learn homeostasis.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n1 (iter 1–4)\nlr sweep\n0.044\nAll failed; MLP\\(_{\\text{node}}\\) dead, MLP\\(_{\\text{sub}}\\) c² linear\n\n\n2 (iter 5–8)\nsub_norm=1.0\n0.067\nMLP\\(_{\\text{sub}}\\) normalization is the single most effective change\n\n\n3 (iter 9–12)\ncombine best\n0.061\nMLP\\(_{\\text{node}}\\) activated; lr_node=0.005 hurts\n\n\n\n\n\n\nThe coeff_k_floor=1.0 penalty at iteration 14 produced a 10× improvement in R² (0.06 → 0.51) by preventing outlier \\(\\log k\\) values from drifting below the true minimum. Longer training then pushed R² to 0.69.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n4 (iter 13–16)\nk_floor=1.0\n0.508\nBreakthrough — R² jumped 10×\n\n\n5 (iter 17–20)\naug=3000\n0.642\nLonger training + k_floor synergistic\n\n\n6 (iter 21–24)\naug=4000\n0.690\nFirst to reach 0.69; \\(\\alpha = 0.85\\), 16 outliers\n\n\n\nBest result — Iteration 82: seed=77, coeff_MLP_sub_diff=8, lr_sub=0.001, data_augmentation_loop=4500, coeff_k_floor=1.0, coeff_MLP_sub_norm=1.0\n\n\n\nMetric\nValue\n\n\n\n\nRaw R²\n0.764\n\n\nOutliers\n15 / 256 (5.9%)\n\n\nSlope\n0.97\n\n\n\\(\\alpha\\)\n0.87\n\n\n\nNote: Optimal sub_diff is seed-dependent: 7 for seed=42 (R²=0.74), 8 for seed=77 (R²=0.76). Training has intrinsic variance of $$0.08 R² even with identical configuration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline). R² = 0.044, 43 outliers. Before k_floor and sub_norm — predicted \\(\\log k\\) values scatter widely with no correlation to ground truth.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The k_floor=1.0 penalty prevents outlier \\(\\log k\\) from drifting below the true minimum, producing a 10\\(\\times\\) R² jump.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Iteration 82 (best). R² = 0.764, 15 outliers. seed=77 + sub_diff=8 + aug=4500 yields the tightest clustering around the diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Kinograph montage for iteration 82 (seed=77, sub_diff=8). Top-left: ground-truth \\(dc/dt\\); top-right: GNN prediction; bottom-left: residual (same color scale as GT); bottom-right: predicted vs. true \\(dc/dt\\) scatter. The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Iteration 1 (baseline). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.42 — under-scaled. MLP\\(_{\\text{node}}\\): flat at zero.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Iteration 82 (best R²). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.87 — close to the true scale. MLP\\(_{\\text{node}}\\): still flat at zero across all 84 iterations — homeostasis not learned.\n\n\n\n\n\n\n\n\n\nTwelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling lr_sub from 0.0005 to 0.001 broke through to R² = 0.726.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n7 (iter 25–28)\naug=5000, seed, batch_size\n0.652\naug=5000 hurt R² but \\(\\alpha=0.95\\) (best ever)\n\n\n8 (iter 29–32)\nL1=0, sub_norm=2.0, lr_k\n0.619\nsub_norm=2.0: fewest outliers (14), best slope (0.99)\n\n\n9 (iter 33–36)\nsub_norm=2.0+aug=3500, recurrent, lr_sub=0.001, aug=3500\n0.726\nlr_sub=0.001 broke the plateau\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstrategies = {\n    'No k_floor\\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],\n    'k_floor\\nsub_diff=5\\nseed=42': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],\n    'sub_diff=7\\nseed=42': [0.736, 0.696, 0.655, 0.662, 0.560, 0.701, 0.718, 0.658],\n    'seed=77\\nsub_diff=8': [0.748, 0.661, 0.764],\n    'Other seeds\\n(7,99,123)': [0.688, 0.430, 0.722, 0.694, 0.387, 0.689],\n    'Arch/k_floor\\nchanges': [0.616, 0.704, 0.565, 0.674, 0.664, 0.609, 0.639, 0.473, 0.550, 0.583, 0.515, 0.473, 0.636, 0.603, 0.603],\n}\n\nfig, ax = plt.subplots(figsize=(12, 5))\npositions = []\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#95a5a6']\nfor i, (label, vals) in enumerate(strategies.items()):\n    x = np.random.normal(i, 0.08, len(vals))\n    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3, color=colors[i], edgecolors='none')\n    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)\n    positions.append(i)\n\nax.set_xticks(positions)\nax.set_xticklabels(list(strategies.keys()), fontsize=8)\nax.set_ylabel('raw R²', fontsize=12)\nax.set_ylim(-0.05, 0.90)\nax.axhline(y=0.764, color='#f39c12', linestyle='--', alpha=0.3, label='Peak R² = 0.764')\nax.grid(axis='y', alpha=0.3)\nax.legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Impact of key strategies on R². Each dot is one iteration. The k_floor penalty is the single most important factor. Optimal sub_diff is seed-dependent: sub_diff=7 for seed=42, sub_diff=8 for seed=77. Training has intrinsic variance of ±0.08 R².\n\n\n\n\n\n\n\n\nBlock 4 first tested combinations with lr_sub=0.001 and revealed seed sensitivity ($$0.2 R²). Then sub_diff=7 (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n10 (iter 37–40)\nsub_norm=2.0, lr_sub=0.002, lr_node=0.002, L1=0\n0.662\nAll combinations with lr_sub=0.001 hurt R² vs baseline\n\n\n11 (iter 41–44)\nseed=123, aug=4500, sub_diff=3, lr_k=0.007\n0.690\naug=4500 most stable (\\(\\alpha = 0.94\\)); seed=123 crashed to R²=0.49\n\n\n12 (iter 45–48)\nsub_diff=7, lr_k=0.004, hidden_dim=128, batch_size=16\n0.736\nsub_diff=7 new best; wider/deeper MLP and larger batch hurt\n\n\n\nIteration 45 (sub_diff=7, aug=4500) achieved R² = 0.736 with \\(\\alpha = 0.90\\) and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46–48 confirmed that lr_k=0.004 is too slow, hidden_dim_sub=128 allows degenerate solutions, and batch_size=16 degrades convergence.\n\n\n\nBlock 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n13 (iter 49–52)\naug=5000, sub_diff=8, seed=123, n_layers=4\n0.696\naug=5000 hurts (confirmed twice); sub_diff=7 improves seed robustness ($$0.08 vs $$0.24)\n\n\n14 (iter 53–56)\naug=4250, sub_diff=6, seed=123+aug=4000, lr_sub=0.0015\n0.701\nAll variations hurt; sub_diff=6 too weak, lr_sub=0.0015 too high\n\n\n15 (iter 57–60)\nseed=123+aug=3500, seed=99, L1=0.5, sub_norm=0.5\n0.718\nseed=99 promising (R²=0.72, \\(\\alpha=0.92\\)); sub_norm=0.5 confirmed essential principle\n\n\n\nTight bounds confirmed: aug=4500 (not 4000, 4250, or 5000), sub_diff=7 (not 5, 6, or 8), lr_sub=0.001 (not 0.0015 or 0.002), lr_k=0.005 (not 0.004 or 0.007). sub_diff=7 improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with sub_diff=5) to 0.08.\n\n\n\nBlock 6 explored new seeds and k_floor variations, then discovered that training has high intrinsic variance — an exact replica of the best configuration (iter 45) achieved only R²=0.66 instead of 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n16 (iter 61–64)\nseed=7, seed=99+aug=4250, lr_node=0.0005, k_floor=1.5\n0.704\nk_floor=1.5 promising (R²=0.70, \\(\\alpha=0.92\\)); seed=99 very sensitive to aug\n\n\n17 (iter 65–68)\nk_floor=1.25, k_floor=1.5+seed=99, aug=4750, sub_diff=6+k_floor=1.5\n0.674\nk_floor response is non-monotonic: 1.25 &lt; 1.0 and 1.5; aug=4750 hurts\n\n\n18 (iter 69–72)\nExact replica of iter 45, lr_k=0.0045, hidden_dim_node=32, sub_norm=1.5\n0.658\nHIGH VARIANCE: replica got R²=0.66 vs original 0.74; MLP\\(_{\\text{sub}}\\) c² failure mode\n\n\n\nThe high variance discovery is significant: identical configurations can produce R² values differing by 0.08, meaning many earlier “improvements” may have been within noise. This makes seed selection and reproducibility central concerns.\n\n\n\nBlock 7 explored new seeds and found that seed=77 is a “golden seed” — achieving R²=0.748 (iter 80) and then R²=0.764 with sub_diff=8 (iter 82), surpassing the previous best.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n19 (iter 73–76)\nseed=123, lr_sub=0.0012, sub_diff=9, aug=4000\n0.722\nseed=123 improved to R²=0.72 (vs 0.66 earlier); all other variations hurt\n\n\n20 (iter 77–80)\nseed=7, k_floor=1.5, n_layers=4, seed=77\n0.748\nseed=77 new best seed (R²=0.748, 12 outliers); n_layers=4 confirmed harmful\n\n\n21 (iter 81–84)\nReplicate seed=77, seed=77+sub_diff=8, seed=78, aug=5000\n0.764\nseed=77+sub_diff=8 = new global best; replica got R²=0.66 (variance); seed=78 poor (R²=0.39)\n\n\n\nThe key finding: optimal sub_diff is seed-dependent. sub_diff=8 hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76). Adjacent seeds (78, 79) do not share seed=77’s properties — “golden seeds” are rare and unpredictable.\n\n\n\n\n\n\n\n\n\nFigure 11: UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the k_floor=1.0 breakthrough (node 14) and converged on the aug=4000 regime. The tree has since grown to 84 nodes; the overall best is node 82 (R² = 0.764).\n\n\n\n\n\n\nThe LLM’s persistent memory has accumulated these validated principles after 84 iterations:\n\ncoeff_MLP_sub_norm=1.0 is essential — enables correct MLP shapes: c² becomes quadratic, MLP\\(_{\\text{node}}\\) activates. sub_norm=0.5 and sub_norm=1.5 both hurt (iter 60, 72).\ncoeff_k_floor=1.0 is critical — R² jumped from 0.06 to 0.51 (10× improvement). k_floor=2.0 too strong. k_floor=1.5 gives R²=0.70 but response is non-monotonic: k_floor=1.25 is worse than both 1.0 and 1.5 (iter 65).\nLonger training helps (up to aug=4500) — aug=2000→3000→4000→4500 consistently improves R². aug=5000 hurts R² (confirmed 3×: iter 25, 49, 84). aug=4750 also hurts (iter 67). aug=4000 slightly suboptimal (iter 76).\nlr_k=0.005 is optimal — lower (0.003, 0.004, 0.0045) too slow, higher (0.007, 0.01) destabilizes (confirmed across 6 iterations)\nlr_sub=0.001 is optimal — 2× increase from 0.0005 broke the R² plateau; lr_sub=0.002 too high (iter 38); lr_sub=0.0015 hurts (iter 56); lr_sub=0.0012 hurts (iter 74)\nlr_node=0.001 is optimal — lr_node=0.005 destabilizes (iter 11); lr_node=0.0005 hurts (iter 63)\nOptimal sub_diff is seed-dependent — sub_diff=7 for seed=42 (R²=0.74, iter 45), sub_diff=8 for seed=77 (R²=0.76, iter 82). sub_diff=8 hurts seed=42 (iter 50) but helps seed=77.\nTraining has high intrinsic variance ($$0.08 R²) — exact replicas: iter 45 R²=0.74 vs iter 69 R²=0.66; iter 80 R²=0.75 vs iter 81 R²=0.66. Many earlier “improvements” may be within noise.\nseed=77 is the best seed — R²=0.748 (iter 80), R²=0.764 with sub_diff=8 (iter 82). Adjacent seeds (78, 79) are poor — “golden seeds” are rare and unpredictable.\nbatch_size=8 is optimal — batch_size=16 hurts R² (iter 48)\nDefault MLP architecture is optimal — hidden_dim_sub=128 allows degenerate solutions (iter 47); hidden_dim_node=32 significantly worse (iter 71); n_layers_sub=4 hurts (iter 52, 79)\ncoeff_MLP_node_L1=1.0 is optimal — L1=0.0 + long training harmful (iter 22, 29, 40); L1=0.5 hurts (iter 59)\nsub_diff=7 improves seed robustness — R² gap between seed=42 and seed=123 dropped from 0.24 (with sub_diff=5) to 0.08 (with sub_diff=7)\nMLP\\(_{\\text{node}}\\) remains flat across all 84 iterations — homeostasis \\(-\\lambda(c - c^{\\text{base}})\\) is never learned, regardless of configuration\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nEvidence\n\n\n\n\nRecurrent training breaks degeneracy\nNo R² improvement, 3.5× slower (iter 13, 34)\n\n\nSmaller MLP improves k recovery\nWorst R² = 0.011 (iter 15)\n\n\nHigher lr_node activates MLP\\(_{\\text{node}}\\)\nlr_node=0.005 hurts (iter 11); 0.002 no effect (iter 26, 39)\n\n\nDifferent seed breaks degeneracy\nSame MLP\\(_{\\text{node}}\\) flatness (iter 27); R² variance $$0.2 (iter 41)\n\n\nStronger monotonicity (sub_diff=10) helps\nR² dropped to 0.41 (iter 32)\n\n\nWeaker monotonicity (sub_diff=3) helps\nR² dropped to 0.61 (iter 43)\n\n\nSmaller batch size helps convergence\nR² dropped (iter 28)\n\n\naug=5000 continues to improve\nConfirmed 3×: iter 25 (0.65), iter 49 (0.70), iter 84 (0.69)\n\n\nlr_sub=0.002 improves over 0.001\nR² dropped from 0.73 to 0.52 (iter 38)\n\n\nsub_norm=2.0 improves R²\nImproves \\(\\alpha\\) but hurts R² (iter 30, 37)\n\n\nCombining lr_sub=0.001 with other changes\nAll combinations hurt R² vs baseline (iter 37–40)\n\n\nWider MLP\\(_{\\text{sub}}\\) (hidden_dim=128) helps\nR² dropped to 0.56 — allows degenerate solutions (iter 47)\n\n\nLarger batch_size=16 stabilizes gradients\nR² dropped to 0.56 (iter 48)\n\n\nsub_diff=8 hurts all seeds\nHurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76)\n\n\nsub_diff=6 is better than 7\nR² dropped from 0.74 to 0.56 (iter 54)\n\n\nDeeper MLP\\(_{\\text{sub}}\\) (n_layers=4) helps\nR² dropped to 0.55 (iter 52); confirmed iter 79 (R²=0.47)\n\n\nlr_sub=0.0015 (intermediate) helps\nR² dropped from 0.74 to 0.60 (iter 56)\n\n\nlr_k=0.004 gives finer convergence\nR² dropped to 0.48 — too slow (iter 46)\n\n\nsub_norm=0.5 helps\nR² dropped from 0.74 to 0.60 (iter 60)\n\n\nIntermediate k_floor=1.25 is optimal\nR²=0.56, worse than both 1.0 and 1.5 — non-monotonic (iter 65)\n\n\naug=4750 within safe range\nR² dropped to 0.66 despite \\(\\alpha\\)=0.96 (iter 67)\n\n\nsub_norm=1.5 better than 1.0\nR²=0.55 vs 0.66 (iter 72)\n\n\nhidden_dim_node=32 simpler is better\nR²=0.47, significantly worse (iter 71)\n\n\nlr_sub=0.0012 helps MLP\\(_{\\text{sub}}\\)\nR²=0.58 vs 0.66 (iter 74)\n\n\nseed=42 is the best seed\nseed=77 achieved R²=0.764 (iter 82), surpassing seed=42’s best of 0.736\n\n\nAdjacent seeds share properties\nseed=78 R²=0.39 (iter 83), seed=79 R²=0.47 (iter 79) — both poor\n\n\nk_floor=1.5 is reproducible\nSame config gave R²=0.70 (iter 64) and R²=0.51 (iter 78)\n\n\n\n\n\n\n\nIs R² \\(\\approx\\) 0.77 achievable? After 84 iterations, the best R²=0.764 (seed=77+sub_diff=8). Training variance of $$0.08 means a lucky run could reach 0.77+, but systematic improvement is unclear.\nHow much variance is seed-dependent vs stochastic? Identical configs yield $$0.08 R² (iter 45 vs 69, iter 80 vs 81). Seed choice adds another $$0.15 on top.\nCan ensemble averaging across seeds reduce variance? Averaging predictions from multiple seeds could improve robustness beyond any single run.\nAre there more “golden seeds” like 77? Adjacent seeds (76, 78, 79) are poor — what makes seed=77 special?\nWhy is the k_floor response non-monotonic? k_floor=1.25 is worse than both 1.0 and 1.5 (iter 65). The mechanism is unknown.\nIs the remaining 24% error from identifiability issues? Multiple \\(k\\) combinations may produce similar \\(dc/dt\\)."
  },
  {
    "objectID": "results.html#rate-constant-recovery-oscillatory-regime-rank-50",
    "href": "results.html#rate-constant-recovery-oscillatory-regime-rank-50",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the oscillatory regime (activity rank \\(\\sim50\\)): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j \\in [10^{-2}, 10^{-1}]\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n84 iterations completed across 7 blocks (21 batches). The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). 100% autocatalytic 3-cycles.\n\n\n\n\n\n\n\n\n\nThe primary metric is raw R² computed on all 256 reactions (after MLP\\(_{\\text{sub}}\\) scalar correction). The trimmed R² excludes outlier reactions (\\(|\\Delta \\log_{10} k| &gt; 0.3\\)) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 85))\nr2 = [\n    # Block 1 (Iter 1-12): Initial exploration\n    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep\n    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough\n    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation\n    # Block 2 (Iter 13-24): k_floor breakthrough\n    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough\n    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor\n    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best\n    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough\n    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns\n    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches\n    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH\n    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7\n    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub\n    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed\n    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST\n    # Block 5 (Iter 49-60): Confirming optimum\n    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests\n    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds\n    0.701, 0.718, 0.603, 0.603,  # Batch 15: seed=99 promising\n    # Block 6 (Iter 61-72): Variance discovery\n    0.688, 0.430, 0.616, 0.704,  # Batch 16: k_floor=1.5 promising\n    0.565, 0.674, 0.664, 0.609,  # Batch 17: k_floor non-monotonic\n    0.658, 0.639, 0.473, 0.550,  # Batch 18: HIGH VARIANCE discovered\n    # Block 7 (Iter 73-84): seed=77 breakthrough\n    0.722, 0.583, 0.682, 0.636,  # Batch 19: seed=123 improved\n    0.694, 0.515, 0.473, 0.748,  # Batch 20: seed=77 NEW BEST\n    0.661, 0.764, 0.387, 0.689,  # Batch 21: seed=77+sub_diff=8 PEAK\n]\noutliers = [\n    43, 47, 45, 53,\n    61, 36, 45, 32,\n    27, 38, 57, 30,\n    28, 33, 36, 33,\n    17, 24, 26, 35,\n    16, 29, 19, 24,\n    18, 18, 15, 17,\n    19, 14, 21, 21,\n    21, 21, 15, 22,  # Batch 9\n    16, 19, 20, 21,  # Batch 10\n    21, 16, 18, 19,  # Batch 11\n    15, 20, 23, 21,  # Batch 12\n    12, 21, 12, 25,  # Batch 13\n    21, 21, 18, 19,  # Batch 14\n    17, 17, 16, 20,  # Batch 15\n    17, 20, 17, 16,  # Batch 16\n    17, 14, 16, 14,  # Batch 17\n    19, 14, 22, 17,  # Batch 18\n    19, 16, 22, 20,  # Batch 19\n    16, 16, 32, 12,  # Batch 20\n    16, 15, 23, 18,  # Batch 21\n]\n\n# Color by block\nblock_colors = ['#3498db', '#2ecc71', '#e67e22', '#9b59b6', '#1abc9c', '#e84393', '#f39c12']\nblock_bounds = [12, 24, 36, 48, 60, 72, 84]\ncolors = []\nfor i in range(len(r2)):\n    for b, bound in enumerate(block_bounds):\n        if i &lt; bound:\n            colors.append(block_colors[b])\n            break\n\nfig, ax1 = plt.subplots(figsize=(16, 5))\n\nbars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)\nax1.axhline(y=0.764, color='#f39c12', linestyle='--', alpha=0.4, label='Peak R² = 0.764')\nax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')\n\n# Block separators and labels\nfor b in range(len(block_bounds) - 1):\n    ax1.axvline(x=block_bounds[b] + 0.5, color='gray', linestyle=':', alpha=0.4)\nblock_labels = ['Block 1', 'Block 2', 'Block 3', 'Block 4', 'Block 5', 'Block 6', 'Block 7']\nblock_centers = [6.5, 18.5, 30.5, 42.5, 54.5, 66.5, 78.5]\nfor i, (lbl, cx) in enumerate(zip(block_labels, block_centers)):\n    ax1.text(cx, 1.0, lbl, ha='center', fontsize=8, color=block_colors[i])\n\n# Annotate key events\nax1.annotate('k_floor=1.0\\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.25),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(35, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('sub_diff=7', xy=(45, 0.736), xytext=(45, 0.86),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\nax1.annotate('seed=77+\\nsub_diff=8', xy=(82, 0.764), xytext=(82, 0.90),\n            arrowprops=dict(arrowstyle='-&gt;', color='black'), fontsize=7, ha='center', color='black')\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('rate_constants R² (raw)')\nax1.set_ylim(-0.02, 1.05)\nax1.set_xticks([1, 12, 14, 21, 24, 35, 36, 45, 48, 60, 72, 80, 82, 84])\n\n# outlier count on secondary axis\nax2 = ax1.twinx()\nax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')\nax2.set_ylabel('outlier count', color='#e74c3c')\nax2.tick_params(axis='y', labelcolor='#e74c3c')\nax2.set_ylim(0, 70)\n\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)\n\nax1.set_title('UCB Exploration: Rate Constant Recovery (84 iterations)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Raw R² (bars) and outlier count (dots) across 84 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74), seed=77+sub_diff=8 (iter 82, 0.76). Training variance of ±0.08 R² is a dominant factor beyond Block 5.\n\n\n\n\n\n\n\n\nAll 12 iterations achieved raw R² &lt; 0.07. The key discovery was that coeff_MLP_sub_norm=1.0 is essential — it corrects the MLP\\(_{\\text{sub}}\\) function shapes (c² becomes quadratic instead of linear) and enables MLP\\(_{\\text{node}}\\) to learn homeostasis.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n1 (iter 1–4)\nlr sweep\n0.044\nAll failed; MLP\\(_{\\text{node}}\\) dead, MLP\\(_{\\text{sub}}\\) c² linear\n\n\n2 (iter 5–8)\nsub_norm=1.0\n0.067\nMLP\\(_{\\text{sub}}\\) normalization is the single most effective change\n\n\n3 (iter 9–12)\ncombine best\n0.061\nMLP\\(_{\\text{node}}\\) activated; lr_node=0.005 hurts\n\n\n\n\n\n\nThe coeff_k_floor=1.0 penalty at iteration 14 produced a 10× improvement in R² (0.06 → 0.51) by preventing outlier \\(\\log k\\) values from drifting below the true minimum. Longer training then pushed R² to 0.69.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n4 (iter 13–16)\nk_floor=1.0\n0.508\nBreakthrough — R² jumped 10×\n\n\n5 (iter 17–20)\naug=3000\n0.642\nLonger training + k_floor synergistic\n\n\n6 (iter 21–24)\naug=4000\n0.690\nFirst to reach 0.69; \\(\\alpha = 0.85\\), 16 outliers\n\n\n\nBest result — Iteration 82: seed=77, coeff_MLP_sub_diff=8, lr_sub=0.001, data_augmentation_loop=4500, coeff_k_floor=1.0, coeff_MLP_sub_norm=1.0\n\n\n\nMetric\nValue\n\n\n\n\nRaw R²\n0.764\n\n\nOutliers\n15 / 256 (5.9%)\n\n\nSlope\n0.97\n\n\n\\(\\alpha\\)\n0.87\n\n\n\nNote: Optimal sub_diff is seed-dependent: 7 for seed=42 (R²=0.74), 8 for seed=77 (R²=0.76). Training has intrinsic variance of $$0.08 R² even with identical configuration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline). R² = 0.044, 43 outliers. Before k_floor and sub_norm — predicted \\(\\log k\\) values scatter widely with no correlation to ground truth.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The k_floor=1.0 penalty prevents outlier \\(\\log k\\) from drifting below the true minimum, producing a 10\\(\\times\\) R² jump.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Iteration 82 (best). R² = 0.764, 15 outliers. seed=77 + sub_diff=8 + aug=4500 yields the tightest clustering around the diagonal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Kinograph montage for iteration 82 (seed=77, sub_diff=8). Top-left: ground-truth \\(dc/dt\\); top-right: GNN prediction; bottom-left: residual (same color scale as GT); bottom-right: predicted vs. true \\(dc/dt\\) scatter. The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Iteration 1 (baseline). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.42 — under-scaled. MLP\\(_{\\text{node}}\\): flat at zero.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Iteration 82 (best R²). MLP\\(_{\\text{sub}}\\): \\(\\alpha\\) at \\(|s|=1\\) is 0.87 — close to the true scale. MLP\\(_{\\text{node}}\\): still flat at zero across all 84 iterations — homeostasis not learned.\n\n\n\n\n\n\n\n\n\nTwelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling lr_sub from 0.0005 to 0.001 broke through to R² = 0.726.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n7 (iter 25–28)\naug=5000, seed, batch_size\n0.652\naug=5000 hurt R² but \\(\\alpha=0.95\\) (best ever)\n\n\n8 (iter 29–32)\nL1=0, sub_norm=2.0, lr_k\n0.619\nsub_norm=2.0: fewest outliers (14), best slope (0.99)\n\n\n9 (iter 33–36)\nsub_norm=2.0+aug=3500, recurrent, lr_sub=0.001, aug=3500\n0.726\nlr_sub=0.001 broke the plateau\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstrategies = {\n    'No k_floor\\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],\n    'k_floor\\nsub_diff=5\\nseed=42': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],\n    'sub_diff=7\\nseed=42': [0.736, 0.696, 0.655, 0.662, 0.560, 0.701, 0.718, 0.658],\n    'seed=77\\nsub_diff=8': [0.748, 0.661, 0.764],\n    'Other seeds\\n(7,99,123)': [0.688, 0.430, 0.722, 0.694, 0.387, 0.689],\n    'Arch/k_floor\\nchanges': [0.616, 0.704, 0.565, 0.674, 0.664, 0.609, 0.639, 0.473, 0.550, 0.583, 0.515, 0.473, 0.636, 0.603, 0.603],\n}\n\nfig, ax = plt.subplots(figsize=(12, 5))\npositions = []\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#95a5a6']\nfor i, (label, vals) in enumerate(strategies.items()):\n    x = np.random.normal(i, 0.08, len(vals))\n    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3, color=colors[i], edgecolors='none')\n    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)\n    positions.append(i)\n\nax.set_xticks(positions)\nax.set_xticklabels(list(strategies.keys()), fontsize=8)\nax.set_ylabel('raw R²', fontsize=12)\nax.set_ylim(-0.05, 0.90)\nax.axhline(y=0.764, color='#f39c12', linestyle='--', alpha=0.3, label='Peak R² = 0.764')\nax.grid(axis='y', alpha=0.3)\nax.legend(fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Impact of key strategies on R². Each dot is one iteration. The k_floor penalty is the single most important factor. Optimal sub_diff is seed-dependent: sub_diff=7 for seed=42, sub_diff=8 for seed=77. Training has intrinsic variance of ±0.08 R².\n\n\n\n\n\n\n\n\nBlock 4 first tested combinations with lr_sub=0.001 and revealed seed sensitivity ($$0.2 R²). Then sub_diff=7 (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n10 (iter 37–40)\nsub_norm=2.0, lr_sub=0.002, lr_node=0.002, L1=0\n0.662\nAll combinations with lr_sub=0.001 hurt R² vs baseline\n\n\n11 (iter 41–44)\nseed=123, aug=4500, sub_diff=3, lr_k=0.007\n0.690\naug=4500 most stable (\\(\\alpha = 0.94\\)); seed=123 crashed to R²=0.49\n\n\n12 (iter 45–48)\nsub_diff=7, lr_k=0.004, hidden_dim=128, batch_size=16\n0.736\nsub_diff=7 new best; wider/deeper MLP and larger batch hurt\n\n\n\nIteration 45 (sub_diff=7, aug=4500) achieved R² = 0.736 with \\(\\alpha = 0.90\\) and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46–48 confirmed that lr_k=0.004 is too slow, hidden_dim_sub=128 allows degenerate solutions, and batch_size=16 degrades convergence.\n\n\n\nBlock 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n13 (iter 49–52)\naug=5000, sub_diff=8, seed=123, n_layers=4\n0.696\naug=5000 hurts (confirmed twice); sub_diff=7 improves seed robustness ($$0.08 vs $$0.24)\n\n\n14 (iter 53–56)\naug=4250, sub_diff=6, seed=123+aug=4000, lr_sub=0.0015\n0.701\nAll variations hurt; sub_diff=6 too weak, lr_sub=0.0015 too high\n\n\n15 (iter 57–60)\nseed=123+aug=3500, seed=99, L1=0.5, sub_norm=0.5\n0.718\nseed=99 promising (R²=0.72, \\(\\alpha=0.92\\)); sub_norm=0.5 confirmed essential principle\n\n\n\nTight bounds confirmed: aug=4500 (not 4000, 4250, or 5000), sub_diff=7 (not 5, 6, or 8), lr_sub=0.001 (not 0.0015 or 0.002), lr_k=0.005 (not 0.004 or 0.007). sub_diff=7 improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with sub_diff=5) to 0.08.\n\n\n\nBlock 6 explored new seeds and k_floor variations, then discovered that training has high intrinsic variance — an exact replica of the best configuration (iter 45) achieved only R²=0.66 instead of 0.74.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n16 (iter 61–64)\nseed=7, seed=99+aug=4250, lr_node=0.0005, k_floor=1.5\n0.704\nk_floor=1.5 promising (R²=0.70, \\(\\alpha=0.92\\)); seed=99 very sensitive to aug\n\n\n17 (iter 65–68)\nk_floor=1.25, k_floor=1.5+seed=99, aug=4750, sub_diff=6+k_floor=1.5\n0.674\nk_floor response is non-monotonic: 1.25 &lt; 1.0 and 1.5; aug=4750 hurts\n\n\n18 (iter 69–72)\nExact replica of iter 45, lr_k=0.0045, hidden_dim_node=32, sub_norm=1.5\n0.658\nHIGH VARIANCE: replica got R²=0.66 vs original 0.74; MLP\\(_{\\text{sub}}\\) c² failure mode\n\n\n\nThe high variance discovery is significant: identical configurations can produce R² values differing by 0.08, meaning many earlier “improvements” may have been within noise. This makes seed selection and reproducibility central concerns.\n\n\n\nBlock 7 explored new seeds and found that seed=77 is a “golden seed” — achieving R²=0.748 (iter 80) and then R²=0.764 with sub_diff=8 (iter 82), surpassing the previous best.\n\n\n\n\n\n\n\n\n\nBatch\nKey mutation\nBest R²\nFinding\n\n\n\n\n19 (iter 73–76)\nseed=123, lr_sub=0.0012, sub_diff=9, aug=4000\n0.722\nseed=123 improved to R²=0.72 (vs 0.66 earlier); all other variations hurt\n\n\n20 (iter 77–80)\nseed=7, k_floor=1.5, n_layers=4, seed=77\n0.748\nseed=77 new best seed (R²=0.748, 12 outliers); n_layers=4 confirmed harmful\n\n\n21 (iter 81–84)\nReplicate seed=77, seed=77+sub_diff=8, seed=78, aug=5000\n0.764\nseed=77+sub_diff=8 = new global best; replica got R²=0.66 (variance); seed=78 poor (R²=0.39)\n\n\n\nThe key finding: optimal sub_diff is seed-dependent. sub_diff=8 hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76). Adjacent seeds (78, 79) do not share seed=77’s properties — “golden seeds” are rare and unpredictable.\n\n\n\n\n\n\n\n\n\nFigure 11: UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the k_floor=1.0 breakthrough (node 14) and converged on the aug=4000 regime. The tree has since grown to 84 nodes; the overall best is node 82 (R² = 0.764).\n\n\n\n\n\n\nThe LLM’s persistent memory has accumulated these validated principles after 84 iterations:\n\ncoeff_MLP_sub_norm=1.0 is essential — enables correct MLP shapes: c² becomes quadratic, MLP\\(_{\\text{node}}\\) activates. sub_norm=0.5 and sub_norm=1.5 both hurt (iter 60, 72).\ncoeff_k_floor=1.0 is critical — R² jumped from 0.06 to 0.51 (10× improvement). k_floor=2.0 too strong. k_floor=1.5 gives R²=0.70 but response is non-monotonic: k_floor=1.25 is worse than both 1.0 and 1.5 (iter 65).\nLonger training helps (up to aug=4500) — aug=2000→3000→4000→4500 consistently improves R². aug=5000 hurts R² (confirmed 3×: iter 25, 49, 84). aug=4750 also hurts (iter 67). aug=4000 slightly suboptimal (iter 76).\nlr_k=0.005 is optimal — lower (0.003, 0.004, 0.0045) too slow, higher (0.007, 0.01) destabilizes (confirmed across 6 iterations)\nlr_sub=0.001 is optimal — 2× increase from 0.0005 broke the R² plateau; lr_sub=0.002 too high (iter 38); lr_sub=0.0015 hurts (iter 56); lr_sub=0.0012 hurts (iter 74)\nlr_node=0.001 is optimal — lr_node=0.005 destabilizes (iter 11); lr_node=0.0005 hurts (iter 63)\nOptimal sub_diff is seed-dependent — sub_diff=7 for seed=42 (R²=0.74, iter 45), sub_diff=8 for seed=77 (R²=0.76, iter 82). sub_diff=8 hurts seed=42 (iter 50) but helps seed=77.\nTraining has high intrinsic variance ($$0.08 R²) — exact replicas: iter 45 R²=0.74 vs iter 69 R²=0.66; iter 80 R²=0.75 vs iter 81 R²=0.66. Many earlier “improvements” may be within noise.\nseed=77 is the best seed — R²=0.748 (iter 80), R²=0.764 with sub_diff=8 (iter 82). Adjacent seeds (78, 79) are poor — “golden seeds” are rare and unpredictable.\nbatch_size=8 is optimal — batch_size=16 hurts R² (iter 48)\nDefault MLP architecture is optimal — hidden_dim_sub=128 allows degenerate solutions (iter 47); hidden_dim_node=32 significantly worse (iter 71); n_layers_sub=4 hurts (iter 52, 79)\ncoeff_MLP_node_L1=1.0 is optimal — L1=0.0 + long training harmful (iter 22, 29, 40); L1=0.5 hurts (iter 59)\nsub_diff=7 improves seed robustness — R² gap between seed=42 and seed=123 dropped from 0.24 (with sub_diff=5) to 0.08 (with sub_diff=7)\nMLP\\(_{\\text{node}}\\) remains flat across all 84 iterations — homeostasis \\(-\\lambda(c - c^{\\text{base}})\\) is never learned, regardless of configuration\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nEvidence\n\n\n\n\nRecurrent training breaks degeneracy\nNo R² improvement, 3.5× slower (iter 13, 34)\n\n\nSmaller MLP improves k recovery\nWorst R² = 0.011 (iter 15)\n\n\nHigher lr_node activates MLP\\(_{\\text{node}}\\)\nlr_node=0.005 hurts (iter 11); 0.002 no effect (iter 26, 39)\n\n\nDifferent seed breaks degeneracy\nSame MLP\\(_{\\text{node}}\\) flatness (iter 27); R² variance $$0.2 (iter 41)\n\n\nStronger monotonicity (sub_diff=10) helps\nR² dropped to 0.41 (iter 32)\n\n\nWeaker monotonicity (sub_diff=3) helps\nR² dropped to 0.61 (iter 43)\n\n\nSmaller batch size helps convergence\nR² dropped (iter 28)\n\n\naug=5000 continues to improve\nConfirmed 3×: iter 25 (0.65), iter 49 (0.70), iter 84 (0.69)\n\n\nlr_sub=0.002 improves over 0.001\nR² dropped from 0.73 to 0.52 (iter 38)\n\n\nsub_norm=2.0 improves R²\nImproves \\(\\alpha\\) but hurts R² (iter 30, 37)\n\n\nCombining lr_sub=0.001 with other changes\nAll combinations hurt R² vs baseline (iter 37–40)\n\n\nWider MLP\\(_{\\text{sub}}\\) (hidden_dim=128) helps\nR² dropped to 0.56 — allows degenerate solutions (iter 47)\n\n\nLarger batch_size=16 stabilizes gradients\nR² dropped to 0.56 (iter 48)\n\n\nsub_diff=8 hurts all seeds\nHurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76)\n\n\nsub_diff=6 is better than 7\nR² dropped from 0.74 to 0.56 (iter 54)\n\n\nDeeper MLP\\(_{\\text{sub}}\\) (n_layers=4) helps\nR² dropped to 0.55 (iter 52); confirmed iter 79 (R²=0.47)\n\n\nlr_sub=0.0015 (intermediate) helps\nR² dropped from 0.74 to 0.60 (iter 56)\n\n\nlr_k=0.004 gives finer convergence\nR² dropped to 0.48 — too slow (iter 46)\n\n\nsub_norm=0.5 helps\nR² dropped from 0.74 to 0.60 (iter 60)\n\n\nIntermediate k_floor=1.25 is optimal\nR²=0.56, worse than both 1.0 and 1.5 — non-monotonic (iter 65)\n\n\naug=4750 within safe range\nR² dropped to 0.66 despite \\(\\alpha\\)=0.96 (iter 67)\n\n\nsub_norm=1.5 better than 1.0\nR²=0.55 vs 0.66 (iter 72)\n\n\nhidden_dim_node=32 simpler is better\nR²=0.47, significantly worse (iter 71)\n\n\nlr_sub=0.0012 helps MLP\\(_{\\text{sub}}\\)\nR²=0.58 vs 0.66 (iter 74)\n\n\nseed=42 is the best seed\nseed=77 achieved R²=0.764 (iter 82), surpassing seed=42’s best of 0.736\n\n\nAdjacent seeds share properties\nseed=78 R²=0.39 (iter 83), seed=79 R²=0.47 (iter 79) — both poor\n\n\nk_floor=1.5 is reproducible\nSame config gave R²=0.70 (iter 64) and R²=0.51 (iter 78)\n\n\n\n\n\n\n\nIs R² \\(\\approx\\) 0.77 achievable? After 84 iterations, the best R²=0.764 (seed=77+sub_diff=8). Training variance of $$0.08 means a lucky run could reach 0.77+, but systematic improvement is unclear.\nHow much variance is seed-dependent vs stochastic? Identical configs yield $$0.08 R² (iter 45 vs 69, iter 80 vs 81). Seed choice adds another $$0.15 on top.\nCan ensemble averaging across seeds reduce variance? Averaging predictions from multiple seeds could improve robustness beyond any single run.\nAre there more “golden seeds” like 77? Adjacent seeds (76, 78, 79) are poor — what makes seed=77 special?\nWhy is the k_floor response non-monotonic? k_floor=1.25 is worse than both 1.0 and 1.5 (iter 65). The mechanism is unknown.\nIs the remaining 24% error from identifiability issues? Multiple \\(k\\) combinations may produce similar \\(dc/dt\\)."
  },
  {
    "objectID": "results.html#why-homeostasis-is-not-learned",
    "href": "results.html#why-homeostasis-is-not-learned",
    "title": "Results",
    "section": "Why Homeostasis Is Not Learned",
    "text": "Why Homeostasis Is Not Learned\nAcross all 84 iterations, MLP\\(_{\\text{node}}\\) remains flat — the homeostasis function \\(-\\lambda_t(c_i - c_i^{\\text{baseline}})\\) is never recovered. This is not a hyperparameter issue. It is a structural limitation of single-step (\\(t \\to t+1\\)) training.\n\nThe scale mismatch problem\nThe GNN predicts \\(dc/dt\\) at each time step:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\prod_k \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{reaction}}\n\\]\nThe reaction term dominates the instantaneous \\(dc/dt\\) — it drives the fast oscillatory dynamics. Homeostasis acts as a slow restoring force that only manifests over many time steps: it prevents concentrations from drifting away from baseline. In a single-step loss \\(\\| \\hat{y}_{t+1} - y_{t+1} \\|^2\\), the gradient signal from homeostasis is negligible compared to the reaction term. The optimizer has no reason to learn it.\n\n\nHomeostasis is an integral problem\nHomeostasis determines the long-term trajectory envelope, not the instantaneous derivative. Its effect accumulates over time:\n\\[\nc_i(t + T) \\approx c_i(t) + \\int_t^{t+T} \\left[ -\\lambda_i (c_i - c_i^{\\text{base}}) + \\text{reaction terms} \\right] d\\tau\n\\]\nA model trained on \\(t \\to t+1\\) can achieve low loss by fitting the dominant reaction signal and ignoring the small homeostatic correction. Over many steps this error accumulates — but the single-step loss never sees it.\n\n\nProposed approach: two-phase training\nTo recover homeostasis, we propose a two-phase training scheme:\n\nPhase 1 — Reaction recovery (\\(t \\to t+1\\)): Train as currently done. Recover \\(k_j\\), MLP\\(_{\\text{sub}}\\), and the stoichiometric structure. Freeze these parameters.\nPhase 2 — Homeostasis recovery (recurrent, \\(t \\to t+T\\)): With the reaction term frozen, train only MLP\\(_{\\text{node}}\\) and embeddings \\(a_i\\) using multi-step rollout. The recurrent loss forces the model to match trajectories over \\(T\\) steps, where the homeostatic drift becomes visible.\n\nThis separates the two learning problems by time scale: fast reactions are learned from instantaneous gradients, slow homeostasis from trajectory matching. The key insight is that recurrent training is not needed for \\(k_j\\) recovery (it was tried and failed — iter 13, 34), but may be essential specifically for homeostasis once the reaction parameters are frozen.\n\n\nAlternative approach: residual-based supervision\nThe kinograph residual (Figure 7, bottom-left) directly reveals what the single-step model cannot learn. After Phase 1, we can roll out the learned model autoregressively:\n\\[\n\\hat{c}_i(t+1) = \\hat{c}_i(t) + \\Delta t \\cdot \\left[ \\sum_j S_{ij} \\cdot k_j \\prod_k \\text{MLP}_{\\text{sub}}(\\hat{c}_k, |S_{kj}|) \\right]\n\\]\nThe rollout trajectory \\(\\hat{c}(t)\\) drifts from the observation \\(c(t)\\) because the missing homeostatic restoring force is not applied at each step. The accumulated residual\n\\[\nr_i(t) = c_i(t) - \\hat{c}_i(t)\n\\]\nis precisely the integrated effect of the missing slow terms — homeostasis, external sources, and degradation. This residual provides a direct supervision signal for Phase 2 without requiring backpropagation through time: we can compute a per-step target\n\\[\n\\frac{r_i(t+1) - r_i(t)}{\\Delta t} \\approx \\text{MLP}_{\\text{node}}(c_i(t), a_i)\n\\]\nand train MLP\\(_{\\text{node}}\\) with a standard single-step loss on this derived target. This avoids the instabilities of recurrent training while still capturing the long-term dynamics that the reaction-only model structurally cannot predict.\nThis approach is an instance of the Universal Differential Equations framework (Rackauckas et al., 2021), where a partially known ODE is augmented with a neural network that learns the missing terms from data. Here, the known part is the reaction dynamics \\(\\sum_j S_{ij} k_j \\prod_k \\text{MLP}_{\\text{sub}}\\), and the unknown part is the homeostatic correction learned by MLP\\(_{\\text{node}}\\). The residual-based supervision strategy is also related to PDE-Refiner (Lippe et al., NeurIPS 2023), which uses iterative refinement on rollout residuals to recover low-amplitude dynamics that single-pass neural solvers miss — analogous to the small homeostatic signal masked by dominant reactions.\n\n\nRelated work\nThe difficulty of learning slow dynamics from single-step training is well established in the literature:\n\nTeacher forcing and long-term dependencies. Williams & Zipser (1989) introduced teacher forcing for RNNs: feeding ground-truth inputs at each step. Bengio et al. (1994) showed that gradient-based learning of long-term dependencies is fundamentally difficult because short-term gradient contributions dominate, exactly the mechanism that prevents our GNN from learning homeostasis.\nScheduled sampling. Bengio et al. (2015) proposed a curriculum strategy that gradually transitions from teacher forcing (single-step) to free-running (multi-step) prediction, reducing the train–inference discrepancy. Our two-phase proposal is conceptually related: Phase 1 is teacher-forced, Phase 2 uses multi-step rollout.\nMultiple shooting for Neural ODEs. Massaroli et al. (2021) introduced differentiable multiple shooting layers that parallelize trajectory integration. Turan & Jäschke (2021) showed that standard Neural ODE fitting on oscillatory data produces “flattened” trajectories — multiple shooting recovers the true dynamics by breaking long horizons into segments. This directly addresses the failure mode we observe.\nStiff Neural ODEs. Kim et al. (2021) showed that learning neural ODEs for systems with widely separated time scales (stiff systems) requires proper output scaling and stabilized gradients — the fast dynamics otherwise dominate training, exactly as in our reaction-vs-homeostasis scale mismatch.\nMulti-scale separation in dynamical systems. Fenichel (1979) established geometric singular perturbation theory: for systems with fast and slow time scales, the fast subsystem is solved first, then the dynamics are reduced to a slow manifold. Our two-phase training mirrors this decomposition — Phase 1 recovers the fast reaction dynamics, Phase 2 learns the slow homeostatic manifold.\nLatent timescales in Neural ODEs. Gupta et al. (2024) showed that training trajectory length directly controls the timescales a Neural ODE can recover: longer trajectories are needed to capture slower dynamics. This supports the need for multi-step rollout in Phase 2.\n\nThe scale mismatch / integral argument is essentially the same observation that Bengio (1994) made for RNNs and Kim et al. (2021) made for stiff ODEs. The two-phase proposal mirrors Fenichel’s fast-then-slow decomposition. The novelty here is applying this reasoning to GNN-based metabolic network recovery, where the bipartite graph structure and the identifiability of rate constants \\(k_j\\) add domain-specific constraints."
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#metabolic-networks-as-bipartite-graphs",
    "href": "model.html#metabolic-networks-as-bipartite-graphs",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#the-stoichiometric-matrix",
    "href": "model.html#the-stoichiometric-matrix",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Stoichiometric Matrix",
    "text": "The Stoichiometric Matrix\nThe stoichiometric matrix \\(\\mathbf{S}\\) is an \\((n_{\\text{metabolites}} \\times n_{\\text{reactions}})\\) matrix where entry \\(S_{ij}\\) indicates how metabolite \\(i\\) participates in reaction \\(j\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix}\n-1 & 0 & \\cdots \\\\\n-1 & +1 & \\cdots \\\\\n+2 & -1 & \\cdots \\\\\n+1 & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\]\nProperties of S:\n\nSparse: most entries are zero (each reaction involves only 2-6 metabolites)\nInteger-valued: entries are typically in \\(\\{-2, -1, 0, +1, +2\\}\\)\nMass conservation: column sums should be zero for balanced reactions\n\nExample:\nConsider the diagram above with 2 reactions:\n\\[\n\\begin{aligned}\n\\text{R1}: \\quad & \\text{Glucose} + \\text{ATP} \\longrightarrow 2\\,\\text{Pyruvate} + \\text{ADP} \\\\\n\\text{R2}: \\quad & \\text{Pyruvate} \\longrightarrow \\text{ATP}\n\\end{aligned}\n\\]\nThe corresponding stoichiometric matrix is:\n\\[\n\\begin{array}{c|cc}\n& \\text{R1} & \\text{R2} \\\\\n\\hline\n\\text{Glucose} & -1 & 0 \\\\\n\\text{ATP} & -1 & +1 \\\\\n\\text{Pyruvate} & +2 & -1 \\\\\n\\text{ADP} & +1 & 0\n\\end{array}\n\\]"
  },
  {
    "objectID": "model.html#reaction-kinetics",
    "href": "model.html#reaction-kinetics",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Reaction Kinetics",
    "text": "Reaction Kinetics\n\nMass-Action Law\nThe rate of a chemical reaction is proportional to the product of the concentrations of its substrates, each raised to the power of its stoichiometric coefficient. For a reaction \\(j\\) with substrates \\(k\\):\n\\[v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nThis is the law of mass action (Guldberg & Waage, 1864): a reaction proceeds faster when its substrates are more abundant. The exponent \\(|S_{kj}|\\) reflects how many molecules of substrate \\(k\\) are consumed — a reaction requiring 2 molecules of ATP depends quadratically on ATP concentration (\\(c_{\\text{ATP}}^2\\)), while a reaction consuming 1 molecule depends linearly (\\(c_{\\text{ATP}}^1\\)).\n\n\nSubstrate Aggregation\nEach reaction consumes multiple substrates. The aggregation rule determines how substrate contributions combine to form the reaction rate:\n\n\n\n\n\n\n\n\nAggregation\nFormula\nPhysical meaning\n\n\n\n\nMultiplicative (product)\n\\(v_j = k_j \\cdot \\prod_k c_k^{\\|S_{kj}\\|}\\)\nTrue mass-action: all substrates must be present simultaneously. Rate drops to zero if any substrate is absent.\n\n\nAdditive (sum)\n\\(v_j = k_j \\cdot \\sum_k c_k^{\\|S_{kj}\\|}\\)\nApproximate: substrates contribute independently. Useful when reactions are not strictly mass-action (e.g., enzyme-mediated).\n\n\n\nMultiplicative aggregation is the physically correct form for mass-action kinetics — it encodes the requirement that a reaction can only proceed if all its substrates are available. It also produces richer dynamics: multiplicative coupling between metabolites creates nonlinear feedback loops that can sustain oscillations.\nAdditive aggregation is a simplification where each substrate contributes independently to the rate. It cannot produce sustained oscillations from autocatalytic cycles because it lacks the multiplicative coupling needed for positive feedback."
  },
  {
    "objectID": "model.html#model-evolution",
    "href": "model.html#model-evolution",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Model Evolution",
    "text": "Model Evolution\n\n1. Pure Reaction\nPure reaction dynamics without homeostasis, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nPure reaction dynamics: concentrations evolve under additive mass-action kinetics without homeostasis. Most metabolites equilibrate within ~100 time steps.\n\n\n\n\n2. Homeostasis\nWith homeostatic regulation pulling concentrations toward baseline, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}}_{\\text{reactions}}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.01\n\n\n\\(c^{\\text{baseline}}\\)\n5.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nHomeostatic regulation: concentrations are pulled toward a baseline (\\(c^{\\text{baseline}} = 5.0\\)) by a linear restoring force. Dynamics converge to steady state.\n\n\n\n\n3. Oscillatory\nAutocatalytic cycles with mass-action kinetics (multiplicative aggregation) for sustained oscillations:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nWith multiplicative aggregation, autocatalytic cycles create positive feedback: in \\(A + B \\to 2B\\), the rate \\(v = k \\cdot c_A \\cdot c_B\\) increases with both \\(A\\) and \\(B\\), so producing more \\(B\\) accelerates the reaction — a nonlinear feedback loop that sustains oscillations when cycles are closed (\\(A \\to B \\to C \\to A\\)).\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nAggregation\nProduct (multiplicative)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.5}, 10^{-1}]\\)\n\n\nInitial concentrations\n[1.0, 9.0]\n\n\nFlux limiting\nDisabled\n\n\n\n\n\n\nOscillatory dynamics (activity rank 20): autocatalytic 3-cycles with multiplicative aggregation produce sustained oscillations. The wide rate constant range \\([10^{-2.5}, 10^{-1}]\\) means ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly."
  },
  {
    "objectID": "model.html#activity-rank",
    "href": "model.html#activity-rank",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Activity Rank",
    "text": "Activity Rank\nTo quantify the complexity of concentration dynamics, we compute the activity rank using singular value decomposition (SVD) of the concentration matrix \\(\\mathbf{C} \\in \\mathbb{R}^{T \\times n}\\) (time frames × metabolites).\nThe rank at 99% variance is the number of singular values needed to capture 99% of the total variance:\n\\[\\text{rank}_{99} = \\min \\left\\{ k : \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.99 \\right\\}\\]\nInterpretation:\n\nLow rank (1-5): concentrations are highly correlated, dynamics are simple (equilibration or uniform decay)\nHigh rank (&gt;20): metabolites evolve independently with rich, complex dynamics\n\n\nIncreasing Activity Rank\nWith the baseline oscillatory config (\\(k_j \\in [10^{-2.5}, 10^{-1}]\\), 256 reactions), the activity rank is 20. Activity rank is controlled by the number of actively contributing reactions per metabolite:\n\n\n\nConfig\nReactions\n\\(k_j\\) range\nActivity Rank\n\n\n\n\nBaseline (rank_20)\n256\n\\([10^{-2.5}, 10^{-1}]\\)\n20\n\n\nNarrow \\(k\\) (rank_50)\n256\n\\([10^{-2.0}, 10^{-1}]\\)\n47\n\n\nMore reactions (rank_70)\n512\n\\([10^{-2.0}, 10^{-1}]\\)\n70\n\n\n\nThe most impactful change is narrowing the rate constant range from \\([10^{-2.5}, 10^{-1}]\\) to \\([10^{-2.0}, 10^{-1}]\\). With the wider range, ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly. Removing these inert reactions more than doubles the activity rank (20 → 47). Doubling the number of reactions further increases overlap per metabolite (47 → 70).\n\n\nGNN Training Dataset\nFor GNN training and rate constant recovery, we use the rank_50 config (256 reactions, activity rank 47) — complex enough to test the inverse problem while keeping it tractable:\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.0}, 10^{-1}]\\)\n\n\nAll other parameters\nSame as oscillatory above\n\n\n\n\n\n\nOscillatory dynamics with narrowed rate constants (activity rank 47). Eliminating slow inert reactions produces richer dynamics. This is the primary dataset for GNN training."
  },
  {
    "objectID": "model.html#summary-the-full-model",
    "href": "model.html#summary-the-full-model",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Summary: The Full Model",
    "text": "Summary: The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate follows mass-action kinetics:\n\\[\nv_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\n\\]"
  },
  {
    "objectID": "model.html#the-inverse-problem",
    "href": "model.html#the-inverse-problem",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nGiven observed concentration time series \\(\\mathbf{C}(t)\\) and the bipartite graph structure, the goal is to recover the model components that generated the dynamics. A graph neural network (GNN) operates on the bipartite metabolite–reaction graph and learns two functions and a set of scalar parameters:\n\nWhat the GNN Learns\nThe forward model is:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\prod_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nThe GNN replaces the known functions with learnable MLPs, and the rate constants with learnable parameters. It must recover all three simultaneously from concentration data alone:\n1. Substrate function \\(f_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\)\nA neural network that learns how each substrate concentration contributes to the reaction rate. The ground-truth function is the mass-action power law \\(c_k^{|S_{kj}|}\\), but the GNN does not know this — it must discover the functional form from data:\n\\[\n\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|) \\;\\overset{?}{\\approx}\\; c_k^{|S_{kj}|}\n\\]\nThis is a function discovery problem: the MLP receives concentration and stoichiometric coefficient as inputs and must learn to output the correct power-law relationship. Since the stoichiometric coefficients are typically 1 or 2, the MLP must learn to distinguish between linear (\\(c^1\\)) and quadratic (\\(c^2\\)) dependencies.\n2. Homeostasis function \\(f_{\\text{node}} \\to \\text{MLP}_{\\text{node}}(c_i)\\)\nA neural network that learns the per-metabolite self-regulation term. The ground truth is a linear function \\(-\\lambda_{\\text{type}(i)} (c_i - c_i^{\\text{baseline}})\\) with small magnitude, but the GNN must discover this from data:\n\\[\n\\text{MLP}_{\\text{node}}(c_i) \\;\\overset{?}{\\approx}\\; -\\lambda_{\\text{type}(i)} \\cdot (c_i - c_i^{\\text{baseline}})\n\\]\nThis function captures how each metabolite is regulated independently of reactions — pulling concentrations back toward a baseline level. The challenge is that homeostatic forces are small compared to reaction rates, so \\(\\text{MLP}_{\\text{node}}\\) must learn a subtle signal without absorbing information that belongs to the reaction terms.\n3. Rate constants \\(k_j\\) (256 learnable scalars)\nPer-reaction rate constants learned in log-space. Unlike the MLPs, these are not functions but a vector of 256 scalar parameters — one per reaction — that scale the reaction fluxes.\n\n\nIdentifiability Challenges\nThe three components interact and can compensate for each other:\n\nScale ambiguity: \\(k_j \\cdot \\text{MLP}_{\\text{sub}}\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). Without anchoring, the MLP can absorb a global scale factor and shift all \\(k\\) values. Regularization (\\(\\texttt{coeff\\_k\\_center}\\)) anchors \\(\\text{mean}(\\log k)\\) to the known range center.\nFunction compensation: If \\(\\text{MLP}_{\\text{sub}}\\) learns a wrong functional form (e.g., \\(c^{1.5}\\) instead of \\(c^2\\)), the rate constants can partially compensate by adjusting their values. This leads to degenerate solutions with high prediction accuracy but poor parameter recovery.\nHomeostasis absorption: \\(\\text{MLP}_{\\text{node}}\\) can grow large and absorb dynamics that should be explained by the reaction terms, masking the true rate constants.\n\n\n\nLearning Modes\n\n\n\n\n\n\n\n\n\nMode\nS matrix\nPrimary metric\nChallenge\n\n\n\n\nS learning\nLearnable\nstoichiometry \\(R^2\\)\nRecovering integer coefficients and sparsity\n\n\nS given\nFrozen from GT\nrate constants \\(R^2\\)\nDisentangling \\(k\\), \\(\\text{MLP}_{\\text{sub}}\\), \\(\\text{MLP}_{\\text{node}}\\)"
  },
  {
    "objectID": "gnn-llm-memory.html",
    "href": "gnn-llm-memory.html",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#the-exploration-loop",
    "href": "gnn-llm-memory.html#the-exploration-loop",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#training-scheme",
    "href": "gnn-llm-memory.html#training-scheme",
    "title": "GNN-LLM-Memory",
    "section": "Training Scheme",
    "text": "Training Scheme\nThe GNN is trained by minimizing the prediction error on \\(dc/dt\\):\n\\[\n\\mathcal{L} = \\sum_{\\text{frames}} \\left\\| \\frac{dc}{dt}_{\\text{pred}} - \\frac{dc}{dt}_{\\text{GT}} \\right\\|_2 + \\mathcal{R}\n\\]\nwhere \\(\\mathcal{R}\\) is the sum of regularization terms described below.\n\nSeparate Learning Rates\nEach model component has its own learning rate to control the balance between parameter groups:\n\n\n\n\n\n\n\n\n\nComponent\nConfig key\nControls\nTypical range\n\n\n\n\nRate constants \\(k_j\\)\nlearning_rate_k\nHow fast k values are updated\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{node}}\\) (homeostasis)\nlearning_rate_node\nHomeostasis function learning speed\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{sub}}\\) (substrate)\nlearning_rate_sub\nSubstrate function learning speed\n1E-4 to 1E-2\n\n\n\nThe learning rate balance is critical:\n\nlr_k too high: \\(k\\) values overshoot, oscillate, or converge to wrong values\nlr_k too low: \\(k\\) barely moves, MLPs compensate\nlr_node/lr_sub imbalance: one function absorbs capacity meant for the other\n\n\n\nRegularization Terms\nThe total regularization \\(\\mathcal{R}\\) is the sum of the following penalties:\n\nMLP\\(_{\\text{sub}}\\) Monotonicity (coeff_MLP_sub_diff)\nMLP\\(_{\\text{sub}}\\) learns \\(c^s\\) which should be monotonically increasing in concentration. This penalty samples concentration pairs \\((c, c+\\delta)\\) and penalizes cases where the output decreases:\n\\[\n\\mathcal{R}_{\\text{sub\\_diff}} = \\left\\| \\text{ReLU}\\left(\\|\\text{MLP}_{\\text{sub}}(c)\\| - \\|\\text{MLP}_{\\text{sub}}(c+\\delta)\\|\\right) \\right\\|_2 \\cdot \\lambda_{\\text{sub\\_diff}}\n\\]\nWithout this constraint, MLP\\(_{\\text{sub}}\\) can develop non-physical local minima that don’t match the true power law behavior.\n\n\nMLP\\(_{\\text{node}}\\) L1 (coeff_MLP_node_L1)\nPenalizes large MLP\\(_{\\text{node}}\\) output to keep homeostasis values small relative to reaction terms:\n\\[\n\\mathcal{R}_{\\text{node\\_L1}} = \\text{mean}\\left(|\\text{MLP}_{\\text{node}}(c_i, a_i)|\\right) \\cdot \\lambda_{\\text{node\\_L1}}\n\\]\nMLP\\(_{\\text{node}}\\) is initialized to zero output so homeostasis starts inactive. Combined with a reduced learning rate, this prevents MLP\\(_{\\text{node}}\\) from dominating the dynamics. The true homeostatic \\(\\lambda\\) values are small (0.001–0.002), so MLP\\(_{\\text{node}}\\) output should remain small.\n\n\nMLP\\(_{\\text{sub}}\\) Normalization (coeff_MLP_sub_norm)\nBreaks the scale ambiguity between \\(k\\) and MLP\\(_{\\text{sub}}\\) at the source. The product \\(k_j \\cdot \\text{MLP}_{\\text{sub}}(c)\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). This penalty enforces that MLP\\(_{\\text{sub}}\\) outputs 1 at the reference point \\(c=1, |s|=1\\), where the true value \\(c^s = 1^1 = 1\\):\n\\[\n\\mathcal{R}_{\\text{sub\\_norm}} = \\left(\\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\| - 1\\right)^2 \\cdot \\lambda_{\\text{sub\\_norm}}\n\\]\nSince \\(c^s = 1\\) at \\(c=1\\) for any stoichiometry \\(s\\), this pins the MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) to 1 and prevents \\(k\\) from absorbing a global factor.\n\n\nRate Constant Floor (coeff_k_floor)\nPrevents \\(\\log_{10} k_j\\) from drifting far below the physically plausible range. Without this, some reactions develop outlier values (e.g. \\(\\log k = -4\\) when the true range is \\([-2, -1]\\)), which distorts the \\(R^2\\) even when most reactions are well-recovered:\n\\[\n\\mathcal{R}_{\\text{k\\_floor}} = \\sum_j \\text{ReLU}\\left(\\tau - \\log_{10} k_j\\right)^2 \\cdot \\lambda_{\\text{k\\_floor}}\n\\]\nwhere \\(\\tau\\) is the configurable threshold (k_floor_threshold, default \\(-3\\)). Only \\(\\log k\\) values below \\(\\tau\\) are penalized.\n\n\n\nScalar Correction\nEven without the normalization regularization, a post-hoc scalar correction is applied when evaluating rate constants. The MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) is measured by evaluating MLP\\(_{\\text{sub}}\\) at the reference point:\n\\[\n\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\n\\]\nWith multiplicative aggregation, each reaction rate absorbs \\(\\alpha^{n_j}\\) where \\(n_j\\) is the number of substrates. The corrected rate constants are:\n\\[\n\\log_{10} k_j^{\\text{corrected}} = \\log_{10} k_j^{\\text{learned}} + n_j \\cdot \\log_{10} \\alpha\n\\]\nThe reported rate_constants_R2 is computed on these corrected values against the identity line \\(y=x\\).\n\n\nSummary of Hyperparameters\n\n\n\n\n\n\n\n\n\nParameter\nConfig key\nDescription\nTypical range\n\n\n\n\nLearning rate \\(k\\)\nlearning_rate_k\nRate constants update speed\n1E-4 to 1E-2\n\n\nLearning rate node\nlearning_rate_node\nMLP\\(_{\\text{node}}\\) update speed\n1E-4 to 1E-2\n\n\nLearning rate sub\nlearning_rate_sub\nMLP\\(_{\\text{sub}}\\) update speed\n1E-4 to 1E-2\n\n\nBatch size\nbatch_size\nTime frames per gradient step\n4 to 32\n\n\nTraining iterations\ndata_augmentation_loop\nMultiplier for iterations per epoch\n100 to 5000\n\n\nEpochs\nn_epochs\nNumber of training epochs\n1 to 5\n\n\nTime step\ntime_step\nSteps per gradient update (1 = single-step, &gt;1 = recurrent rollout)\n1 to 8\n\n\nRecurrent training\nrecurrent_training\nEnable multi-step rollout training\ntrue / false\n\n\nMLP\\(_{\\text{sub}}\\) monotonicity\ncoeff_MLP_sub_diff\nPenalize non-increasing MLP\\(_{\\text{sub}}\\)\n0 to 500\n\n\nMLP\\(_{\\text{node}}\\) L1\ncoeff_MLP_node_L1\nPenalize large homeostasis output\n0 to 10\n\n\nMLP\\(_{\\text{sub}}\\) normalization\ncoeff_MLP_sub_norm\nPin MLP\\(_{\\text{sub}}(c{=}1, |s|{=}1)\\) to 1\n0 to 10\n\n\nRate constant floor\ncoeff_k_floor\nPenalize \\(\\log k\\) below threshold\n0 to 10\n\n\nFloor threshold\nk_floor_threshold\nThreshold for k floor penalty\n\\(-3\\) (default)\n\n\nMLP\\(_{\\text{sub}}\\) hidden dim\nhidden_dim_sub\nHidden layer width for substrate MLP\n16 to 128\n\n\nMLP\\(_{\\text{sub}}\\) layers\nn_layers_sub\nNumber of layers for substrate MLP\n2 to 5\n\n\nMLP\\(_{\\text{node}}\\) hidden dim\nhidden_dim_node\nHidden layer width for homeostasis MLP\n16 to 128\n\n\nMLP\\(_{\\text{node}}\\) layers\nn_layers_node\nNumber of layers for homeostasis MLP\n2 to 5"
  },
  {
    "objectID": "gnn-llm-memory.html#metrics",
    "href": "gnn-llm-memory.html#metrics",
    "title": "GNN-LLM-Memory",
    "section": "Metrics",
    "text": "Metrics\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood value\n\n\n\n\nrate_constants_R2\nR² between learned and true rate constants \\(k\\) (after scalar correction, excluding outliers)\n&gt; 0.9\n\n\nn_outliers\nNumber of reactions with \\(|\\Delta \\log_{10} k| &gt; 0.3\\) (factor of 2 error in \\(k\\)-space)\n&lt; 25\n\n\nslope\nSlope of linear fit between learned and true \\(\\log_{10} k\\). Slope \\(&lt; 1\\) means the learned range is compressed.\n\\(\\approx 1.0\\)\n\n\ntest_R2\nR² on held-out test frames (rollout prediction)\n&gt; 0.9\n\n\ntest_pearson\nPearson correlation on test frames\n&gt; 0.95\n\n\nfinal_loss\nFinal prediction loss (MSE on \\(dc/dt\\))\nLower is better\n\n\nalpha\nMLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\\). Ideal value is 1.0 — indicates MLP\\(_{\\text{sub}}\\) has learned the correct scale. Deviations from 1 indicate residual scale ambiguity that the scalar correction must compensate for.\n\\(\\approx 1.0\\)\n\n\n\n\nDiagnostic Interpretation\nThe relationship between rate_constants_R2 and test_pearson diagnoses whether the model found the true rate constants or a degenerate solution:\n\n\n\n\n\n\n\n\ntest_pearson\n\\(R^2\\)\nDiagnosis\n\n\n\n\n&gt; 0.95\n&gt; 0.9\nHealthy — good dynamics from correct \\(k\\)\n\n\n&gt; 0.95\n0.3–0.9\nDegenerate — good dynamics from wrong \\(k\\) (MLPs compensate)\n\n\n&gt; 0.95\n&lt; 0.3\nSeverely degenerate — MLPs absorb all dynamics\n\n\n&lt; 0.5\n&gt; 0.9\nGood \\(k\\), poor rollout — rate constants correct but MLP errors compound during integration\n\n\n&lt; 0.5\n&lt; 0.5\nFailed — both dynamics and \\(k\\) poor\n\n\n\nThe current oscillatory regime experiments show the good \\(k\\), poor rollout pattern: \\(R^2 \\approx 0.93\\) but Pearson \\(\\approx 0.05\\). The MLP\\(_{\\text{sub}}\\) scale compression (\\(\\alpha \\approx 0.4\\)) and MLP\\(_{\\text{node}}\\) failure (flat at zero) produce errors that accumulate during multi-step rollout, even though the rate constants themselves are well-recovered."
  },
  {
    "objectID": "gnn-llm-memory.html#ucb-tree-search",
    "href": "gnn-llm-memory.html#ucb-tree-search",
    "title": "GNN-LLM-Memory",
    "section": "UCB Tree Search",
    "text": "UCB Tree Search\nThe LLM selects parent configurations to mutate using an Upper Confidence Bound (UCB) strategy that balances exploitation of high-performing branches with exploration of under-visited regions:\n\\[\n\\text{UCB}(i) = \\bar{X}_i + c \\cdot \\sqrt{\\frac{\\ln N}{n_i}}\n\\]\nwhere \\(\\bar{X}_i\\) is the mean reward of node \\(i\\), \\(N\\) is the total number of visits, \\(n_i\\) is the number of visits to node \\(i\\), and \\(c\\) is the exploration constant.\n4 parallel slots run per batch with diversified roles:\n\n\n\n\n\n\n\n\nSlot\nRole\nDescription\n\n\n\n\n0\nexploit\nHighest UCB node, conservative mutation\n\n\n1\nexploit\n2nd highest UCB, or same parent different param\n\n\n2\nexplore\nUnder-visited node, or new parameter dimension\n\n\n3\nprinciple-test\nTest or challenge one established principle from memory"
  },
  {
    "objectID": "application.html",
    "href": "application.html",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#motivation",
    "href": "application.html#motivation",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "href": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Chronic Jugular Microdialysis in Freely Moving Mice",
    "text": "Chronic Jugular Microdialysis in Freely Moving Mice\nNardin et al. developed a workflow combining chronic jugular microdialysis with chemical isotope labeling LC-MS to continuously measure bloodborne metabolites in freely moving mice:\n\n\n\nProperty\nValue\n\n\n\n\nCompounds measured\n~123 high-quality amines and small peptides\n\n\nSampling cadence\n7.5 minutes\n\n\nRecording duration\n~8 hours per session\n\n\nTime frames\n~64 per session\n\n\nAnimals\n3 mice, implants patent for &gt;7 days\n\n\n\nThe data captures real metabolic dynamics: purine turnover correlating with movement, delayed histamine/5-HIAA changes, coordinated amino-acid dynamics, and state-dependent metabolic shifts between locomotion and rest.\n\nLow-Rank Physiological State Space\nA key finding: 10 principal components explain 73% of the variance across all measured compounds. The first component alone (rPC1) captures 35.5% and is strongly correlated with locomotion (Spearman \\(r = -0.59\\), peaking at 7.5 min lag).\nThis low-rank structure is consistent with what we observe in our simulations:\n\n\n\n\n\n\n\n\n\nNardin et al.\nMetabolismGraph simulations\n\n\n\n\nn_metabolites\n~123\n100\n\n\nRank at 70% variance\n~10\n~5–15 (estimated)\n\n\nRank at 99% variance\n—\n24–50\n\n\nDominant mode\nLocomotion-driven amino acid metabolism\nAutocatalytic cycle oscillations\n\n\n\nThe paper concludes that “endocrine and metabolic control may operate over a compact set of latent variables” — the same low-rank assumption that underlies our activity rank analysis via SVD."
  },
  {
    "objectID": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "href": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Applying MetabolismGraph to Microdialysis Data",
    "text": "Applying MetabolismGraph to Microdialysis Data\n\nWhat changes\nMoving from synthetic to real data introduces several differences:\n1. The stoichiometric matrix S is partially known\nIn simulation, \\(\\mathbf{S}\\) is either generated (ground truth) or learned from data. For real metabolomics, the reaction network connecting measured compounds can be partially reconstructed from biochemical databases (KEGG, Recon3D, HMDB). However:\n\nNot all reactions are known\nNot all participants in a reaction are measured (the 123 compounds are a subset of the full metabolome)\nSome measured compounds participate in multiple pathways\n\nThis suggests an intermediate mode between “S given” and “S learning”: initialize S from database knowledge, then refine with data.\n2. Observation is incomplete\nThe microdialysis probe measures only the free (unbound) fraction of molecules below the 6 kDa molecular weight cutoff. Many metabolites, enzymes, and signaling molecules are invisible. The GNN must learn dynamics from a partial observation of the full system — analogous to learning neural dynamics from a subset of recorded neurons.\n3. External drivers exist\nThe synthetic model is autonomous: \\(dc/dt\\) depends only on current concentrations and the network. Real metabolic dynamics are driven by external inputs — feeding, locomotion, circadian rhythms, stress — that are not part of the reaction network. The paper shows locomotion is a dominant driver (rPC1).\nThis maps to extensions of the current model:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot v_j}_{\\text{reactions}} + \\underbrace{f_{\\text{ext}}(x_{\\text{locomotion}}, t)}_{\\text{external drive}}\n\\]\nThe circadian_amplitude and circadian_period parameters already in the simulation config are a first step toward modeling time-varying external inputs.\n4. Kinetics may not be mass-action\nEnzyme-mediated reactions follow Michaelis-Menten or Hill kinetics rather than pure mass-action. The flexible MLP\\(_{\\text{sub}}\\) architecture is well-suited here — it can discover the true functional form without assuming \\(c^{|S|}\\) a priori. The additive aggregation mode may be more appropriate for enzyme-mediated reactions where substrates contribute more independently.\n\n\nWhat stays the same\nThe core framework transfers directly:\n\nBipartite graph structure: metabolites connected to reactions through stoichiometric edges\nGNN message passing: substrate concentrations aggregated per reaction, then distributed back to metabolites\nFunction discovery: MLP\\(_{\\text{sub}}\\) learns the concentration-to-rate mapping, MLP\\(_{\\text{node}}\\) learns homeostatic regulation\nRate constant recovery: per-reaction \\(k_j\\) learned in log-space\nIdentifiability challenges: scale ambiguity, function compensation, and homeostasis absorption are equally present (and harder to diagnose without ground truth)\n\n\n\nProposed workflow\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    A[Microdialysis time series&lt;br/&gt;123 compounds × 64 frames] --&gt; B[Construct bipartite graph&lt;br/&gt;from KEGG/Recon3D]\n    B --&gt; C[Initialize S from&lt;br/&gt;database stoichiometry]\n    C --&gt; D[Train GNN&lt;br/&gt;predict dc/dt]\n    D --&gt; E{Evaluate}\n    E --&gt;|test R² &gt; 0.9| F[Extract learned parameters]\n    E --&gt;|test R² &lt; 0.9| G[Refine: unfreeze S,&lt;br/&gt;add external inputs]\n    G --&gt; D\n    F --&gt; H[Rate constants k_j]\n    F --&gt; I[Learned kinetics&lt;br/&gt;MLP_sub shape]\n    F --&gt; J[Homeostatic regulation&lt;br/&gt;MLP_node per metabolite type]\n\n\n\n\n\n\nStep 1. Build the metabolite-reaction bipartite graph from KEGG pathway maps for the 123 identified compounds. Estimate the number of reactions linking measured metabolites.\nStep 2. Initialize \\(\\mathbf{S}\\) from known stoichiometry (S given mode). Train the GNN to predict \\(dc/dt\\) from concentration snapshots, learning \\(k_j\\), MLP\\(_{\\text{sub}}\\), and MLP\\(_{\\text{node}}\\).\nStep 3. Evaluate prediction quality on held-out time frames. If prediction R\\(^2\\) is high but the learned MLP\\(_{\\text{sub}}\\) does not match expected kinetics (mass-action or Michaelis-Menten), investigate degeneracy.\nStep 4. Optionally unfreeze \\(\\mathbf{S}\\) to discover missing reactions or correct database errors (S learning mode). Compare learned stoichiometry against biochemical databases.\nStep 5. Incorporate locomotion and other behavioral covariates as external inputs to account for the dominant non-metabolic drivers of concentration change."
  },
  {
    "objectID": "application.html#challenges-and-open-questions",
    "href": "application.html#challenges-and-open-questions",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Challenges and Open Questions",
    "text": "Challenges and Open Questions\nTemporal resolution. The 7.5-minute sampling cadence gives ~64 frames per session. Our simulations use 2880 frames. With fewer time points, the GNN has less data to disentangle rate constants from MLP functions, exacerbating identifiability issues. Multi-session data across the 7+ days of implant patency could help.\nMissing metabolites. The 123 measured compounds are a fraction of the full metabolome. Reactions involving unmeasured substrates will appear as unexplained variance. The MLP\\(_{\\text{node}}\\) may absorb some of this as apparent “homeostasis.”\nNon-stationarity. Real metabolic dynamics are non-stationary — circadian rhythms, feeding cycles, and adaptation shift the baseline over hours. The current model assumes fixed \\(c^{\\text{baseline}}\\) and \\(\\lambda\\). Time-varying homeostatic parameters may be needed.\nValidation without ground truth. In simulation, we validate against known \\(k_j\\) values (rate constants R\\(^2\\)). With real data, validation must rely on: (1) held-out prediction accuracy, (2) consistency with known biochemistry, (3) perturbation experiments (the paper notes microdialysis can also deliver molecules), and (4) cross-animal reproducibility."
  },
  {
    "objectID": "application.html#references",
    "href": "application.html#references",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "References",
    "text": "References\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\nSee the current results for rate constant recovery on an oscillatory regime with 100 metabolites and 256 reactions.\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\nSee the current results for rate constant recovery on an oscillatory regime with 100 metabolites and 256 reactions.\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#the-full-model",
    "href": "index.html#the-full-model",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Full Model",
    "text": "The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate \\(v_j\\) depends on aggregation type:\n\n\n\n\n\n\n\nAggregation\nRate \\(v_j\\)\n\n\n\n\nAdditive\n\\(v_j = k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\nMultiplicative\n\\(v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\n\nSee Model for detailed equations, diagrams, and model configurations."
  },
  {
    "objectID": "index.html#the-inverse-problem",
    "href": "index.html#the-inverse-problem",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe forward model describes how concentrations evolve given all parameters. In practice, the parameters themselves are unknown. The inverse problem is to recover them from observed dynamics.\nGiven:\n\nConcentration trajectories \\(\\{c_i(t)\\}_{i=1}^{n}\\) measured over time\nStoichiometric matrix \\(\\mathbf{S}\\) (known from biochemistry)\n\nTo learn:\n\nSubstrate function \\(\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\) — discovers the mass-action power law \\(c_k^{|S_{kj}|}\\)\nHomeostasis function \\(\\text{MLP}_{\\text{node}}(c_i)\\) — discovers per-metabolite regulation \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\nRate constants \\(k_j\\) — per-reaction speed scalars\n\nThis is challenging because the system is high-dimensional (\\(n\\) metabolites, \\(m\\) reactions), the mapping from parameters to dynamics is nonlinear, and multiple parameter combinations can produce similar trajectories (identifiability). Classical optimization approaches struggle with this combinatorial landscape.\nWe address this by casting the inverse problem as a Graph Neural Network learning task. The metabolic network is naturally a bipartite graph (metabolites \\(\\leftrightarrow\\) reactions), and we replace the unknown functions with learnable MLPs that operate on this graph structure. The GNN is trained end-to-end by minimizing the prediction error on \\(dc/dt\\), recovering the rate constants and homeostatic functions simultaneously. An LLM-driven closed-loop exploration engine systematically searches the hyperparameter space — see GNN-LLM-Memory for the training scheme, regularization terms, and exploration loop.\n\nGNN Parameterization\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\prod_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nwhere:\n\n\\(a_i \\in \\mathbb{R}^d\\) is a learnable embedding for metabolite \\(i\\)\n\\(k_j\\) are learnable rate constants\n\\(\\prod\\) denotes multiplicative aggregation (mass-action kinetics)\n\n\n\nLearnable Parameters\n\n\n\n\n\n\n\n\nParameter\nType\nPurpose\n\n\n\n\n\\(a_i\\)\nEmbedding vectors\nPer-metabolite identity\n\n\n\\(k_j\\)\nScalars\nPer-reaction rate constants\n\n\n\\(\\text{MLP}_{\\text{node}}\\)\nNeural network\nLearns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\n\n\n\\(\\text{MLP}_{\\text{sub}}\\)\nNeural network\nLearns \\(c_k^{|S_{kj}|}\\)"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "Citation",
    "text": "Citation\nIf you use MetabolismGraph in your research, please cite:\n@software{metabolismgraph2025,\n  author = {Allier, Cédric},\n  title = {MetabolismGraph: Learning Metabolism Dynamics with GNNs},\n  year = {2026},\n  url = {https://github.com/allierc/MetabolismGraph}\n}"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This page was generated by Claude and may contain inaccuracies in author lists, publication details, or descriptions. Please verify citations before use."
  },
  {
    "objectID": "references.html#context",
    "href": "references.html#context",
    "title": "References",
    "section": "Context",
    "text": "Context\nMetabolismGraph sits at the intersection of several active research areas: graph neural networks for physical and biological systems, inverse problems in systems biology, neural differential equations, and LLM-driven scientific exploration. The framework uses message-passing GNNs on bipartite metabolite-reaction graphs to solve the inverse problem of recovering kinetic parameters from concentration dynamics, with an LLM-based closed-loop exploration engine for hyperparameter optimization.\nBelow we collect key references organized by topic. Where available, we provide arXiv or DOI links."
  },
  {
    "objectID": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "href": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "title": "References",
    "section": "Graph Neural Networks for Physical and Biological Systems",
    "text": "Graph Neural Networks for Physical and Biological Systems\n\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., & Battaglia, P. W. (2020). Learning to simulate complex physics with graph networks. ICML 2020. arXiv:2002.09405 — Graph network-based simulators (GNS) for learning physical dynamics from particle-based representations. Demonstrates that GNNs can learn accurate forward simulators for complex physical systems.\nKipf, T., Fetaya, E., Wang, K.-C., Welling, M., & Zemel, R. (2018). Neural relational inference for interacting systems. ICML 2018. arXiv:1802.04687 — Learns interaction graphs from observed trajectories using variational autoencoders on graph structures. Closely related to our inverse-problem setting where the goal is to recover network structure from dynamics.\nCranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., & Ho, S. (2020). Discovering symbolic models from deep learning with inductive biases. NeurIPS 2020. arXiv:2006.11287 — Combines GNNs with symbolic regression to extract interpretable physical laws from learned representations. Relevant to our approach of recovering interpretable kinetic parameters from learned MLP functions."
  },
  {
    "objectID": "references.html#neural-differential-equations-and-scientific-ml",
    "href": "references.html#neural-differential-equations-and-scientific-ml",
    "title": "References",
    "section": "Neural Differential Equations and Scientific ML",
    "text": "Neural Differential Equations and Scientific ML\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural ordinary differential equations. NeurIPS 2018. arXiv:1806.07366 — Foundational work on continuous-depth neural networks parameterized as ODEs, enabling gradient-based learning of dynamical systems.\nRackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R., Skinner, D., Ramadhan, A., & Edelman, A. (2020). Universal differential equations for scientific machine learning. arXiv:2001.04385 — Framework combining differential equations with neural networks for scientific modeling. The UDE approach of embedding learnable components within known differential equation structure is conceptually similar to how MetabolismGraph embeds learnable MLPs within the known stoichiometric framework."
  },
  {
    "objectID": "references.html#symbolic-regression-for-network-dynamics",
    "href": "references.html#symbolic-regression-for-network-dynamics",
    "title": "References",
    "section": "Symbolic Regression for Network Dynamics",
    "text": "Symbolic Regression for Network Dynamics\n\nYu, Z., Ding, J., & Li, Y. (2025). Discovering network dynamics with neural symbolic regression. Nature Computational Science. DOI:10.1038/s43588-025-00893-8 — ND2: neural symbolic regression that discovers governing equations of network dynamics directly from data. Applied to gene regulatory networks, the method corrects the classical Hill equation model by replacing per-neighbor nonlinear terms with a logistic function applied to the aggregate neighbor sum (see comparison below).\n\n\nComparison with MetabolismGraph\nYu et al.’s corrected gene regulation model (their Eq. 2) and MetabolismGraph share the same general ODE structure — self-dynamics plus interaction dynamics — but differ in how neighbor contributions are aggregated:\nYu et al. — Gene regulation (ND2 corrected):\n\\[\\frac{dx_i}{dt} = \\underbrace{s_i - \\gamma_i x_i}_{\\text{self-dynamics}} + \\underbrace{\\beta \\, \\tilde{S}\\!\\left(\\sum_j A_{ij}\\, x_j\\right)}_{\\text{interaction}}\\]\nwhere \\(\\tilde{S}(x) = (1 + e^{-x})^{-1}\\) is the logistic function. The nonlinearity acts on the sum of weighted neighbor states — no per-edge rate constants, no multiplicative aggregation.\nMetabolismGraph — Metabolic kinetics:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i(c_i - c_i^{\\text{baseline}})}_{\\text{self-dynamics}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}}_{\\text{interaction}}\\]\nKey differences:\n\n\n\n\n\n\n\n\n\nYu et al. (gene regulation)\nMetabolismGraph (metabolism)\n\n\n\n\nGraph\nMonopartite (gene → gene)\nBipartite (metabolite ↔︎ reaction)\n\n\nAggregation\n\\(\\tilde{S}(\\sum_j A_{ij} x_j)\\) — logistic of sum\n\\(\\sum_j S_{ij} k_j \\prod_k c_k^{s_{kj}}\\) — sum of products\n\n\nRate constants\nSingle global \\(\\beta\\)\nPer-reaction \\(k_j\\) (256 parameters)\n\n\nNonlinearity\nBounded logistic \\(\\tilde{S} \\in [0,1]\\)\nUnbounded power law \\(c^s\\)\n\n\nHigher-order\nImplicit: \\(\\partial \\dot{x}_i / \\partial x_j\\) depends on all neighbors via \\(\\tilde{S}\\)\nExplicit: mass-action products couple substrates within each reaction\n\n\n\nThe gene regulation model has no per-reaction aggregation step — it sums all neighbor states into a single scalar, then applies a saturating nonlinearity. MetabolismGraph instead computes a separate rate for each reaction (multiplicative aggregation of substrate concentrations), then sums the stoichiometric contributions. This reflects a fundamental difference between gene regulation (bounded transcriptional response) and metabolism (unbounded mass-action kinetics)."
  },
  {
    "objectID": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "href": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "title": "References",
    "section": "In Vivo Metabolomics and Physiological State Spaces",
    "text": "In Vivo Metabolomics and Physiological State Spaces\n\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974 — Chronic jugular microdialysis paired with LC-MS measures ~123 bloodborne compounds at 7.5-min cadence in freely moving mice. PCA reveals a low-rank physiological manifold: 10 components explain 73% of variance, with rPC1 aligned to locomotion. Provides the real-world metabolomic time series that MetabolismGraph’s inverse problem framework is designed to analyze. See Application for how the GNN framework maps to this data."
  },
  {
    "objectID": "references.html#llm-driven-scientific-discovery",
    "href": "references.html#llm-driven-scientific-discovery",
    "title": "References",
    "section": "LLM-Driven Scientific Discovery",
    "text": "LLM-Driven Scientific Discovery\n\nRomera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468-475. DOI:10.1038/s41586-023-06924-6 — FunSearch: uses LLMs to discover new mathematical constructions through evolutionary program search. Pioneering demonstration that LLMs can make genuine scientific contributions when embedded in a search loop.\nNovikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration. Google DeepMind. — Extends the LLM-driven exploration paradigm to broader scientific and algorithmic discovery tasks. The closed-loop LLM exploration engine in MetabolismGraph draws inspiration from this line of work.\nLu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024). The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv:2408.06292 — End-to-end autonomous research agent that generates hypotheses, runs experiments, and writes papers."
  }
]