[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the oscillatory regime (activity rank \\(\\sim50\\)): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j \\in [10^{-2}, 10^{-1}]\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n8 iterations completed (Block 1). The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). 100% autocatalytic 3-cycles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Iteration 4 (best, \\(R^2 = 0.934\\), slope \\(= 0.964\\)): 206 of 256 reactions recovered correctly (black dots on diagonal). 50 outlier reactions (red) collapse to \\(\\log k \\approx -3\\) to \\(-10\\). The scalar correction \\(\\log_{10} \\alpha = -0.33\\) shifts the uncorrected cloud (gray) onto the diagonal.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline, \\(R^2 = 0.910\\), slope \\(= 0.989\\)): Starting configuration from previous exploration. Already strong, but with 59 outliers (23%). The baseline uses lr_sub=0.0005 — increasing it to 0.001 in iteration 4 improved both \\(R^2\\) and outlier count.\n\n\n\n\n\n\nThe key mutation from iteration 1 to iteration 4 was lr_sub: 0.0005 → 0.001. Faster MLP\\(_{\\text{sub}}\\) learning improved the substrate function fit, which in turn improved rate constant recovery (\\(R^2\\): 0.910 → 0.934, outliers: 59 → 50).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Iteration 4 (best): Left — MLP\\(_{\\text{sub}}\\) learns \\(c^1\\) well (solid blue near dashed GT) but underestimates \\(c^2\\) (solid orange below dashed GT). Scale factor \\(\\alpha = 0.42\\) at \\(|s|=1\\) (should be 1.0). Right — MLP\\(_{\\text{node}}\\) (solid) stays flat at zero despite GT (dashed) showing linear homeostatic functions.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Iteration 1 (baseline): Same qualitative pattern — MLP\\(_{\\text{sub}}\\) learns the shape but with compressed scale (\\(\\alpha = 0.39\\)), MLP\\(_{\\text{node}}\\) inactive. The MLP\\(_{\\text{node}}\\) failure persists across all 8 iterations regardless of hyperparameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Kinograph (iteration 4): Ground truth (top left) vs GNN rollout (bottom left). Despite \\(R^2 = 0.934\\) on rate constants, the dynamics prediction remains poor (Pearson = 0.054). The GNN produces saturated flat bands instead of oscillations. The residual heatmap (top right) and scatter plot (bottom right) confirm the mismatch. This is the degeneracy gap: good \\(k\\) recovery but poor dynamics, suggesting that MLP\\(_{\\text{sub}}\\) and MLP\\(_{\\text{node}}\\) errors compound during rollout integration.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: UCB tree after 8 iterations. Node 4 (\\(R^2 = 0.934\\), lr_sub: 0.0005 → 0.001) is the best-performing node and parent of 4 child mutations. All nodes are green (\\(R^2 \\geq 0.9\\)) except Node 7 (red, \\(R^2 = 0.04\\)) where coeff_MLP_sub_norm=5.0 caused catastrophic failure.\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 9))\nr2 = [0.910, 0.903, 0.928, 0.934, 0.926, 0.916, 0.035, 0.907]\noutliers = [59, 47, 53, 50, 46, 46, 246, 51]\n\nlabels = [\n    'baseline',\n    'lr_k=0.01',\n    'lr_node\\n=0.0005',\n    'lr_sub\\n=0.001\\n(BEST)',\n    '+sub_norm\\n=1.0',\n    '+k_floor\\n=1.0',\n    '+sub_norm\\n=5.0\\n(FAIL)',\n    'node_L1\\n=0.1',\n]\n\ncolors = []\nfor r in r2:\n    if r &gt;= 0.9:\n        colors.append('#2ecc71')\n    elif r &gt;= 0.5:\n        colors.append('#f39c12')\n    else:\n        colors.append('#e74c3c')\n\nfig, ax1 = plt.subplots(figsize=(12, 5))\n\nbars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)\nax1.axhline(y=0.934, color='#2ecc71', linestyle='--', alpha=0.4, label='Best R² = 0.934')\nax1.axvspan(0.5, 4.5, alpha=0.04, color='blue', label='Batch 1 (initial sweep)')\nax1.axvspan(4.5, 8.5, alpha=0.04, color='orange', label='Batch 2 (regularization)')\n\nfor i, (it, v, lab) in enumerate(zip(iters, r2, labels)):\n    ax1.text(it, max(v + 0.02, 0.06), f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n    ax1.text(it, -0.08, lab, ha='center', va='top', fontsize=7)\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('rate_constants R²')\nax1.set_ylim(-0.18, 1.05)\nax1.set_xticks(iters)\n\n# outlier count on secondary axis\nax2 = ax1.twinx()\nax2.plot(iters, outliers, 'o', color='#e67e22', markersize=5, alpha=0.7, label='outliers')\nax2.set_ylabel('outlier count', color='#e67e22')\nax2.tick_params(axis='y', labelcolor='#e67e22')\nax2.set_ylim(0, 270)\n\n# combined legend\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=8)\n\nax1.set_title('UCB Exploration: Rate Constant Recovery (Block 1, 8 iterations)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Rate constants R² and outlier count across 8 iterations. Left axis: R² (bars). Right axis: outlier count (orange line). Iteration 7 (coeff_MLP_sub_norm=5.0) is the only failure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Iteration 7 (\\(R^2 = 0.035\\), 246 outliers): coeff_MLP_sub_norm=5.0 destabilized training. Nearly all 256 reactions became outliers. Interestingly, \\(\\alpha = 0.80\\) moved closer to the target 1.0 — but at the cost of destroying rate constant recovery entirely.\n\n\n\n\n\n\n\n\nTable 1: MLP\\(_{\\text{sub}}\\) normalization penalty. The penalty pushes \\(\\alpha\\) toward 1.0 but is harmful above 1.0.\n\n\n\n\n\ncoeff_MLP_sub_norm\n\\(R^2\\)\noutliers\n\\(\\alpha\\)\n\n\n\n\n0.0\n0.934\n50\n0.47\n\n\n1.0\n0.926\n46\n0.43\n\n\n5.0\n0.035\n246\n0.80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncreasing the MLP\\(_{\\text{sub}}\\) learning rate from 0.0005 to 0.001 produced the best result (\\(R^2 = 0.934\\), slope \\(= 0.964\\)). Faster MLP\\(_{\\text{sub}}\\) learning helps it better approximate \\(c^s\\), which reduces the compensation burden on \\(k\\).\n\n\n\nAcross all 8 iterations, MLP\\(_{\\text{node}}\\) stays flat at zero (Figure 5, Figure 6, right panels). This persists regardless of:\n\nLower lr_node (0.0005): no effect\nHigher lr_k (0.01): no effect\nReduced L1 penalty (coeff_MLP_node_L1 = 0.1): no effect\n\nThe homeostatic terms (\\(\\lambda \\sim 0.001\\)–\\(0.002\\)) are $\\(100\\)$ smaller than the reaction rate terms (\\(k \\sim 0.01\\)–\\(0.1\\)). The gradient signal flowing to MLP\\(_{\\text{node}}\\) may be insufficient to escape the zero initialization.\n\n\n\nAll tested regularization terms worsened \\(R^2\\) relative to the unregularized parent (iter 4):\n\n\n\nRegularization\n\\(R^2\\)\n\\(\\Delta R^2\\)\n\n\n\n\nNone (iter 4)\n0.934\n—\n\n\ncoeff_MLP_sub_norm=1.0\n0.926\n–0.008\n\n\ncoeff_k_floor=1.0\n0.916\n–0.018\n\n\ncoeff_MLP_node_L1=0.1\n0.907\n–0.027\n\n\ncoeff_MLP_sub_norm=5.0\n0.035\n–0.899\n\n\n\nThe current best strategy is no extra regularization — only the default coeff_MLP_sub_diff=5 and coeff_MLP_node_L1=1.0.\n\n\n\nApproximately 50 of 256 reactions (20%) consistently learn incorrect \\(\\log_{10}(k)\\) values, collapsing to \\(-3\\) to \\(-10\\) (far from the true range \\([-2, -1]\\)). These outliers are stable across configurations and may correspond to reactions whose substrate concentrations provide insufficient gradient signal.\n\n\n\nMLP\\(_{\\text{sub}}\\) learns \\(\\alpha \\cdot c^s\\) with \\(\\alpha \\approx 0.4\\) instead of \\(c^s\\) (\\(\\alpha = 1.0\\)). The post-hoc scalar correction compensates for this (gray → black dots in Figure 3), but the \\(|s|=2\\) curve remains underestimated. Stronger normalization penalties destabilize training (Table 1).\n\n\n\n\nThe LLM’s persistent memory has accumulated these validated principles after 8 iterations:\n\nlr_sub=0.001 helps (confidence: medium) — higher lr_sub improves \\(R^2\\) and slope\ncoeff_MLP_sub_norm &gt; 1.0 is harmful (confidence: high) — catastrophic at 5.0, marginal at 1.0\nMLP\\(_{\\text{node}}\\) does not learn from reduced L1 alone (confidence: medium) — gradient signal issue\ncoeff_k_floor doesn’t reduce outliers (confidence: medium) — same count, worse \\(R^2\\)\n\n\n\n\nThe exploration continues into Block 2 with planned strategies:\n\n\n\n\n\n\n\n\nSlot\nStrategy\nMutation\n\n\n\n\n0\nexploit\nlr_sub → 0.002 (push further)\n\n\n1\nexploit\nlr_node → 0.002 (wake up MLP\\(_{\\text{node}}\\))\n\n\n2\nexplore\nrecurrent_training=true, time_step=4\n\n\n3\nboundary-probe\ndata_augmentation_loop → 2000 (longer training)\n\n\n\n\n\n\n\nMLP\\(_{\\text{node}}\\) activation: Can lr_node = 0.005–0.01 finally activate homeostasis learning?\nOutlier structure: What structural property do the ~50 outlier reactions share? Are they in specific network motifs?\nRecurrent training: Can multi-step rollout break the degeneracy between correct and incorrect \\(k\\) values?\nMLP architecture: Would smaller networks (hidden_dim=32, n_layers=2) act as implicit regularization?"
  },
  {
    "objectID": "results.html#rate-constant-recovery-oscillatory-regime-rank-50",
    "href": "results.html#rate-constant-recovery-oscillatory-regime-rank-50",
    "title": "Results",
    "section": "",
    "text": "The LLM-driven exploration engine is running on the oscillatory regime (activity rank \\(\\sim50\\)): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix \\(\\mathbf{S}\\) frozen from ground truth. The goal is to recover the 256 rate constants \\(k_j \\in [10^{-2}, 10^{-1}]\\) by optimizing training hyperparameters through UCB tree search with 4 parallel slots.\n8 iterations completed (Block 1). The exploration is ongoing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Stoichiometric matrix \\(\\mathbf{S}\\) (100 metabolites \\(\\times\\) 256 reactions). Red = products (+1), blue = substrates (–1). 100% autocatalytic 3-cycles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Iteration 4 (best, \\(R^2 = 0.934\\), slope \\(= 0.964\\)): 206 of 256 reactions recovered correctly (black dots on diagonal). 50 outlier reactions (red) collapse to \\(\\log k \\approx -3\\) to \\(-10\\). The scalar correction \\(\\log_{10} \\alpha = -0.33\\) shifts the uncorrected cloud (gray) onto the diagonal.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Iteration 1 (baseline, \\(R^2 = 0.910\\), slope \\(= 0.989\\)): Starting configuration from previous exploration. Already strong, but with 59 outliers (23%). The baseline uses lr_sub=0.0005 — increasing it to 0.001 in iteration 4 improved both \\(R^2\\) and outlier count.\n\n\n\n\n\n\nThe key mutation from iteration 1 to iteration 4 was lr_sub: 0.0005 → 0.001. Faster MLP\\(_{\\text{sub}}\\) learning improved the substrate function fit, which in turn improved rate constant recovery (\\(R^2\\): 0.910 → 0.934, outliers: 59 → 50).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Iteration 4 (best): Left — MLP\\(_{\\text{sub}}\\) learns \\(c^1\\) well (solid blue near dashed GT) but underestimates \\(c^2\\) (solid orange below dashed GT). Scale factor \\(\\alpha = 0.42\\) at \\(|s|=1\\) (should be 1.0). Right — MLP\\(_{\\text{node}}\\) (solid) stays flat at zero despite GT (dashed) showing linear homeostatic functions.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Iteration 1 (baseline): Same qualitative pattern — MLP\\(_{\\text{sub}}\\) learns the shape but with compressed scale (\\(\\alpha = 0.39\\)), MLP\\(_{\\text{node}}\\) inactive. The MLP\\(_{\\text{node}}\\) failure persists across all 8 iterations regardless of hyperparameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Kinograph (iteration 4): Ground truth (top left) vs GNN rollout (bottom left). Despite \\(R^2 = 0.934\\) on rate constants, the dynamics prediction remains poor (Pearson = 0.054). The GNN produces saturated flat bands instead of oscillations. The residual heatmap (top right) and scatter plot (bottom right) confirm the mismatch. This is the degeneracy gap: good \\(k\\) recovery but poor dynamics, suggesting that MLP\\(_{\\text{sub}}\\) and MLP\\(_{\\text{node}}\\) errors compound during rollout integration.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: UCB tree after 8 iterations. Node 4 (\\(R^2 = 0.934\\), lr_sub: 0.0005 → 0.001) is the best-performing node and parent of 4 child mutations. All nodes are green (\\(R^2 \\geq 0.9\\)) except Node 7 (red, \\(R^2 = 0.04\\)) where coeff_MLP_sub_norm=5.0 caused catastrophic failure.\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\niters = list(range(1, 9))\nr2 = [0.910, 0.903, 0.928, 0.934, 0.926, 0.916, 0.035, 0.907]\noutliers = [59, 47, 53, 50, 46, 46, 246, 51]\n\nlabels = [\n    'baseline',\n    'lr_k=0.01',\n    'lr_node\\n=0.0005',\n    'lr_sub\\n=0.001\\n(BEST)',\n    '+sub_norm\\n=1.0',\n    '+k_floor\\n=1.0',\n    '+sub_norm\\n=5.0\\n(FAIL)',\n    'node_L1\\n=0.1',\n]\n\ncolors = []\nfor r in r2:\n    if r &gt;= 0.9:\n        colors.append('#2ecc71')\n    elif r &gt;= 0.5:\n        colors.append('#f39c12')\n    else:\n        colors.append('#e74c3c')\n\nfig, ax1 = plt.subplots(figsize=(12, 5))\n\nbars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)\nax1.axhline(y=0.934, color='#2ecc71', linestyle='--', alpha=0.4, label='Best R² = 0.934')\nax1.axvspan(0.5, 4.5, alpha=0.04, color='blue', label='Batch 1 (initial sweep)')\nax1.axvspan(4.5, 8.5, alpha=0.04, color='orange', label='Batch 2 (regularization)')\n\nfor i, (it, v, lab) in enumerate(zip(iters, r2, labels)):\n    ax1.text(it, max(v + 0.02, 0.06), f'{v:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n    ax1.text(it, -0.08, lab, ha='center', va='top', fontsize=7)\n\nax1.set_xlabel('Iteration')\nax1.set_ylabel('rate_constants R²')\nax1.set_ylim(-0.18, 1.05)\nax1.set_xticks(iters)\n\n# outlier count on secondary axis\nax2 = ax1.twinx()\nax2.plot(iters, outliers, 'o', color='#e67e22', markersize=5, alpha=0.7, label='outliers')\nax2.set_ylabel('outlier count', color='#e67e22')\nax2.tick_params(axis='y', labelcolor='#e67e22')\nax2.set_ylim(0, 270)\n\n# combined legend\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=8)\n\nax1.set_title('UCB Exploration: Rate Constant Recovery (Block 1, 8 iterations)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Rate constants R² and outlier count across 8 iterations. Left axis: R² (bars). Right axis: outlier count (orange line). Iteration 7 (coeff_MLP_sub_norm=5.0) is the only failure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Iteration 7 (\\(R^2 = 0.035\\), 246 outliers): coeff_MLP_sub_norm=5.0 destabilized training. Nearly all 256 reactions became outliers. Interestingly, \\(\\alpha = 0.80\\) moved closer to the target 1.0 — but at the cost of destroying rate constant recovery entirely.\n\n\n\n\n\n\n\n\nTable 1: MLP\\(_{\\text{sub}}\\) normalization penalty. The penalty pushes \\(\\alpha\\) toward 1.0 but is harmful above 1.0.\n\n\n\n\n\ncoeff_MLP_sub_norm\n\\(R^2\\)\noutliers\n\\(\\alpha\\)\n\n\n\n\n0.0\n0.934\n50\n0.47\n\n\n1.0\n0.926\n46\n0.43\n\n\n5.0\n0.035\n246\n0.80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncreasing the MLP\\(_{\\text{sub}}\\) learning rate from 0.0005 to 0.001 produced the best result (\\(R^2 = 0.934\\), slope \\(= 0.964\\)). Faster MLP\\(_{\\text{sub}}\\) learning helps it better approximate \\(c^s\\), which reduces the compensation burden on \\(k\\).\n\n\n\nAcross all 8 iterations, MLP\\(_{\\text{node}}\\) stays flat at zero (Figure 5, Figure 6, right panels). This persists regardless of:\n\nLower lr_node (0.0005): no effect\nHigher lr_k (0.01): no effect\nReduced L1 penalty (coeff_MLP_node_L1 = 0.1): no effect\n\nThe homeostatic terms (\\(\\lambda \\sim 0.001\\)–\\(0.002\\)) are $\\(100\\)$ smaller than the reaction rate terms (\\(k \\sim 0.01\\)–\\(0.1\\)). The gradient signal flowing to MLP\\(_{\\text{node}}\\) may be insufficient to escape the zero initialization.\n\n\n\nAll tested regularization terms worsened \\(R^2\\) relative to the unregularized parent (iter 4):\n\n\n\nRegularization\n\\(R^2\\)\n\\(\\Delta R^2\\)\n\n\n\n\nNone (iter 4)\n0.934\n—\n\n\ncoeff_MLP_sub_norm=1.0\n0.926\n–0.008\n\n\ncoeff_k_floor=1.0\n0.916\n–0.018\n\n\ncoeff_MLP_node_L1=0.1\n0.907\n–0.027\n\n\ncoeff_MLP_sub_norm=5.0\n0.035\n–0.899\n\n\n\nThe current best strategy is no extra regularization — only the default coeff_MLP_sub_diff=5 and coeff_MLP_node_L1=1.0.\n\n\n\nApproximately 50 of 256 reactions (20%) consistently learn incorrect \\(\\log_{10}(k)\\) values, collapsing to \\(-3\\) to \\(-10\\) (far from the true range \\([-2, -1]\\)). These outliers are stable across configurations and may correspond to reactions whose substrate concentrations provide insufficient gradient signal.\n\n\n\nMLP\\(_{\\text{sub}}\\) learns \\(\\alpha \\cdot c^s\\) with \\(\\alpha \\approx 0.4\\) instead of \\(c^s\\) (\\(\\alpha = 1.0\\)). The post-hoc scalar correction compensates for this (gray → black dots in Figure 3), but the \\(|s|=2\\) curve remains underestimated. Stronger normalization penalties destabilize training (Table 1).\n\n\n\n\nThe LLM’s persistent memory has accumulated these validated principles after 8 iterations:\n\nlr_sub=0.001 helps (confidence: medium) — higher lr_sub improves \\(R^2\\) and slope\ncoeff_MLP_sub_norm &gt; 1.0 is harmful (confidence: high) — catastrophic at 5.0, marginal at 1.0\nMLP\\(_{\\text{node}}\\) does not learn from reduced L1 alone (confidence: medium) — gradient signal issue\ncoeff_k_floor doesn’t reduce outliers (confidence: medium) — same count, worse \\(R^2\\)\n\n\n\n\nThe exploration continues into Block 2 with planned strategies:\n\n\n\n\n\n\n\n\nSlot\nStrategy\nMutation\n\n\n\n\n0\nexploit\nlr_sub → 0.002 (push further)\n\n\n1\nexploit\nlr_node → 0.002 (wake up MLP\\(_{\\text{node}}\\))\n\n\n2\nexplore\nrecurrent_training=true, time_step=4\n\n\n3\nboundary-probe\ndata_augmentation_loop → 2000 (longer training)\n\n\n\n\n\n\n\nMLP\\(_{\\text{node}}\\) activation: Can lr_node = 0.005–0.01 finally activate homeostasis learning?\nOutlier structure: What structural property do the ~50 outlier reactions share? Are they in specific network motifs?\nRecurrent training: Can multi-step rollout break the degeneracy between correct and incorrect \\(k\\) values?\nMLP architecture: Would smaller networks (hidden_dim=32, n_layers=2) act as implicit regularization?"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#metabolic-networks-as-bipartite-graphs",
    "href": "model.html#metabolic-networks-as-bipartite-graphs",
    "title": "Model: Metabolic Network Dynamics",
    "section": "",
    "text": "A metabolic network can be represented as a bipartite graph connecting metabolites to reactions:\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        A((Glucose))\n        B((ATP))\n        C((Pyruvate))\n        D((ADP))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n    end\n\n    A --&gt;|\"−1\"| R1\n    B --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+2\"| C\n    R1 --&gt;|\"+1\"| D\n\n    C --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| B\n\n    style A fill:#e1f5fe,stroke:#0277bd\n    style B fill:#e1f5fe,stroke:#0277bd\n    style C fill:#e1f5fe,stroke:#0277bd\n    style D fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nEach edge has a stoichiometric coefficient:\n\nNegative coefficients: substrates (consumed by the reaction)\nPositive coefficients: products (produced by the reaction)"
  },
  {
    "objectID": "model.html#the-stoichiometric-matrix",
    "href": "model.html#the-stoichiometric-matrix",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Stoichiometric Matrix",
    "text": "The Stoichiometric Matrix\nThe stoichiometric matrix \\(\\mathbf{S}\\) is an \\((n_{\\text{metabolites}} \\times n_{\\text{reactions}})\\) matrix where entry \\(S_{ij}\\) indicates how metabolite \\(i\\) participates in reaction \\(j\\):\n\\[\n\\mathbf{S} = \\begin{pmatrix}\n-1 & 0 & \\cdots \\\\\n-1 & +1 & \\cdots \\\\\n+2 & -1 & \\cdots \\\\\n+1 & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n\\]\nProperties of S:\n\nSparse: most entries are zero (each reaction involves only 2-6 metabolites)\nInteger-valued: entries are typically in \\(\\{-2, -1, 0, +1, +2\\}\\)\nMass conservation: column sums should be zero for balanced reactions\n\nExample:\nConsider the diagram above with 2 reactions:\n\\[\n\\begin{aligned}\n\\text{R1}: \\quad & \\text{Glucose} + \\text{ATP} \\longrightarrow 2\\,\\text{Pyruvate} + \\text{ADP} \\\\\n\\text{R2}: \\quad & \\text{Pyruvate} \\longrightarrow \\text{ATP}\n\\end{aligned}\n\\]\nThe corresponding stoichiometric matrix is:\n\\[\n\\begin{array}{c|cc}\n& \\text{R1} & \\text{R2} \\\\\n\\hline\n\\text{Glucose} & -1 & 0 \\\\\n\\text{ATP} & -1 & +1 \\\\\n\\text{Pyruvate} & +2 & -1 \\\\\n\\text{ADP} & +1 & 0\n\\end{array}\n\\]"
  },
  {
    "objectID": "model.html#reaction-kinetics",
    "href": "model.html#reaction-kinetics",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Reaction Kinetics",
    "text": "Reaction Kinetics\n\nMass-Action Law\nThe rate of a chemical reaction is proportional to the product of the concentrations of its substrates, each raised to the power of its stoichiometric coefficient. For a reaction \\(j\\) with substrates \\(k\\):\n\\[v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nThis is the law of mass action (Guldberg & Waage, 1864): a reaction proceeds faster when its substrates are more abundant. The exponent \\(|S_{kj}|\\) reflects how many molecules of substrate \\(k\\) are consumed — a reaction requiring 2 molecules of ATP depends quadratically on ATP concentration (\\(c_{\\text{ATP}}^2\\)), while a reaction consuming 1 molecule depends linearly (\\(c_{\\text{ATP}}^1\\)).\n\n\nSubstrate Aggregation\nEach reaction consumes multiple substrates. The aggregation rule determines how substrate contributions combine to form the reaction rate:\n\n\n\n\n\n\n\n\nAggregation\nFormula\nPhysical meaning\n\n\n\n\nMultiplicative (product)\n\\(v_j = k_j \\cdot \\prod_k c_k^{\\|S_{kj}\\|}\\)\nTrue mass-action: all substrates must be present simultaneously. Rate drops to zero if any substrate is absent.\n\n\nAdditive (sum)\n\\(v_j = k_j \\cdot \\sum_k c_k^{\\|S_{kj}\\|}\\)\nApproximate: substrates contribute independently. Useful when reactions are not strictly mass-action (e.g., enzyme-mediated).\n\n\n\nMultiplicative aggregation is the physically correct form for mass-action kinetics — it encodes the requirement that a reaction can only proceed if all its substrates are available. It also produces richer dynamics: multiplicative coupling between metabolites creates nonlinear feedback loops that can sustain oscillations.\nAdditive aggregation is a simplification where each substrate contributes independently to the rate. It cannot produce sustained oscillations from autocatalytic cycles because it lacks the multiplicative coupling needed for positive feedback."
  },
  {
    "objectID": "model.html#model-evolution",
    "href": "model.html#model-evolution",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Model Evolution",
    "text": "Model Evolution\n\n1. Pure Reaction\nPure reaction dynamics without homeostasis, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nPure reaction dynamics: concentrations evolve under additive mass-action kinetics without homeostasis. Most metabolites equilibrate within ~100 time steps.\n\n\n\n\n2. Homeostasis\nWith homeostatic regulation pulling concentrations toward baseline, using additive aggregation:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}}_{\\text{reactions}}\\]\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n64\n\n\nStoichiometry\nRandom\n\n\nAggregation\nSum (additive)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.01\n\n\n\\(c^{\\text{baseline}}\\)\n5.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-3}, 10^{-1}]\\)\n\n\nInitial concentrations\n[2.5, 7.5]\n\n\nFlux limiting\nEnabled\n\n\n\n\n\n\nHomeostatic regulation: concentrations are pulled toward a baseline (\\(c^{\\text{baseline}} = 5.0\\)) by a linear restoring force. Dynamics converge to steady state.\n\n\n\n\n3. Oscillatory\nAutocatalytic cycles with mass-action kinetics (multiplicative aggregation) for sustained oscillations:\n\\[\\frac{dc_i}{dt} = \\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\]\nWith multiplicative aggregation, autocatalytic cycles create positive feedback: in \\(A + B \\to 2B\\), the rate \\(v = k \\cdot c_A \\cdot c_B\\) increases with both \\(A\\) and \\(B\\), so producing more \\(B\\) accelerates the reaction — a nonlinear feedback loop that sustains oscillations when cycles are closed (\\(A \\to B \\to C \\to A\\)).\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nAggregation\nProduct (multiplicative)\n\n\n\\(\\lambda\\) (homeostatic strength)\n0.0\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.5}, 10^{-1}]\\)\n\n\nInitial concentrations\n[1.0, 9.0]\n\n\nFlux limiting\nDisabled\n\n\n\n\n\n\nOscillatory dynamics (activity rank 20): autocatalytic 3-cycles with multiplicative aggregation produce sustained oscillations. The wide rate constant range \\([10^{-2.5}, 10^{-1}]\\) means ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly."
  },
  {
    "objectID": "model.html#activity-rank",
    "href": "model.html#activity-rank",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Activity Rank",
    "text": "Activity Rank\nTo quantify the complexity of concentration dynamics, we compute the activity rank using singular value decomposition (SVD) of the concentration matrix \\(\\mathbf{C} \\in \\mathbb{R}^{T \\times n}\\) (time frames × metabolites).\nThe rank at 99% variance is the number of singular values needed to capture 99% of the total variance:\n\\[\\text{rank}_{99} = \\min \\left\\{ k : \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.99 \\right\\}\\]\nInterpretation:\n\nLow rank (1-5): concentrations are highly correlated, dynamics are simple (equilibration or uniform decay)\nHigh rank (&gt;20): metabolites evolve independently with rich, complex dynamics\n\n\nIncreasing Activity Rank\nWith the baseline oscillatory config (\\(k_j \\in [10^{-2.5}, 10^{-1}]\\), 256 reactions), the activity rank is 20. Activity rank is controlled by the number of actively contributing reactions per metabolite:\n\n\n\nConfig\nReactions\n\\(k_j\\) range\nActivity Rank\n\n\n\n\nBaseline (rank_20)\n256\n\\([10^{-2.5}, 10^{-1}]\\)\n20\n\n\nNarrow \\(k\\) (rank_50)\n256\n\\([10^{-2.0}, 10^{-1}]\\)\n47\n\n\nMore reactions (rank_70)\n512\n\\([10^{-2.0}, 10^{-1}]\\)\n70\n\n\n\nThe most impactful change is narrowing the rate constant range from \\([10^{-2.5}, 10^{-1}]\\) to \\([10^{-2.0}, 10^{-1}]\\). With the wider range, ~40% of reactions have \\(k &lt; 0.01\\) and contribute negligibly. Removing these inert reactions more than doubles the activity rank (20 → 47). Doubling the number of reactions further increases overlap per metabolite (47 → 70).\n\n\nGNN Training Dataset\nFor GNN training and rate constant recovery, we use the rank_50 config (256 reactions, activity rank 47) — complex enough to test the inverse problem while keeping it tractable:\n\n\n\nParameter\nValue\n\n\n\n\nn_metabolites\n100\n\n\nn_reactions\n256\n\n\nStoichiometry\n100% autocatalytic 3-cycles\n\n\nRate constants \\(k_j\\)\n\\([10^{-2.0}, 10^{-1}]\\)\n\n\nAll other parameters\nSame as oscillatory above\n\n\n\n\n\n\nOscillatory dynamics with narrowed rate constants (activity rank 47). Eliminating slow inert reactions produces richer dynamics. This is the primary dataset for GNN training."
  },
  {
    "objectID": "model.html#summary-the-full-model",
    "href": "model.html#summary-the-full-model",
    "title": "Model: Metabolic Network Dynamics",
    "section": "Summary: The Full Model",
    "text": "Summary: The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate follows mass-action kinetics:\n\\[\nv_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\n\\]"
  },
  {
    "objectID": "model.html#the-inverse-problem",
    "href": "model.html#the-inverse-problem",
    "title": "Model: Metabolic Network Dynamics",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nGiven observed concentration time series \\(\\mathbf{C}(t)\\) and the bipartite graph structure, the goal is to recover the model components that generated the dynamics. A graph neural network (GNN) operates on the bipartite metabolite–reaction graph and learns two functions and a set of scalar parameters:\n\nWhat the GNN Learns\nThe forward model is:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\prod_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nThe GNN replaces the known functions with learnable MLPs, and the rate constants with learnable parameters. It must recover all three simultaneously from concentration data alone:\n1. Substrate function \\(f_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\)\nA neural network that learns how each substrate concentration contributes to the reaction rate. The ground-truth function is the mass-action power law \\(c_k^{|S_{kj}|}\\), but the GNN does not know this — it must discover the functional form from data:\n\\[\n\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|) \\;\\overset{?}{\\approx}\\; c_k^{|S_{kj}|}\n\\]\nThis is a function discovery problem: the MLP receives concentration and stoichiometric coefficient as inputs and must learn to output the correct power-law relationship. Since the stoichiometric coefficients are typically 1 or 2, the MLP must learn to distinguish between linear (\\(c^1\\)) and quadratic (\\(c^2\\)) dependencies.\n2. Homeostasis function \\(f_{\\text{node}} \\to \\text{MLP}_{\\text{node}}(c_i)\\)\nA neural network that learns the per-metabolite self-regulation term. The ground truth is a linear function \\(-\\lambda_{\\text{type}(i)} (c_i - c_i^{\\text{baseline}})\\) with small magnitude, but the GNN must discover this from data:\n\\[\n\\text{MLP}_{\\text{node}}(c_i) \\;\\overset{?}{\\approx}\\; -\\lambda_{\\text{type}(i)} \\cdot (c_i - c_i^{\\text{baseline}})\n\\]\nThis function captures how each metabolite is regulated independently of reactions — pulling concentrations back toward a baseline level. The challenge is that homeostatic forces are small compared to reaction rates, so \\(\\text{MLP}_{\\text{node}}\\) must learn a subtle signal without absorbing information that belongs to the reaction terms.\n3. Rate constants \\(k_j\\) (256 learnable scalars)\nPer-reaction rate constants learned in log-space. Unlike the MLPs, these are not functions but a vector of 256 scalar parameters — one per reaction — that scale the reaction fluxes.\n\n\nIdentifiability Challenges\nThe three components interact and can compensate for each other:\n\nScale ambiguity: \\(k_j \\cdot \\text{MLP}_{\\text{sub}}\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). Without anchoring, the MLP can absorb a global scale factor and shift all \\(k\\) values. Regularization (\\(\\texttt{coeff\\_k\\_center}\\)) anchors \\(\\text{mean}(\\log k)\\) to the known range center.\nFunction compensation: If \\(\\text{MLP}_{\\text{sub}}\\) learns a wrong functional form (e.g., \\(c^{1.5}\\) instead of \\(c^2\\)), the rate constants can partially compensate by adjusting their values. This leads to degenerate solutions with high prediction accuracy but poor parameter recovery.\nHomeostasis absorption: \\(\\text{MLP}_{\\text{node}}\\) can grow large and absorb dynamics that should be explained by the reaction terms, masking the true rate constants.\n\n\n\nLearning Modes\n\n\n\n\n\n\n\n\n\nMode\nS matrix\nPrimary metric\nChallenge\n\n\n\n\nS learning\nLearnable\nstoichiometry \\(R^2\\)\nRecovering integer coefficients and sparsity\n\n\nS given\nFrozen from GT\nrate constants \\(R^2\\)\nDisentangling \\(k\\), \\(\\text{MLP}_{\\text{sub}}\\), \\(\\text{MLP}_{\\text{node}}\\)"
  },
  {
    "objectID": "gnn-llm-memory.html",
    "href": "gnn-llm-memory.html",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#the-exploration-loop",
    "href": "gnn-llm-memory.html#the-exploration-loop",
    "title": "GNN-LLM-Memory",
    "section": "",
    "text": "flowchart LR\n    A[Experiment] --&gt; B[LLM]\n    B --&gt; A\n\n    B --&gt; C[(Memory)]\n    C --&gt; B\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n\n\n\n\n\n\nThe framework implements a closed-loop exploration engine composed of three interacting components:\n\nExperiment The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see Home) is trained to predict \\(dc/dt\\) while jointly recovering rate constants \\(k_j\\) and homeostatic functions. 4 parallel slots run simultaneously per batch via UCB tree search.\nLLM The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.\nMemory Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error."
  },
  {
    "objectID": "gnn-llm-memory.html#training-scheme",
    "href": "gnn-llm-memory.html#training-scheme",
    "title": "GNN-LLM-Memory",
    "section": "Training Scheme",
    "text": "Training Scheme\nThe GNN is trained by minimizing the prediction error on \\(dc/dt\\):\n\\[\n\\mathcal{L} = \\sum_{\\text{frames}} \\left\\| \\frac{dc}{dt}_{\\text{pred}} - \\frac{dc}{dt}_{\\text{GT}} \\right\\|_2 + \\mathcal{R}\n\\]\nwhere \\(\\mathcal{R}\\) is the sum of regularization terms described below.\n\nSeparate Learning Rates\nEach model component has its own learning rate to control the balance between parameter groups:\n\n\n\n\n\n\n\n\n\nComponent\nConfig key\nControls\nTypical range\n\n\n\n\nRate constants \\(k_j\\)\nlearning_rate_k\nHow fast k values are updated\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{node}}\\) (homeostasis)\nlearning_rate_node\nHomeostasis function learning speed\n1E-4 to 1E-2\n\n\nMLP\\(_{\\text{sub}}\\) (substrate)\nlearning_rate_sub\nSubstrate function learning speed\n1E-4 to 1E-2\n\n\n\nThe learning rate balance is critical:\n\nlr_k too high: \\(k\\) values overshoot, oscillate, or converge to wrong values\nlr_k too low: \\(k\\) barely moves, MLPs compensate\nlr_node/lr_sub imbalance: one function absorbs capacity meant for the other\n\n\n\nRegularization Terms\nThe total regularization \\(\\mathcal{R}\\) is the sum of the following penalties:\n\nMLP\\(_{\\text{sub}}\\) Monotonicity (coeff_MLP_sub_diff)\nMLP\\(_{\\text{sub}}\\) learns \\(c^s\\) which should be monotonically increasing in concentration. This penalty samples concentration pairs \\((c, c+\\delta)\\) and penalizes cases where the output decreases:\n\\[\n\\mathcal{R}_{\\text{sub\\_diff}} = \\left\\| \\text{ReLU}\\left(\\|\\text{MLP}_{\\text{sub}}(c)\\| - \\|\\text{MLP}_{\\text{sub}}(c+\\delta)\\|\\right) \\right\\|_2 \\cdot \\lambda_{\\text{sub\\_diff}}\n\\]\nWithout this constraint, MLP\\(_{\\text{sub}}\\) can develop non-physical local minima that don’t match the true power law behavior.\n\n\nMLP\\(_{\\text{node}}\\) L1 (coeff_MLP_node_L1)\nPenalizes large MLP\\(_{\\text{node}}\\) output to keep homeostasis values small relative to reaction terms:\n\\[\n\\mathcal{R}_{\\text{node\\_L1}} = \\text{mean}\\left(|\\text{MLP}_{\\text{node}}(c_i, a_i)|\\right) \\cdot \\lambda_{\\text{node\\_L1}}\n\\]\nMLP\\(_{\\text{node}}\\) is initialized to zero output so homeostasis starts inactive. Combined with a reduced learning rate, this prevents MLP\\(_{\\text{node}}\\) from dominating the dynamics. The true homeostatic \\(\\lambda\\) values are small (0.001–0.002), so MLP\\(_{\\text{node}}\\) output should remain small.\n\n\nMLP\\(_{\\text{sub}}\\) Normalization (coeff_MLP_sub_norm)\nBreaks the scale ambiguity between \\(k\\) and MLP\\(_{\\text{sub}}\\) at the source. The product \\(k_j \\cdot \\text{MLP}_{\\text{sub}}(c)\\) is invariant under \\(k \\to \\alpha k\\), \\(\\text{MLP}_{\\text{sub}} \\to \\text{MLP}_{\\text{sub}} / \\alpha\\). This penalty enforces that MLP\\(_{\\text{sub}}\\) outputs 1 at the reference point \\(c=1, |s|=1\\), where the true value \\(c^s = 1^1 = 1\\):\n\\[\n\\mathcal{R}_{\\text{sub\\_norm}} = \\left(\\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\| - 1\\right)^2 \\cdot \\lambda_{\\text{sub\\_norm}}\n\\]\nSince \\(c^s = 1\\) at \\(c=1\\) for any stoichiometry \\(s\\), this pins the MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) to 1 and prevents \\(k\\) from absorbing a global factor.\n\n\nRate Constant Floor (coeff_k_floor)\nPrevents \\(\\log_{10} k_j\\) from drifting far below the physically plausible range. Without this, some reactions develop outlier values (e.g. \\(\\log k = -4\\) when the true range is \\([-2, -1]\\)), which distorts the \\(R^2\\) even when most reactions are well-recovered:\n\\[\n\\mathcal{R}_{\\text{k\\_floor}} = \\sum_j \\text{ReLU}\\left(\\tau - \\log_{10} k_j\\right)^2 \\cdot \\lambda_{\\text{k\\_floor}}\n\\]\nwhere \\(\\tau\\) is the configurable threshold (k_floor_threshold, default \\(-3\\)). Only \\(\\log k\\) values below \\(\\tau\\) are penalized.\n\n\n\nScalar Correction\nEven without the normalization regularization, a post-hoc scalar correction is applied when evaluating rate constants. The MLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha\\) is measured by evaluating MLP\\(_{\\text{sub}}\\) at the reference point:\n\\[\n\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\n\\]\nWith multiplicative aggregation, each reaction rate absorbs \\(\\alpha^{n_j}\\) where \\(n_j\\) is the number of substrates. The corrected rate constants are:\n\\[\n\\log_{10} k_j^{\\text{corrected}} = \\log_{10} k_j^{\\text{learned}} + n_j \\cdot \\log_{10} \\alpha\n\\]\nThe reported rate_constants_R2 is computed on these corrected values against the identity line \\(y=x\\).\n\n\nSummary of Hyperparameters\n\n\n\n\n\n\n\n\n\nParameter\nConfig key\nDescription\nTypical range\n\n\n\n\nLearning rate \\(k\\)\nlearning_rate_k\nRate constants update speed\n1E-4 to 1E-2\n\n\nLearning rate node\nlearning_rate_node\nMLP\\(_{\\text{node}}\\) update speed\n1E-4 to 1E-2\n\n\nLearning rate sub\nlearning_rate_sub\nMLP\\(_{\\text{sub}}\\) update speed\n1E-4 to 1E-2\n\n\nBatch size\nbatch_size\nTime frames per gradient step\n4 to 32\n\n\nTraining iterations\ndata_augmentation_loop\nMultiplier for iterations per epoch\n100 to 5000\n\n\nEpochs\nn_epochs\nNumber of training epochs\n1 to 5\n\n\nTime step\ntime_step\nSteps per gradient update (1 = single-step, &gt;1 = recurrent rollout)\n1 to 8\n\n\nRecurrent training\nrecurrent_training\nEnable multi-step rollout training\ntrue / false\n\n\nMLP\\(_{\\text{sub}}\\) monotonicity\ncoeff_MLP_sub_diff\nPenalize non-increasing MLP\\(_{\\text{sub}}\\)\n0 to 500\n\n\nMLP\\(_{\\text{node}}\\) L1\ncoeff_MLP_node_L1\nPenalize large homeostasis output\n0 to 10\n\n\nMLP\\(_{\\text{sub}}\\) normalization\ncoeff_MLP_sub_norm\nPin MLP\\(_{\\text{sub}}(c{=}1, |s|{=}1)\\) to 1\n0 to 10\n\n\nRate constant floor\ncoeff_k_floor\nPenalize \\(\\log k\\) below threshold\n0 to 10\n\n\nFloor threshold\nk_floor_threshold\nThreshold for k floor penalty\n\\(-3\\) (default)\n\n\nMLP\\(_{\\text{sub}}\\) hidden dim\nhidden_dim_sub\nHidden layer width for substrate MLP\n16 to 128\n\n\nMLP\\(_{\\text{sub}}\\) layers\nn_layers_sub\nNumber of layers for substrate MLP\n2 to 5\n\n\nMLP\\(_{\\text{node}}\\) hidden dim\nhidden_dim_node\nHidden layer width for homeostasis MLP\n16 to 128\n\n\nMLP\\(_{\\text{node}}\\) layers\nn_layers_node\nNumber of layers for homeostasis MLP\n2 to 5"
  },
  {
    "objectID": "gnn-llm-memory.html#metrics",
    "href": "gnn-llm-memory.html#metrics",
    "title": "GNN-LLM-Memory",
    "section": "Metrics",
    "text": "Metrics\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood value\n\n\n\n\nrate_constants_R2\nR² between learned and true rate constants \\(k\\) (after scalar correction, excluding outliers)\n&gt; 0.9\n\n\nn_outliers\nNumber of reactions with \\(|\\Delta \\log_{10} k| &gt; 0.3\\) (factor of 2 error in \\(k\\)-space)\n&lt; 25\n\n\nslope\nSlope of linear fit between learned and true \\(\\log_{10} k\\). Slope \\(&lt; 1\\) means the learned range is compressed.\n\\(\\approx 1.0\\)\n\n\ntest_R2\nR² on held-out test frames (rollout prediction)\n&gt; 0.9\n\n\ntest_pearson\nPearson correlation on test frames\n&gt; 0.95\n\n\nfinal_loss\nFinal prediction loss (MSE on \\(dc/dt\\))\nLower is better\n\n\nalpha\nMLP\\(_{\\text{sub}}\\) scale factor \\(\\alpha = \\|\\text{MLP}_{\\text{sub}}(c{=}1, |s|{=}1)\\|\\). Ideal value is 1.0 — indicates MLP\\(_{\\text{sub}}\\) has learned the correct scale. Deviations from 1 indicate residual scale ambiguity that the scalar correction must compensate for.\n\\(\\approx 1.0\\)\n\n\n\n\nDiagnostic Interpretation\nThe relationship between rate_constants_R2 and test_pearson diagnoses whether the model found the true rate constants or a degenerate solution:\n\n\n\n\n\n\n\n\ntest_pearson\n\\(R^2\\)\nDiagnosis\n\n\n\n\n&gt; 0.95\n&gt; 0.9\nHealthy — good dynamics from correct \\(k\\)\n\n\n&gt; 0.95\n0.3–0.9\nDegenerate — good dynamics from wrong \\(k\\) (MLPs compensate)\n\n\n&gt; 0.95\n&lt; 0.3\nSeverely degenerate — MLPs absorb all dynamics\n\n\n&lt; 0.5\n&gt; 0.9\nGood \\(k\\), poor rollout — rate constants correct but MLP errors compound during integration\n\n\n&lt; 0.5\n&lt; 0.5\nFailed — both dynamics and \\(k\\) poor\n\n\n\nThe current oscillatory regime experiments show the good \\(k\\), poor rollout pattern: \\(R^2 \\approx 0.93\\) but Pearson \\(\\approx 0.05\\). The MLP\\(_{\\text{sub}}\\) scale compression (\\(\\alpha \\approx 0.4\\)) and MLP\\(_{\\text{node}}\\) failure (flat at zero) produce errors that accumulate during multi-step rollout, even though the rate constants themselves are well-recovered."
  },
  {
    "objectID": "gnn-llm-memory.html#ucb-tree-search",
    "href": "gnn-llm-memory.html#ucb-tree-search",
    "title": "GNN-LLM-Memory",
    "section": "UCB Tree Search",
    "text": "UCB Tree Search\nThe LLM selects parent configurations to mutate using an Upper Confidence Bound (UCB) strategy that balances exploitation of high-performing branches with exploration of under-visited regions:\n\\[\n\\text{UCB}(i) = \\bar{X}_i + c \\cdot \\sqrt{\\frac{\\ln N}{n_i}}\n\\]\nwhere \\(\\bar{X}_i\\) is the mean reward of node \\(i\\), \\(N\\) is the total number of visits, \\(n_i\\) is the number of visits to node \\(i\\), and \\(c\\) is the exploration constant.\n4 parallel slots run per batch with diversified roles:\n\n\n\n\n\n\n\n\nSlot\nRole\nDescription\n\n\n\n\n0\nexploit\nHighest UCB node, conservative mutation\n\n\n1\nexploit\n2nd highest UCB, or same parent different param\n\n\n2\nexplore\nUnder-visited node, or new parameter dimension\n\n\n3\nprinciple-test\nTest or challenge one established principle from memory"
  },
  {
    "objectID": "application.html",
    "href": "application.html",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#motivation",
    "href": "application.html#motivation",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "",
    "text": "MetabolismGraph is developed and validated on synthetic data where ground truth is known. But the ultimate goal is to apply the GNN framework to real metabolomic time series — recovering reaction rate constants and network structure from experimentally measured concentration dynamics.\nA recent study by Nardin et al. (2025) provides exactly the kind of data this framework is designed to analyze."
  },
  {
    "objectID": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "href": "application.html#chronic-jugular-microdialysis-in-freely-moving-mice",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Chronic Jugular Microdialysis in Freely Moving Mice",
    "text": "Chronic Jugular Microdialysis in Freely Moving Mice\nNardin et al. developed a workflow combining chronic jugular microdialysis with chemical isotope labeling LC-MS to continuously measure bloodborne metabolites in freely moving mice:\n\n\n\nProperty\nValue\n\n\n\n\nCompounds measured\n~123 high-quality amines and small peptides\n\n\nSampling cadence\n7.5 minutes\n\n\nRecording duration\n~8 hours per session\n\n\nTime frames\n~64 per session\n\n\nAnimals\n3 mice, implants patent for &gt;7 days\n\n\n\nThe data captures real metabolic dynamics: purine turnover correlating with movement, delayed histamine/5-HIAA changes, coordinated amino-acid dynamics, and state-dependent metabolic shifts between locomotion and rest.\n\nLow-Rank Physiological State Space\nA key finding: 10 principal components explain 73% of the variance across all measured compounds. The first component alone (rPC1) captures 35.5% and is strongly correlated with locomotion (Spearman \\(r = -0.59\\), peaking at 7.5 min lag).\nThis low-rank structure is consistent with what we observe in our simulations:\n\n\n\n\n\n\n\n\n\nNardin et al.\nMetabolismGraph simulations\n\n\n\n\nn_metabolites\n~123\n100\n\n\nRank at 70% variance\n~10\n~5–15 (estimated)\n\n\nRank at 99% variance\n—\n24–50\n\n\nDominant mode\nLocomotion-driven amino acid metabolism\nAutocatalytic cycle oscillations\n\n\n\nThe paper concludes that “endocrine and metabolic control may operate over a compact set of latent variables” — the same low-rank assumption that underlies our activity rank analysis via SVD."
  },
  {
    "objectID": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "href": "application.html#applying-metabolismgraph-to-microdialysis-data",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Applying MetabolismGraph to Microdialysis Data",
    "text": "Applying MetabolismGraph to Microdialysis Data\n\nWhat changes\nMoving from synthetic to real data introduces several differences:\n1. The stoichiometric matrix S is partially known\nIn simulation, \\(\\mathbf{S}\\) is either generated (ground truth) or learned from data. For real metabolomics, the reaction network connecting measured compounds can be partially reconstructed from biochemical databases (KEGG, Recon3D, HMDB). However:\n\nNot all reactions are known\nNot all participants in a reaction are measured (the 123 compounds are a subset of the full metabolome)\nSome measured compounds participate in multiple pathways\n\nThis suggests an intermediate mode between “S given” and “S learning”: initialize S from database knowledge, then refine with data.\n2. Observation is incomplete\nThe microdialysis probe measures only the free (unbound) fraction of molecules below the 6 kDa molecular weight cutoff. Many metabolites, enzymes, and signaling molecules are invisible. The GNN must learn dynamics from a partial observation of the full system — analogous to learning neural dynamics from a subset of recorded neurons.\n3. External drivers exist\nThe synthetic model is autonomous: \\(dc/dt\\) depends only on current concentrations and the network. Real metabolic dynamics are driven by external inputs — feeding, locomotion, circadian rhythms, stress — that are not part of the reaction network. The paper shows locomotion is a dominant driver (rPC1).\nThis maps to extensions of the current model:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_j S_{ij} \\cdot v_j}_{\\text{reactions}} + \\underbrace{f_{\\text{ext}}(x_{\\text{locomotion}}, t)}_{\\text{external drive}}\n\\]\nThe circadian_amplitude and circadian_period parameters already in the simulation config are a first step toward modeling time-varying external inputs.\n4. Kinetics may not be mass-action\nEnzyme-mediated reactions follow Michaelis-Menten or Hill kinetics rather than pure mass-action. The flexible MLP\\(_{\\text{sub}}\\) architecture is well-suited here — it can discover the true functional form without assuming \\(c^{|S|}\\) a priori. The additive aggregation mode may be more appropriate for enzyme-mediated reactions where substrates contribute more independently.\n\n\nWhat stays the same\nThe core framework transfers directly:\n\nBipartite graph structure: metabolites connected to reactions through stoichiometric edges\nGNN message passing: substrate concentrations aggregated per reaction, then distributed back to metabolites\nFunction discovery: MLP\\(_{\\text{sub}}\\) learns the concentration-to-rate mapping, MLP\\(_{\\text{node}}\\) learns homeostatic regulation\nRate constant recovery: per-reaction \\(k_j\\) learned in log-space\nIdentifiability challenges: scale ambiguity, function compensation, and homeostasis absorption are equally present (and harder to diagnose without ground truth)\n\n\n\nProposed workflow\n\n\n\n\n\n%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    A[Microdialysis time series&lt;br/&gt;123 compounds × 64 frames] --&gt; B[Construct bipartite graph&lt;br/&gt;from KEGG/Recon3D]\n    B --&gt; C[Initialize S from&lt;br/&gt;database stoichiometry]\n    C --&gt; D[Train GNN&lt;br/&gt;predict dc/dt]\n    D --&gt; E{Evaluate}\n    E --&gt;|test R² &gt; 0.9| F[Extract learned parameters]\n    E --&gt;|test R² &lt; 0.9| G[Refine: unfreeze S,&lt;br/&gt;add external inputs]\n    G --&gt; D\n    F --&gt; H[Rate constants k_j]\n    F --&gt; I[Learned kinetics&lt;br/&gt;MLP_sub shape]\n    F --&gt; J[Homeostatic regulation&lt;br/&gt;MLP_node per metabolite type]\n\n\n\n\n\n\nStep 1. Build the metabolite-reaction bipartite graph from KEGG pathway maps for the 123 identified compounds. Estimate the number of reactions linking measured metabolites.\nStep 2. Initialize \\(\\mathbf{S}\\) from known stoichiometry (S given mode). Train the GNN to predict \\(dc/dt\\) from concentration snapshots, learning \\(k_j\\), MLP\\(_{\\text{sub}}\\), and MLP\\(_{\\text{node}}\\).\nStep 3. Evaluate prediction quality on held-out time frames. If prediction R\\(^2\\) is high but the learned MLP\\(_{\\text{sub}}\\) does not match expected kinetics (mass-action or Michaelis-Menten), investigate degeneracy.\nStep 4. Optionally unfreeze \\(\\mathbf{S}\\) to discover missing reactions or correct database errors (S learning mode). Compare learned stoichiometry against biochemical databases.\nStep 5. Incorporate locomotion and other behavioral covariates as external inputs to account for the dominant non-metabolic drivers of concentration change."
  },
  {
    "objectID": "application.html#challenges-and-open-questions",
    "href": "application.html#challenges-and-open-questions",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "Challenges and Open Questions",
    "text": "Challenges and Open Questions\nTemporal resolution. The 7.5-minute sampling cadence gives ~64 frames per session. Our simulations use 2880 frames. With fewer time points, the GNN has less data to disentangle rate constants from MLP functions, exacerbating identifiability issues. Multi-session data across the 7+ days of implant patency could help.\nMissing metabolites. The 123 measured compounds are a fraction of the full metabolome. Reactions involving unmeasured substrates will appear as unexplained variance. The MLP\\(_{\\text{node}}\\) may absorb some of this as apparent “homeostasis.”\nNon-stationarity. Real metabolic dynamics are non-stationary — circadian rhythms, feeding cycles, and adaptation shift the baseline over hours. The current model assumes fixed \\(c^{\\text{baseline}}\\) and \\(\\lambda\\). Time-varying homeostatic parameters may be needed.\nValidation without ground truth. In simulation, we validate against known \\(k_j\\) values (rate constants R\\(^2\\)). With real data, validation must rely on: (1) held-out prediction accuracy, (2) consistency with known biochemistry, (3) perturbation experiments (the paper notes microdialysis can also deliver molecules), and (4) cross-animal reproducibility."
  },
  {
    "objectID": "application.html#references",
    "href": "application.html#references",
    "title": "Application: From Synthetic to Real Metabolomics",
    "section": "References",
    "text": "References\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "",
    "text": "MetabolismGraph is a framework for learning the structure of metabolic networks from concentration dynamics using Graph Neural Networks (GNNs). Given time-series measurements of metabolite concentrations, the model recovers:\n\nRate constants \\(k_j\\) — the intrinsic speed of each reaction\nFunctional forms \\(f_{\\text{sub}}(c, s)\\), \\(f_{\\text{node}}(c)\\) — how substrates drive reactions and how metabolites self-regulate\n\n\n\n\n\n\nflowchart LR\n    subgraph met[\"Metabolites\"]\n        direction TB\n        c1((c₁))\n        c2((c₂))\n        c3((c₃))\n        c4((c₄))\n    end\n\n    subgraph rxn[\"Reactions\"]\n        direction TB\n        R1[\"R₁ · k₁\"]\n        R2[\"R₂ · k₂\"]\n        R3[\"R₃ · k₃\"]\n    end\n\n    c1 --&gt;|\"−1\"| R1\n    c2 --&gt;|\"−1\"| R1\n    R1 --&gt;|\"+1\"| c3\n    R1 --&gt;|\"+1\"| c4\n    c3 --&gt;|\"−1\"| R2\n    R2 --&gt;|\"+1\"| c1\n    c2 --&gt;|\"−1\"| R3\n    c4 --&gt;|\"−1\"| R3\n    R3 --&gt;|\"+1\"| c2\n\n    style c1 fill:#e1f5fe,stroke:#0277bd\n    style c2 fill:#e1f5fe,stroke:#0277bd\n    style c3 fill:#e1f5fe,stroke:#0277bd\n    style c4 fill:#e1f5fe,stroke:#0277bd\n    style R1 fill:#fff3e0,stroke:#ef6c00\n    style R2 fill:#fff3e0,stroke:#ef6c00\n    style R3 fill:#fff3e0,stroke:#ef6c00\n    style met fill:none,stroke:#0277bd,stroke-dasharray: 5 5\n    style rxn fill:none,stroke:#ef6c00,stroke-dasharray: 5 5\n\n\n\n\n\n\nMetabolites (blue circles) and reactions (orange boxes) form a bipartite graph — a graph with two distinct node types where edges only connect nodes of different types. Each edge carries a stoichiometric coefficient \\(S_{ij}\\). A standard single-partite graph (metabolite \\(\\leftrightarrow\\) metabolite) cannot represent this system because each reaction involves multiple substrates and products simultaneously. A single edge between two metabolites would lose the information that they participate in the same reaction with a specific rate constant \\(k_j\\)."
  },
  {
    "objectID": "index.html#the-full-model",
    "href": "index.html#the-full-model",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Full Model",
    "text": "The Full Model\nThe complete metabolic dynamics:\n\\[\n\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i \\cdot (c_i - c_i^{\\text{baseline}})}_{\\text{homeostasis}} + \\underbrace{\\sum_{j=1}^{m} S_{ij} \\cdot v_j}_{\\text{reaction dynamics}}\n\\]\nwhere the reaction rate \\(v_j\\) depends on aggregation type:\n\n\n\n\n\n\n\nAggregation\nRate \\(v_j\\)\n\n\n\n\nAdditive\n\\(v_j = k_j \\cdot \\sum_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\nMultiplicative\n\\(v_j = k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{|S_{kj}|}\\)\n\n\n\nSee Model for detailed equations, diagrams, and model configurations."
  },
  {
    "objectID": "index.html#the-inverse-problem",
    "href": "index.html#the-inverse-problem",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe forward model describes how concentrations evolve given all parameters. In practice, the parameters themselves are unknown. The inverse problem is to recover them from observed dynamics.\nGiven:\n\nConcentration trajectories \\(\\{c_i(t)\\}_{i=1}^{n}\\) measured over time\nStoichiometric matrix \\(\\mathbf{S}\\) (known from biochemistry)\n\nTo learn:\n\nSubstrate function \\(\\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)\\) — discovers the mass-action power law \\(c_k^{|S_{kj}|}\\)\nHomeostasis function \\(\\text{MLP}_{\\text{node}}(c_i)\\) — discovers per-metabolite regulation \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\nRate constants \\(k_j\\) — per-reaction speed scalars\n\nThis is challenging because the system is high-dimensional (\\(n\\) metabolites, \\(m\\) reactions), the mapping from parameters to dynamics is nonlinear, and multiple parameter combinations can produce similar trajectories (identifiability). Classical optimization approaches struggle with this combinatorial landscape.\nWe address this by casting the inverse problem as a Graph Neural Network learning task. The metabolic network is naturally a bipartite graph (metabolites \\(\\leftrightarrow\\) reactions), and we replace the unknown functions with learnable MLPs that operate on this graph structure. The GNN is trained end-to-end by minimizing the prediction error on \\(dc/dt\\), recovering the rate constants and homeostatic functions simultaneously. An LLM-driven closed-loop exploration engine systematically searches the hyperparameter space — see GNN-LLM-Memory for the training scheme, regularization terms, and exploration loop.\n\nGNN Parameterization\n\\[\n\\frac{dc_i}{dt} = \\underbrace{\\text{MLP}_{\\text{node}}(c_i, a_i)}_{\\text{learns } -\\lambda_i(c_i - c_i^{\\text{baseline}})} + \\sum_{j=1}^{m} S_{ij} \\cdot \\underbrace{k_j \\cdot \\prod_{k \\in \\text{sub}(j)} \\text{MLP}_{\\text{sub}}(c_k, |S_{kj}|)}_{\\text{learns } k_j \\text{ and } c_k^{|S_{kj}|}}\n\\]\nwhere:\n\n\\(a_i \\in \\mathbb{R}^d\\) is a learnable embedding for metabolite \\(i\\)\n\\(k_j\\) are learnable rate constants\n\\(\\prod\\) denotes multiplicative aggregation (mass-action kinetics)\n\n\n\nLearnable Parameters\n\n\n\n\n\n\n\n\nParameter\nType\nPurpose\n\n\n\n\n\\(a_i\\)\nEmbedding vectors\nPer-metabolite identity\n\n\n\\(k_j\\)\nScalars\nPer-reaction rate constants\n\n\n\\(\\text{MLP}_{\\text{node}}\\)\nNeural network\nLearns \\(-\\lambda_i(c_i - c_i^{\\text{baseline}})\\)\n\n\n\\(\\text{MLP}_{\\text{sub}}\\)\nNeural network\nLearns \\(c_k^{|S_{kj}|}\\)"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "MetabolismGraph: Learning Metabolism Dynamics with Graph Neural Networks",
    "section": "Citation",
    "text": "Citation\nIf you use MetabolismGraph in your research, please cite:\n@software{metabolismgraph2025,\n  author = {Allier, Cédric},\n  title = {MetabolismGraph: Learning Metabolism Dynamics with GNNs},\n  year = {2026},\n  url = {https://github.com/allierc/MetabolismGraph}\n}"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This page was generated by Claude and may contain inaccuracies in author lists, publication details, or descriptions. Please verify citations before use."
  },
  {
    "objectID": "references.html#context",
    "href": "references.html#context",
    "title": "References",
    "section": "Context",
    "text": "Context\nMetabolismGraph sits at the intersection of several active research areas: graph neural networks for physical and biological systems, inverse problems in systems biology, neural differential equations, and LLM-driven scientific exploration. The framework uses message-passing GNNs on bipartite metabolite-reaction graphs to solve the inverse problem of recovering kinetic parameters from concentration dynamics, with an LLM-based closed-loop exploration engine for hyperparameter optimization.\nBelow we collect key references organized by topic. Where available, we provide arXiv or DOI links."
  },
  {
    "objectID": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "href": "references.html#graph-neural-networks-for-physical-and-biological-systems",
    "title": "References",
    "section": "Graph Neural Networks for Physical and Biological Systems",
    "text": "Graph Neural Networks for Physical and Biological Systems\n\nSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., & Battaglia, P. W. (2020). Learning to simulate complex physics with graph networks. ICML 2020. arXiv:2002.09405 — Graph network-based simulators (GNS) for learning physical dynamics from particle-based representations. Demonstrates that GNNs can learn accurate forward simulators for complex physical systems.\nKipf, T., Fetaya, E., Wang, K.-C., Welling, M., & Zemel, R. (2018). Neural relational inference for interacting systems. ICML 2018. arXiv:1802.04687 — Learns interaction graphs from observed trajectories using variational autoencoders on graph structures. Closely related to our inverse-problem setting where the goal is to recover network structure from dynamics.\nCranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., & Ho, S. (2020). Discovering symbolic models from deep learning with inductive biases. NeurIPS 2020. arXiv:2006.11287 — Combines GNNs with symbolic regression to extract interpretable physical laws from learned representations. Relevant to our approach of recovering interpretable kinetic parameters from learned MLP functions."
  },
  {
    "objectID": "references.html#neural-differential-equations-and-scientific-ml",
    "href": "references.html#neural-differential-equations-and-scientific-ml",
    "title": "References",
    "section": "Neural Differential Equations and Scientific ML",
    "text": "Neural Differential Equations and Scientific ML\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural ordinary differential equations. NeurIPS 2018. arXiv:1806.07366 — Foundational work on continuous-depth neural networks parameterized as ODEs, enabling gradient-based learning of dynamical systems.\nRackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R., Skinner, D., Ramadhan, A., & Edelman, A. (2020). Universal differential equations for scientific machine learning. arXiv:2001.04385 — Framework combining differential equations with neural networks for scientific modeling. The UDE approach of embedding learnable components within known differential equation structure is conceptually similar to how MetabolismGraph embeds learnable MLPs within the known stoichiometric framework."
  },
  {
    "objectID": "references.html#symbolic-regression-for-network-dynamics",
    "href": "references.html#symbolic-regression-for-network-dynamics",
    "title": "References",
    "section": "Symbolic Regression for Network Dynamics",
    "text": "Symbolic Regression for Network Dynamics\n\nYu, Z., Ding, J., & Li, Y. (2025). Discovering network dynamics with neural symbolic regression. Nature Computational Science. DOI:10.1038/s43588-025-00893-8 — ND2: neural symbolic regression that discovers governing equations of network dynamics directly from data. Applied to gene regulatory networks, the method corrects the classical Hill equation model by replacing per-neighbor nonlinear terms with a logistic function applied to the aggregate neighbor sum (see comparison below).\n\n\nComparison with MetabolismGraph\nYu et al.’s corrected gene regulation model (their Eq. 2) and MetabolismGraph share the same general ODE structure — self-dynamics plus interaction dynamics — but differ in how neighbor contributions are aggregated:\nYu et al. — Gene regulation (ND2 corrected):\n\\[\\frac{dx_i}{dt} = \\underbrace{s_i - \\gamma_i x_i}_{\\text{self-dynamics}} + \\underbrace{\\beta \\, \\tilde{S}\\!\\left(\\sum_j A_{ij}\\, x_j\\right)}_{\\text{interaction}}\\]\nwhere \\(\\tilde{S}(x) = (1 + e^{-x})^{-1}\\) is the logistic function. The nonlinearity acts on the sum of weighted neighbor states — no per-edge rate constants, no multiplicative aggregation.\nMetabolismGraph — Metabolic kinetics:\n\\[\\frac{dc_i}{dt} = \\underbrace{-\\lambda_i(c_i - c_i^{\\text{baseline}})}_{\\text{self-dynamics}} + \\underbrace{\\sum_j S_{ij} \\cdot k_j \\cdot \\prod_{k \\in \\text{sub}(j)} c_k^{s_{kj}}}_{\\text{interaction}}\\]\nKey differences:\n\n\n\n\n\n\n\n\n\nYu et al. (gene regulation)\nMetabolismGraph (metabolism)\n\n\n\n\nGraph\nMonopartite (gene → gene)\nBipartite (metabolite ↔︎ reaction)\n\n\nAggregation\n\\(\\tilde{S}(\\sum_j A_{ij} x_j)\\) — logistic of sum\n\\(\\sum_j S_{ij} k_j \\prod_k c_k^{s_{kj}}\\) — sum of products\n\n\nRate constants\nSingle global \\(\\beta\\)\nPer-reaction \\(k_j\\) (256 parameters)\n\n\nNonlinearity\nBounded logistic \\(\\tilde{S} \\in [0,1]\\)\nUnbounded power law \\(c^s\\)\n\n\nHigher-order\nImplicit: \\(\\partial \\dot{x}_i / \\partial x_j\\) depends on all neighbors via \\(\\tilde{S}\\)\nExplicit: mass-action products couple substrates within each reaction\n\n\n\nThe gene regulation model has no per-reaction aggregation step — it sums all neighbor states into a single scalar, then applies a saturating nonlinearity. MetabolismGraph instead computes a separate rate for each reaction (multiplicative aggregation of substrate concentrations), then sums the stoichiometric contributions. This reflects a fundamental difference between gene regulation (bounded transcriptional response) and metabolism (unbounded mass-action kinetics)."
  },
  {
    "objectID": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "href": "references.html#in-vivo-metabolomics-and-physiological-state-spaces",
    "title": "References",
    "section": "In Vivo Metabolomics and Physiological State Spaces",
    "text": "In Vivo Metabolomics and Physiological State Spaces\n\nNardin, M., Wang, N., Elziny, S., Boyer, C., Pjanovic, V., Schuster, L., Boklund, P., Lindo, S., Morris, K., Ilanges, A., Voigts, J., & Dennis, E.J. (2025). Reconstructing a physiological state space via chronic jugular microdialysis in freely moving mice. bioRxiv. doi:10.64898/2025.12.08.692974 — Chronic jugular microdialysis paired with LC-MS measures ~123 bloodborne compounds at 7.5-min cadence in freely moving mice. PCA reveals a low-rank physiological manifold: 10 components explain 73% of variance, with rPC1 aligned to locomotion. Provides the real-world metabolomic time series that MetabolismGraph’s inverse problem framework is designed to analyze. See Application for how the GNN framework maps to this data."
  },
  {
    "objectID": "references.html#llm-driven-scientific-discovery",
    "href": "references.html#llm-driven-scientific-discovery",
    "title": "References",
    "section": "LLM-Driven Scientific Discovery",
    "text": "LLM-Driven Scientific Discovery\n\nRomera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. Nature, 625, 468-475. DOI:10.1038/s41586-023-06924-6 — FunSearch: uses LLMs to discover new mathematical constructions through evolutionary program search. Pioneering demonstration that LLMs can make genuine scientific contributions when embedded in a search loop.\nNovikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration. Google DeepMind. — Extends the LLM-driven exploration paradigm to broader scientific and algorithmic discovery tasks. The closed-loop LLM exploration engine in MetabolismGraph draws inspiration from this line of work.\nLu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024). The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv:2408.06292 — End-to-end autonomous research agent that generates hypotheses, runs experiments, and writes papers."
  }
]