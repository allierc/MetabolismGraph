---
title: "GNN-LLM-Memory"
subtitle: "Closed-loop exploration of metabolic rate constant recovery"
---

## The Exploration Loop

```{mermaid}
%%| fig-width: 6
flowchart LR
    A[Experiment] --> B[LLM]
    B --> A

    B --> C[(Memory)]
    C --> B

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#f3e5f5
```

The framework implements a **closed-loop exploration engine** composed of three interacting components:

1. **Experiment**
   The metabolic simulator generates concentration dynamics from a known stoichiometric network. A GNN (see [Home](index.qmd)) is trained to predict $dc/dt$ while jointly recovering rate constants $k_j$ and homeostatic functions. **4 parallel slots** run simultaneously per batch via UCB tree search.

2. **LLM**
   The LLM interprets training results in context of accumulated memory, diagnoses failure modes (scale ambiguity, function degeneracy, MLP compensation), and selects the next hyperparameter mutation via UCB tree search.

3. **Memory**
   Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error.

---

## Training Scheme

The GNN is trained by minimizing the prediction error on $dc/dt$:

$$
\mathcal{L} = \sum_{\text{frames}} \left\| \frac{dc}{dt}_{\text{pred}} - \frac{dc}{dt}_{\text{GT}} \right\|_2 + \mathcal{R}
$$

where $\mathcal{R}$ is the sum of regularization terms described below.

### Separate Learning Rates

Each model component has its own learning rate to control the balance between parameter groups:

| Component | Config key | Controls | Typical range |
|-----------|-----------|----------|---------------|
| Rate constants $k_j$ | `learning_rate_k` | How fast k values are updated | 1E-4 to 1E-2 |
| MLP$_{\text{node}}$ (homeostasis) | `learning_rate_node` | Homeostasis function learning speed | 1E-4 to 1E-2 |
| MLP$_{\text{sub}}$ (substrate) | `learning_rate_sub` | Substrate function learning speed | 1E-4 to 1E-2 |

The learning rate balance is critical:

- **`lr_k` too high**: $k$ values overshoot, oscillate, or converge to wrong values
- **`lr_k` too low**: $k$ barely moves, MLPs compensate
- **`lr_node`/`lr_sub` imbalance**: one function absorbs capacity meant for the other

### Regularization Terms

The total regularization $\mathcal{R}$ is the sum of the following penalties:

#### MLP$_{\text{sub}}$ Monotonicity (`coeff_MLP_sub_diff`)

MLP$_{\text{sub}}$ learns $c^s$ which should be monotonically increasing in concentration. This penalty samples concentration pairs $(c, c+\delta)$ and penalizes cases where the output decreases:

$$
\mathcal{R}_{\text{sub\_diff}} = \left\| \text{ReLU}\left(\|\text{MLP}_{\text{sub}}(c)\| - \|\text{MLP}_{\text{sub}}(c+\delta)\|\right) \right\|_2 \cdot \lambda_{\text{sub\_diff}}
$$

Without this constraint, MLP$_{\text{sub}}$ can develop non-physical local minima that don't match the true power law behavior.

#### MLP$_{\text{node}}$ L1 (`coeff_MLP_node_L1`)

Penalizes large MLP$_{\text{node}}$ output to keep homeostasis values small relative to reaction terms:

$$
\mathcal{R}_{\text{node\_L1}} = \text{mean}\left(|\text{MLP}_{\text{node}}(c_i, a_i)|\right) \cdot \lambda_{\text{node\_L1}}
$$

MLP$_{\text{node}}$ is **initialized to zero output** so homeostasis starts inactive. Combined with a reduced learning rate, this prevents MLP$_{\text{node}}$ from dominating the dynamics. The true homeostatic $\lambda$ values are small (0.001--0.002), so MLP$_{\text{node}}$ output should remain small.

#### MLP$_{\text{sub}}$ Normalization (`coeff_MLP_sub_norm`)

Breaks the **scale ambiguity** between $k$ and MLP$_{\text{sub}}$ at the source. The product $k_j \cdot \text{MLP}_{\text{sub}}(c)$ is invariant under $k \to \alpha k$, $\text{MLP}_{\text{sub}} \to \text{MLP}_{\text{sub}} / \alpha$. This penalty enforces that MLP$_{\text{sub}}$ outputs 1 at the reference point $c=1, |s|=1$, where the true value $c^s = 1^1 = 1$:

$$
\mathcal{R}_{\text{sub\_norm}} = \left(\|\text{MLP}_{\text{sub}}(c{=}1, |s|{=}1)\| - 1\right)^2 \cdot \lambda_{\text{sub\_norm}}
$$

Since $c^s = 1$ at $c=1$ for any stoichiometry $s$, this pins the MLP$_{\text{sub}}$ scale factor $\alpha$ to 1 and prevents $k$ from absorbing a global factor.

#### Rate Constant Floor (`coeff_k_floor`)

Prevents $\log_{10} k_j$ from drifting far below the physically plausible range. Without this, some reactions develop outlier values (e.g. $\log k = -4$ when the true range is $[-2, -1]$), which distorts the $R^2$ even when most reactions are well-recovered:

$$
\mathcal{R}_{\text{k\_floor}} = \sum_j \text{ReLU}\left(\tau - \log_{10} k_j\right)^2 \cdot \lambda_{\text{k\_floor}}
$$

where $\tau$ is the configurable threshold (`k_floor_threshold`, default $-3$). Only $\log k$ values below $\tau$ are penalized.

### Scalar Correction

Even without the normalization regularization, a post-hoc **scalar correction** is applied when evaluating rate constants. The MLP$_{\text{sub}}$ scale factor $\alpha$ is measured by evaluating MLP$_{\text{sub}}$ at the reference point:

$$
\alpha = \|\text{MLP}_{\text{sub}}(c{=}1, |s|{=}1)\|
$$

With multiplicative aggregation, each reaction rate absorbs $\alpha^{n_j}$ where $n_j$ is the number of substrates. The corrected rate constants are:

$$
\log_{10} k_j^{\text{corrected}} = \log_{10} k_j^{\text{learned}} + n_j \cdot \log_{10} \alpha
$$

The reported `rate_constants_R2` is computed on these corrected values against the identity line $y=x$.

### Summary of Hyperparameters

| Parameter | Config key | Description | Typical range |
|-----------|-----------|-------------|---------------|
| Learning rate $k$ | `learning_rate_k` | Rate constants update speed | 1E-4 to 1E-2 |
| Learning rate node | `learning_rate_node` | MLP$_{\text{node}}$ update speed | 1E-4 to 1E-2 |
| Learning rate sub | `learning_rate_sub` | MLP$_{\text{sub}}$ update speed | 1E-4 to 1E-2 |
| Batch size | `batch_size` | Time frames per gradient step | 4 to 32 |
| Training iterations | `data_augmentation_loop` | Multiplier for iterations per epoch | 100 to 5000 |
| Epochs | `n_epochs` | Number of training epochs | 1 to 5 |
| Time step | `time_step` | Steps per gradient update (1 = single-step, >1 = recurrent rollout) | 1 to 8 |
| Recurrent training | `recurrent_training` | Enable multi-step rollout training | true / false |
| MLP$_{\text{sub}}$ monotonicity | `coeff_MLP_sub_diff` | Penalize non-increasing MLP$_{\text{sub}}$ | 0 to 500 |
| MLP$_{\text{node}}$ L1 | `coeff_MLP_node_L1` | Penalize large homeostasis output | 0 to 10 |
| MLP$_{\text{sub}}$ normalization | `coeff_MLP_sub_norm` | Pin MLP$_{\text{sub}}(c{=}1, |s|{=}1)$ to 1 | 0 to 10 |
| Rate constant floor | `coeff_k_floor` | Penalize $\log k$ below threshold | 0 to 10 |
| Floor threshold | `k_floor_threshold` | Threshold for k floor penalty | $-3$ (default) |
| MLP$_{\text{sub}}$ hidden dim | `hidden_dim_sub` | Hidden layer width for substrate MLP | 16 to 128 |
| MLP$_{\text{sub}}$ layers | `n_layers_sub` | Number of layers for substrate MLP | 2 to 5 |
| MLP$_{\text{node}}$ hidden dim | `hidden_dim_node` | Hidden layer width for homeostasis MLP | 16 to 128 |
| MLP$_{\text{node}}$ layers | `n_layers_node` | Number of layers for homeostasis MLP | 2 to 5 |

---

## Metrics

| Metric | Description | Good value |
|--------|-------------|------------|
| `rate_constants_R2` | R² between learned and true rate constants $k$ (after scalar correction, excluding outliers) | > 0.9 |
| `n_outliers` | Number of reactions with $|\Delta \log_{10} k| > 0.3$ (factor of 2 error in $k$-space) | < 25 |
| `slope` | Slope of linear fit between learned and true $\log_{10} k$. Slope $< 1$ means the learned range is compressed. | $\approx 1.0$ |
| `test_R2` | R² on held-out test frames (rollout prediction) | > 0.9 |
| `test_pearson` | Pearson correlation on test frames | > 0.95 |
| `final_loss` | Final prediction loss (MSE on $dc/dt$) | Lower is better |
| `alpha` | MLP$_{\text{sub}}$ scale factor $\alpha = \|\text{MLP}_{\text{sub}}(c{=}1, |s|{=}1)\|$. Ideal value is 1.0 — indicates MLP$_{\text{sub}}$ has learned the correct scale. Deviations from 1 indicate residual scale ambiguity that the scalar correction must compensate for. | $\approx 1.0$ |

### Diagnostic Interpretation

The relationship between `rate_constants_R2` and `test_pearson` diagnoses whether the model found the true rate constants or a degenerate solution:

| test_pearson | $R^2$ | Diagnosis |
|:------------:|:-----:|-----------|
| > 0.95 | > 0.9 | **Healthy** — good dynamics from correct $k$ |
| > 0.95 | 0.3--0.9 | **Degenerate** — good dynamics from wrong $k$ (MLPs compensate) |
| > 0.95 | < 0.3 | **Severely degenerate** — MLPs absorb all dynamics |
| < 0.5 | > 0.9 | **Good $k$, poor rollout** — rate constants correct but MLP errors compound during integration |
| < 0.5 | < 0.5 | **Failed** — both dynamics and $k$ poor |

The current oscillatory regime experiments show the **good $k$, poor rollout** pattern: $R^2 \approx 0.93$ but Pearson $\approx 0.05$. The MLP$_{\text{sub}}$ scale compression ($\alpha \approx 0.4$) and MLP$_{\text{node}}$ failure (flat at zero) produce errors that accumulate during multi-step rollout, even though the rate constants themselves are well-recovered.

---

## UCB Tree Search

The LLM selects parent configurations to mutate using an **Upper Confidence Bound** (UCB) strategy that balances exploitation of high-performing branches with exploration of under-visited regions:

$$
\text{UCB}(i) = \bar{X}_i + c \cdot \sqrt{\frac{\ln N}{n_i}}
$$

where $\bar{X}_i$ is the mean reward of node $i$, $N$ is the total number of visits, $n_i$ is the number of visits to node $i$, and $c$ is the exploration constant.

**4 parallel slots** run per batch with diversified roles:

| Slot | Role | Description |
| ---- | ---- | ----------- |
| 0 | **exploit** | Highest UCB node, conservative mutation |
| 1 | **exploit** | 2nd highest UCB, or same parent different param |
| 2 | **explore** | Under-visited node, or new parameter dimension |
| 3 | **principle-test** | Test or challenge one established principle from memory |
