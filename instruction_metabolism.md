# Metabolism Stoichiometry Recovery — Training Parameter Optimization

## Goal

Find GNN training hyperparameters **and GNN code-level parameters** that recover the **stoichiometric matrix S** from metabolic concentration dynamics generated by PDE_M1 (100 metabolites, 256 reactions, max 4 metabolites per reaction).

**This is a fixed-regime exploration**: simulation parameters are FROZEN. GNN training parameters AND GNN code may be changed. There are NO block boundary simulation changes. **Code changes to GNN architecture/training are encouraged** if config-only sweeps plateau.

## Metabolism Regime Characteristics

The system models a metabolic network as a bipartite graph (metabolites <-> reactions) governed by stoichiometric kinetics:

```
dc_i/dt = sum_j S_ij * v_j(c)
```

- **c** = metabolite concentrations (100 metabolites)
- **S** = stoichiometric matrix (100 x 256), sparse, entries in {-2, -1, 0, +1, +2}
  - Negative entries: substrates (consumed by reaction)
  - Positive entries: products (produced by reaction)
  - Most entries are zero (each reaction involves 2-6 metabolites out of 100)
- **v_j(c)** = reaction rate for reaction j, computed as `k_j * softplus(rate_func(h_j))`
  - `k_j` = per-reaction rate constant, fixed, log-uniform in [0.001, 0.1]
  - `h_j` = aggregated contribution from substrate concentrations via substrate_func

### Stoichiometric Matrix Structure

- 256 reactions, each with 1-3 substrates and 1-3 products (coefficients 1 or 2)
- Total ~600 substrate edges (negative S entries) and ~500 product edges (positive S entries)
- S is very sparse: ~1100 non-zero entries out of 25,600 possible (100 x 256)
- Each column (reaction) has exactly 2-6 non-zero entries
- S has no special low-rank structure — it is a random sparse integer matrix

### Expected Challenges

- **Discrete target**: True S values are integers {-2,-1,+1,+2}, but the model learns continuous values. Post-training rounding may be needed for evaluation
- **Sign recovery**: Getting the correct sign (substrate vs product) for each edge is critical — wrong sign flips the contribution direction
- **Sparsity**: S is very sparse (~4% fill), so L1 regularization should help zero out incorrect entries
- **Function compensation**: As with neural signal recovery, substrate_func and rate_func can compensate for incorrect S coefficients
- **Rate constant interaction**: The model learns both S (stoichiometry) and the MLPs (rate functions). The rate constants k_j are also learnable — the model may find different k/S combinations that produce similar dynamics
- **Scale ambiguity**: scaling S by alpha and v by 1/alpha gives the same dx/dt. Regularization must break this symmetry

## Training Loss

```
L = L_pred + coeff_S_L1 * ||sto_all||_1 + coeff_S_L2 * ||sto_all||_2 + coeff_mass * sum_j (sum_i S_ij)^2
```

- `L_pred`: MSE between predicted dc/dt and true dc/dt
- `coeff_S_L1`: L1 on sto_all (promotes sparsity = correct zero pattern in S)
- `coeff_S_L2`: L2 on sto_all (weight decay on stoichiometric coefficients)
- `coeff_mass_conservation`: penalizes non-zero column sums of S (mass balance per reaction)

## Metabolic Network Architecture (Metabolism_Propagation)

The training model mirrors the PDE_M1 generator but with **learnable stoichiometric coefficients**:

```
dc_i/dt = sum_j  sto_all[e] * v[rxn_all[e]]     for all edges e where met_all[e] == i
```

Where:
- `sto_all` (nn.Parameter): signed stoichiometric coefficients for all edges (~1100)
- `v[j] = k_j * softplus(rate_func(h_j))`: non-negative reaction rate
- `h_j = sum over substrate edges of reaction j: substrate_func([concentration, |sto_all[sub_to_all]|])`
- `k_j = 10^(log_k[j])`: per-reaction rate constant (learnable)
- `sub_to_all` (buffer): maps substrate edge index to its position in sto_all

### Bipartite Graph Structure (fixed buffers)

```
met_sub[e], rxn_sub[e]  — substrate edges (metabolite -> reaction)
met_all[e], rxn_all[e]  — all edges (substrate + product)
```

The graph structure is fixed (which metabolites participate in which reactions). Only the stoichiometric coefficients on edges are learnable.

### Generator (PDE_M1) — Ground Truth

```
dc_i/dt = sum_j  S_true[i,j] * v_j(c)
```

- S_true: fixed stoichiometric matrix (100 x 256), entries in {-2, -1, 0, +1, +2}
- v_j: reaction rates from random MLPs with fixed rate constants k_j
- No external input (PDE_M1 = pure stoichiometric kinetics)
