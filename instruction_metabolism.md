# Metabolism Stoichiometry Recovery — Training Parameter Optimization

## Goal

Find GNN training hyperparameters **and GNN code-level parameters** that recover the **stoichiometric matrix S** from metabolic concentration dynamics generated by PDE_M1 (100 metabolites, 256 reactions, max 4 metabolites per reaction).

**This is a fixed-regime exploration**: simulation parameters are FROZEN. GNN training parameters AND GNN code may be changed. There are NO block boundary simulation changes. **Code changes to GNN architecture/training are encouraged** if config-only sweeps plateau.

## Metabolism Regime Characteristics

The system models a metabolic network as a bipartite graph (metabolites <-> reactions) governed by stoichiometric kinetics:

```
dc_i/dt = sum_j S_ij * v_j(c)
```

- **c** = metabolite concentrations (100 metabolites)
- **S** = stoichiometric matrix (100 x 256), sparse, entries in {-2, -1, 0, +1, +2}
  - Negative entries: substrates (consumed by reaction)
  - Positive entries: products (produced by reaction)
  - Most entries are zero (each reaction involves 2-6 metabolites out of 100)
- **v_j(c)** = reaction rate for reaction j, computed as `k_j * softplus(rate_func(h_j))`
  - `k_j` = per-reaction rate constant, fixed, log-uniform in [0.001, 0.1]
  - `h_j` = aggregated contribution from substrate concentrations via substrate_func

### Stoichiometric Matrix Structure

- 256 reactions, each with 1-3 substrates and 1-3 products (coefficients 1 or 2)
- Total ~600 substrate edges (negative S entries) and ~500 product edges (positive S entries)
- S is very sparse: ~1100 non-zero entries out of 25,600 possible (100 x 256)
- Each column (reaction) has exactly 2-6 non-zero entries
- S has no special low-rank structure — it is a random sparse integer matrix

### Expected Challenges

- **Discrete target**: True S values are integers {-2,-1,+1,+2}, but the model learns continuous values. Post-training rounding may be needed for evaluation
- **Sign recovery**: Getting the correct sign (substrate vs product) for each edge is critical — wrong sign flips the contribution direction
- **Sparsity**: S is very sparse (~4% fill), so L1 regularization should help zero out incorrect entries
- **Function compensation**: As with neural signal recovery, substrate_func and rate_func can compensate for incorrect S coefficients
- **Rate constant interaction**: The model learns both S (stoichiometry) and the MLPs (rate functions). The rate constants k_j are also learnable — the model may find different k/S combinations that produce similar dynamics
- **Scale ambiguity**: scaling S by alpha and v by 1/alpha gives the same dx/dt. Regularization must break this symmetry

## Training Loss

```
L = L_pred + coeff_S_L1 * ||sto_all||_1 + coeff_S_integer * mean(sin²(π * sto_all)) + coeff_mass * sum_j (sum_i S_ij)^2
```

- `L_pred`: MSE between predicted dc/dt and true dc/dt
- `coeff_S_L1`: L1 on sto_all (promotes sparsity = correct zero pattern in S)
- `coeff_S_integer`: sin²(π·S) penalty on sto_all (encourages integer-valued coefficients)
- `coeff_mass_conservation`: penalizes non-zero column sums of S (mass balance per reaction)

## Metabolic Network Architecture (Metabolism_Propagation)

The training model mirrors the PDE_M1 generator but with **learnable stoichiometric coefficients**:

```
dc_i/dt = sum_j  sto_all[e] * v[rxn_all[e]]     for all edges e where met_all[e] == i
```

Where:
- `sto_all` (nn.Parameter): signed stoichiometric coefficients for all edges (~1100)
- `v[j] = k_j * softplus(rate_func(h_j))`: non-negative reaction rate
- `h_j = sum over substrate edges of reaction j: substrate_func([concentration, |sto_all[sub_to_all]|])`
- `k_j = 10^(log_k[j])`: per-reaction rate constant (learnable)
- `sub_to_all` (buffer): maps substrate edge index to its position in sto_all

### Bipartite Graph Structure (fixed buffers)

```
met_sub[e], rxn_sub[e]  — substrate edges (metabolite -> reaction)
met_all[e], rxn_all[e]  — all edges (substrate + product)
```

The graph structure is fixed (which metabolites participate in which reactions). Only the stoichiometric coefficients on edges are learnable.

### Generator (PDE_M1) — Ground Truth

```
dc_i/dt = sum_j  S_true[i,j] * v_j(c)
```

- S_true: fixed stoichiometric matrix (100 x 256), entries in {-2, -1, 0, +1, +2}
- v_j: reaction rates from random MLPs with fixed rate constants k_j
- No external input (PDE_M1 = pure stoichiometric kinetics)

## Training Parameters Reference

All parameters below are in the `training:` section of the YAML config. **Simulation parameters are FROZEN — do not change them.**

### Learning Rates

| Parameter | Config key | Description | Typical range |
|-----------|-----------|-------------|---------------|
| `lr` | `learning_rate_start` | Learning rate for MLP parameters (substrate_func, rate_func, log_k) | 1E-4 to 1E-2 |
| `lr_embedding` | `learning_rate_embedding_start` | Learning rate for embeddings (if used) | 1E-5 to 1E-3 |
| `lr_S` | `learning_rate_S_start` | Separate learning rate for stoichiometric coefficients (sto_all). If 0, sto_all uses the same lr as MLPs | 1E-4 to 1E-2 |

**Key insight**: `lr_S` controls how fast the stoichiometric matrix is learned relative to the MLPs. Too fast and S overfits before the MLPs learn good rate functions; too slow and S barely moves.

### Regularization Coefficients

| Parameter | Config key | Loss term | Effect | Typical range |
|-----------|-----------|-----------|--------|---------------|
| `coeff_S_L1` | `coeff_S_L1` | `coeff * \|\|sto_all\|\|_1` | Promotes sparsity — pushes small coefficients toward zero. Helps recover the correct zero pattern in S (~96% of entries are zero) | 0 to 1E-2 |
| `coeff_S_integer` | `coeff_S_integer` | `coeff * mean(sin²(π * sto_all))` | Encourages integer values — penalty is zero at every integer (0, ±1, ±2, ...) and maximal at half-integers. True S entries are in {-2, -1, +1, +2} | 0 to 1E-1 |
| `coeff_mass` | `coeff_mass_conservation` | `coeff * mean_j(sum_i S_ij)²` | Enforces mass conservation per reaction — the column sum of S should be zero (substrates consumed = products produced). This is true by construction in the ground truth | 0 to 1.0 |

**Regularization strategy**: These three regularizations encode different prior knowledge about S:
- **L1** knows S is sparse (most entries are zero)
- **Integer** knows S has integer entries
- **Mass conservation** knows each reaction conserves mass

All three are valid physics priors. The challenge is balancing them against the prediction loss — too strong and the model can't fit the dynamics; too weak and S drifts to non-physical values.

### Training Schedule

| Parameter | Config key | Description | Typical range |
|-----------|-----------|-------------|---------------|
| `n_epochs` | `n_epochs` | Number of training epochs | 1 to 20 |
| `batch_size` | `batch_size` | Number of time frames per gradient step | 4 to 32 |
| `data_augmentation_loop` | `data_augmentation_loop` | Multiplier for iterations per epoch. Total iterations ≈ n_frames * data_augmentation_loop / batch_size * 0.2 | 100 to 10000 |
| `n_runs` | `n_runs` | Number of independent simulation runs used for training | 1 to 5 |
| `seed` | `seed` | Random seed for training reproducibility | any integer |
| `time_step` | `time_step` | Number of integration steps for recurrent training (requires `recurrent_training=True`) | 1 to 10 |

**Key insight**: `data_augmentation_loop` controls total training iterations. Higher values = more gradient steps per epoch. With `n_epochs=1` and `data_augmentation_loop=1000`, training does ~72,000 gradient steps (2880 frames * 1000 / 8 batch * 0.2).

### Recurrent Training (optional)

When `recurrent_training=True` and `time_step=T`, the model is trained to predict T steps ahead using its own predictions (autoregressive rollout):

1. Sample frame k
2. Target = actual concentration at frame `k + time_step` (not derivative)
3. First step: `pred_c = c + delta_t * model(c) + noise`
4. Steps 2..T: feed `pred_c` back into model, accumulate Euler steps
5. Loss = `||pred_c - c_true|| / (delta_t * time_step)` (backprop through all T steps)

**Key parameters:**

| Parameter | Config key | Description | Range |
|-----------|-----------|-------------|-------|
| `recurrent_training` | `recurrent_training` | Enable multi-step rollout | True/False |
| `time_step` | `time_step` | Rollout depth (1 = single-step, no recurrence) | 1, 4, 8, 16 |
| `noise_recurrent_level` | `noise_recurrent_level` | Noise per rollout step (regularization) | 0 to 0.1 |

**Single-step vs Recurrent targets:**
- Single-step (`time_step=1`): target = `y_list[k]` (true dc/dt at frame k)
- Recurrent (`time_step>1`): target = `x_list[k + time_step, :, 3]` (true concentration at frame k + time_step)

**Expected trade-offs:**

| time_step | Stoichiometry R² | Rollout Stability | Training Cost |
|-----------|------------------|-------------------|---------------|
| 1 | Best | May overfit short-term | Low |
| 4 | Good | Better generalization | Moderate |
| 8-16 | Moderate | Good long-term | High |

**Guidance:**

- Start with `recurrent_training=False` (default) to establish baseline
- Enable at later epochs: set `recurrent_training=True` + `time_step=4` as first test
- `noise_recurrent_level=0.01-0.05` helps prevent rollout instability
- Higher `time_step` costs proportionally more compute — reduce `data_augmentation_loop` to compensate

### Two-Phase Training (optional)

| Parameter | Config key | Description |
|-----------|-----------|-------------|
| `n_epochs_init` | `n_epochs_init` | Number of initial epochs with different L1 coefficient. Default 0 (disabled) |
| `first_coeff_L1` | `first_coeff_L1` | L1 coefficient during initial phase. Default same as `coeff_S_L1` |

This allows a warm-up strategy: e.g., train with no L1 first (let S find approximate values), then turn on L1 to sparsify.

### GNN Architecture (graph_model section)

| Parameter | Config key | Description | Default |
|-----------|-----------|-------------|---------|
| `hidden_dim` | `hidden_dim` | Hidden layer width for substrate_func and rate_func MLPs | 32 |
| `output_size` | `output_size` | Output dimension of substrate_func = input dimension of rate_func (message dimension) | 16 |
| `n_layers` | `n_layers` | Number of layers in MLPs | 3 |

**Note**: substrate_func is `MLP([2, hidden_dim, output_size])` with Tanh activation. rate_func is `MLP([output_size, hidden_dim, 1])` with Tanh + softplus output.

## Parameter Sweep Guidelines

### Suggested exploration order

1. **Learning rates first**: Sweep `lr` and `lr_S` — these have the largest impact on convergence
2. **Regularization balance**: Once a good lr is found, sweep regularization coefficients
3. **Training duration**: Increase `data_augmentation_loop` or `n_epochs` if the model is still improving at the end of training
4. **Architecture**: Try different `hidden_dim` and `output_size` if loss plateaus

### Interactions to watch for

- **lr_S vs regularization**: High `lr_S` + strong regularization can cause oscillations. Lower `lr_S` with stronger regularization is more stable
- **L1 vs integer**: L1 pushes toward zero, integer pushes toward nearest integer. For non-zero entries, these cooperate (integer wants ±1, ±2). For zero entries, only L1 helps
- **mass conservation vs L1**: Mass conservation constrains column sums without affecting individual coefficient magnitudes. It's orthogonal to L1 and should generally be used together
- **Scale ambiguity**: Without regularization, the model can learn S*α and v/α for any α. L1 and integer regularization both break this symmetry by preferring small integer S values

## Training Metrics

The following metrics are written to `analysis.log` at the end of training:

| Metric | Description | Good value |
|--------|-------------|------------|
| `final_loss` | Final prediction loss (MSE on dc/dt) | Lower is better |
| `stoichiometry_R2` | R² between learned and true stoichiometric coefficients | > 0.9 |
| `substrate_func_R2` | R² between learned and GT substrate_func outputs on test grid | > 0.8 |
| `substrate_func_corr` | Pearson correlation for substrate_func | > 0.9 |
| `rate_func_R2` | R² between learned and GT rate_func outputs on test grid | > 0.8 |
| `rate_func_corr` | Pearson correlation for rate_func | > 0.9 |

### Function comparison methodology

The `substrate_func` and `rate_func` are compared by:

1. **substrate_func**: Evaluated on a 50×50 grid of inputs `[concentration, |stoich|]` where concentration ∈ [0, 10] and |stoich| ∈ [0, 3]. R² and correlation computed between GT and learned outputs.

2. **rate_func**: Evaluated on 100 random message vectors (sampled from N(0, 2)). R² and correlation computed between `softplus(GT_rate_func(h))` and `softplus(learned_rate_func(h))`.

### Interpretation

- **High stoichiometry_R2 + low func_R2**: Model learned correct S but compensated with different rate functions. May still give good dynamics.
- **Low stoichiometry_R2 + high func_R2**: Model learned similar functions but wrong S. Likely poor dynamics.
- **All high**: Ideal — model recovered both the stoichiometric matrix and the reaction kinetics.
