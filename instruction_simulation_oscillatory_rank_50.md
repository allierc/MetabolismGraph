# Metabolism Rate Constants Recovery — S Given Mode (High-Rank Regime)

## Regime Note

This config uses a **narrower rate constant range** (`log_k` in [-2.0, -1.0] instead of [-2.5, -1.0]) which produces **activity rank ~50** (vs ~24 with the original range). All reactions are actively contributing to the dynamics, making this a harder inverse problem with richer dynamics.

Previous exploration on the original oscillatory config found optimal training params: `lr_k=0.005`, `lr_node=0.001`, `lr_sub=0.0005`, `coeff_k_center=5.0`, `coeff_MLP_sub_diff=3-5`, `data_augmentation_loop=2000`. These are used as starting values but may need re-tuning for the higher-rank regime.

## Goal

Find GNN training hyperparameters that recover the **rate constants k** from metabolic concentration dynamics generated by PDE_M1. The **stoichiometric matrix S is frozen** (given from ground truth). The model learns:

- **Rate constants k_j** (256 learnable log-scale parameters)
- **MLP_sub** (substrate function): computes c^s contribution
- **MLP_node** (homeostasis function): computes per-metabolite homeostatic regulation

**Primary optimization target**: `rate_constants_R2` — R² between learned and true rate constants.

## Metabolism Regime Characteristics

The system models a metabolic network as a bipartite graph (metabolites <-> reactions):

```
dc_i/dt = homeostasis_i(c_i) + sum_j S_ij * v_j(c)
```

- **c** = metabolite concentrations (100 metabolites, 2 types)
- **S** = stoichiometric matrix (100 x 256), **FROZEN** from ground truth
- **v_j(c)** = reaction rate for reaction j: `v_j = k_j * prod_i(c_i^|S_ij|)` for substrates
- **k_j** = per-reaction rate constant (learnable), true values log-uniform in config range
- **homeostasis_i** = per-metabolite homeostatic term: `-λ_type(i) * (c_i - c_baseline_type(i))`

### Key Differences from S Learning Mode

| Aspect | S Learning Mode | S Given Mode (this) |
|--------|-----------------|---------------------|
| S matrix | Learnable | Frozen from GT |
| Primary metric | stoichiometry_R2 | rate_constants_R2 |
| Learning rates | lr_S, lr | lr_k, lr_node, lr_sub |
| Regularization | coeff_S_L1, coeff_S_integer, coeff_mass | None (S is given) |
| Challenge | Integer recovery, sparsity | Function learning, scale |

### Expected Challenges

- **Scale ambiguity**: The model learns k in log space. Multiple (k, MLP_sub, MLP_node) combinations can produce similar dynamics
- **Function compensation**: MLP_sub can compensate for incorrect k by reshaping the concentration-to-rate mapping
- **Homeostasis vs reaction rates**: MLP_node must learn the correct per-type homeostatic regulation without absorbing reaction rate information
- **Learning rate balance**: lr_k, lr_node, lr_sub must be balanced — too fast on one component and others can't catch up

## Training Parameters Reference

All parameters below are in the `training:` section of the YAML config. **Simulation parameters are FROZEN — do not change them.**

### Learning Rates

| Parameter | Config key | Description | Typical range |
|-----------|-----------|-------------|---------------|
| `lr_k` | `learning_rate_k` | Learning rate for rate constants (log_k) | 1E-4 to 1E-2 |
| `lr_node` | `learning_rate_node` | Learning rate for MLP_node (homeostasis function) | 1E-4 to 1E-2 |
| `lr_sub` | `learning_rate_sub` | Learning rate for MLP_sub (substrate function) | 1E-4 to 1E-2 |
| `lr` | `learning_rate_start` | Fallback learning rate for other parameters | 1E-4 to 1E-2 |

**Key insight**: The three learning rates control how fast each component is learned:
- **lr_k too high**: k values overshoot, oscillate, or converge to wrong values
- **lr_k too low**: k barely moves, MLPs compensate
- **lr_node/lr_sub imbalance**: One function absorbs capacity meant for the other

### Training Schedule

| Parameter | Config key | Description | Typical range |
|-----------|-----------|-------------|---------------|
| `n_epochs` | `n_epochs` | Number of training epochs | 1 to 10 |
| `batch_size` | `batch_size` | Number of time frames per gradient step | 4 to 32 |
| `data_augmentation_loop` | `data_augmentation_loop` | Multiplier for iterations per epoch | 100 to 2000 |
| `seed` | `seed` | Random seed for training reproducibility | any integer |

**Key insight**: `data_augmentation_loop` controls total training iterations. With `n_epochs=1` and `data_augmentation_loop=1000`, training does ~72,000 gradient steps (2880 frames * 1000 / 8 batch * 0.2). If results are still improving at the end of training, increase `data_augmentation_loop` up to 2000 for longer convergence.

### MLP Regularization

| Parameter | Config key | Description | Typical range |
|-----------|-----------|-------------|---------------|
| `coeff_MLP_sub_diff` | `coeff_MLP_sub_diff` | Monotonicity constraint for MLP_sub — penalizes decreasing output as concentration increases. MLP_sub learns c^s which should be increasing. Higher values enforce stronger monotonicity | 0 to 500 |
| `coeff_MLP_node_L1` | `coeff_MLP_node_L1` | L1 penalty on MLP_node output magnitude — keeps homeostasis values small relative to reaction terms. Prevents MLP_node from dominating the dynamics | 0 to 10 |
| `coeff_k_center` | `coeff_k_center` | Penalizes `mean(log_k)` deviating from the GT range center. Breaks the scale ambiguity between k and MLP_sub: without this, MLP_sub can absorb a global scale factor and shift all k values | 0 to 10 |

**Key insight**: `coeff_MLP_sub_diff` prevents MLP_sub from learning non-physical functions. Without this constraint, MLP_sub can develop spurious local minima that don't match the true c^s power law behavior.

**Key insight**: MLP_node is initialized to zero output, so homeostasis starts inactive. `coeff_MLP_node_L1` + reduced `learning_rate_node` keep it from growing too large and overwhelming the reaction terms (k * MLP_sub). The true homeostatic lambdas are small (0.001–0.002), so MLP_node output should stay small.

**Key insight**: `coeff_k_center` addresses the scale ambiguity: $k \cdot \text{MLP}_{\text{sub}}$ is invariant under $k \to \alpha k$, $\text{MLP}_{\text{sub}} \to \text{MLP}_{\text{sub}} / \alpha$. The target is the midpoint of `[log_k_min, log_k_max]` from the simulation config.

### Frozen Parameters (DO NOT CHANGE)

| Parameter | Config key | Value | Reason |
|-----------|-----------|-------|--------|
| `freeze_stoichiometry` | `freeze_stoichiometry` | true | S is given from GT |
| `lr_S` | `learning_rate_S_start` | 0.0 | S is frozen |
| `coeff_S_L1` | `coeff_S_L1` | 0.0 | No S regularization |
| `coeff_S_integer` | `coeff_S_integer` | 0.0 | No S regularization |
| `coeff_mass` | `coeff_mass_conservation` | 0.0 | No S regularization |

## Training Metrics

The following metrics are written to `analysis.log` at the end of training:

| Metric | Description | Good value |
|--------|-------------|------------|
| `final_loss` | Final prediction loss (MSE on dc/dt) | Lower is better |
| `rate_constants_R2` | R² between learned and true rate constants k | > 0.9 |
| `rate_constants_R2_shifted` | R² after removing mean offset in log-space — measures correlation independent of global scale | > 0.9 |
| `test_R2` | R² on held-out test frames | > 0.9 |
| `test_pearson` | Pearson correlation on test frames | > 0.95 |

### Interpretation

- **High rate_constants_R2**: Model recovered the true reaction rate constants
- **High R2_shifted + low R2**: Scale ambiguity — k values are correlated but globally shifted. Increase `coeff_k_center` to anchor the scale
- **High test_R2 + low rate_constants_R2**: Model found alternative k values that produce similar dynamics (degeneracy)
- **Low both**: Training failed — try different learning rates

## Iteration Workflow

### Step 1: Read Working Memory

Read `{config}_memory.md` to recall:
- Established principles about lr_k, lr_node, lr_sub interactions
- Previous iteration findings
- Current block progress

### Step 2: Analyze Current Results

**Metrics from `analysis.log`:**
- `rate_constants_R2`: Primary metric — R² of learned vs true k values
- `rate_constants_R2_shifted`: R² after removing mean offset — measures correlation independent of global scale
- `test_R2`: Dynamics prediction quality
- `test_pearson`: Correlation on test frames
- `final_loss`: Training loss

**Classification:**
- **Converged**: rate_constants_R2 > 0.9
- **Partial**: rate_constants_R2 0.1-0.9
- **Failed**: rate_constants_R2 < 0.1

**Degeneracy Detection:**

Compute the **degeneracy gap** = `test_pearson - rate_constants_R2`:

| test_pearson | rate_constants_R2 | Gap | Diagnosis |
|:------------:|:-----------------:|:---:|-----------|
| > 0.95 | > 0.9 | < 0.1 | **Healthy** — good dynamics from correct k |
| > 0.95 | 0.3–0.9 | 0.1–0.7 | **Degenerate** — good dynamics from wrong k |
| > 0.95 | < 0.3 | > 0.7 | **Severely degenerate** — MLPs compensating |
| < 0.5 | < 0.5 | ~0 | **Failed** — both dynamics and k poor |

**Visual Analysis:**

Examine the **last** (highest iteration number) plot in each folder under `{log_dir}/tmp_training/`:
- `function/substrate_func/MLP_sub_*.png`: Should show c^1 and c^2 curves matching ground truth (dashed lines). Good: learned curves overlap dashed GT. Bad: curves diverge or wrong shape.
- `function/rate_func/MLP_node_*.png`: Should show per-type linear homeostasis with small magnitude. Good: near-linear, small values (~0.001 scale). Bad: large flat plateaus, values >> 1, or non-linear shapes.
- `rate/rate_constants_*.png`: Scatter plot of learned vs true k (should be diagonal)

**IMPORTANT**: Always read the last plot file in `function/substrate_func/` and `function/rate_func/` to visually assess how well the functions are learned. Sort by filename to find the latest iteration. Include your visual assessment in the log.

**UCB scores from `ucb_scores.txt`:**
- Provides computed UCB scores for all exploration nodes
- At block boundaries, the UCB file will be empty — use `parent=root`

### Step 3: Write Outputs

Append to Full Log (`{config}_analysis.md`) and Working Memory (`{config}_memory.md`):

**Log Form:**

```
## Iter N: [converged/partial/failed]
Node: id=N, parent=P
Mode/Strategy: [exploit/explore/boundary]
Config: seed=S, lr_k=X, lr_node=Y, lr_sub=Z, batch_size=B, n_epochs=E, data_augmentation_loop=A, coeff_MLP_node_L1=L, coeff_k_center=K
Metrics: rate_constants_R2=C, rate_constants_R2_shifted=D, test_R2=A, test_pearson=B, final_loss=E
Visual: MLP_sub=[good/partial/bad: brief description], MLP_node=[good/partial/bad: brief description]
Mutation: [param]: [old] -> [new]
Parent rule: [one line]
Observation: [one line]
Next: parent=P
```

**CRITICAL**: The `Visual:` line must describe what you see in the last MLP_sub and MLP_node plots. Example: `Visual: MLP_sub=good: c^1 and c^2 match GT, MLP_node=bad: values ~5, flat plateaus`

**CRITICAL**: The `Mutation:` line is parsed by the UCB tree builder. Always include the exact parameter change.

**CRITICAL**: The `Next: parent=P` line selects the parent for the next iteration.

### Step 4: Parent Selection (UCB)

1. Read `ucb_scores.txt`
2. If empty → `parent=root`
3. Otherwise → select node with **highest UCB** as parent

### Step 5: Propose Next Mutation

**Strategy selection:**

| Condition | Strategy | Action |
|-----------|----------|--------|
| Default | **exploit** | Highest UCB node, conservative mutation |
| 3+ consecutive R² ≥ 0.9 | **boundary-probe** | Extreme parameter to find boundary |
| All partial for 4+ iters | **explore** | Try different parameter dimension |
| Degeneracy gap > 0.3 for 3+ iters | **degeneracy-break** | Increase lr_k, decrease lr_sub/lr_node |

**Parameter exploration order:**

1. **lr_k first**: This has the largest impact on rate_constants_R2
2. **lr_node/lr_sub balance**: Once lr_k is good, tune the MLP learning rates
3. **Training duration**: Increase data_augmentation_loop if still improving

**Typical interactions:**

- **lr_k ↑ + lr_sub ↓**: Helps k learn faster without MLP compensation
- **lr_node ↑**: May help homeostasis learning but can destabilize
- **batch_size ↑**: Smoother gradients but fewer updates per epoch

### Step 6: Edit Config

Edit the config YAML file with the proposed mutation.

**DO NOT change simulation parameters** — this is a fixed-regime exploration.

## File Structure

You maintain **TWO** files:

### 1. Full Log (append-only)

**File**: `{config}_analysis.md`
- Append every iteration's full log entry
- **Never read this file** — it's for human record only

### 2. Working Memory

**File**: `{config}_memory.md`
- **READ at start of each iteration**
- **UPDATE at end of each iteration**
- Contains: established principles + current block iterations
- Fixed size (~300 lines max)

## Loss Figure

The training progress is visualized in `{log_dir}/tmp_training/loss.tif`:

| Color | Component | Description |
|-------|-----------|-------------|
| **blue** (thick) | `loss` | Prediction loss (MSE on dc/dt) |
| **cyan** | `regul_total` | Total regularization (should be ~0 in S given mode) |

**Monitor**: Blue curve should decrease steadily. If it plateaus early, increase lr_k or data_augmentation_loop.

## Parameter Sweep Guidelines

### Suggested initial spread

- Slot 0: baseline (lr_k=1E-3, lr_node=1E-3, lr_sub=1E-3)
- Slot 1: vary lr_k (5E-4 or 2E-3)
- Slot 2: vary lr_node (5E-4 or 2E-3)
- Slot 3: vary lr_sub (5E-4 or 2E-3)

### Boundary conditions to probe

- **lr_k upper cliff**: Where does rate_constants_R2 collapse?
- **lr balance**: What ratio of lr_k : lr_node : lr_sub works best?
- **Training duration**: Diminishing returns on data_augmentation_loop?
