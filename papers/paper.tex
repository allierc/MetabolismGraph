\documentclass[10pt,twocolumn]{article}
\usepackage[top=1in,bottom=1in,lmargin=0.75in,rmargin=0.75in]{geometry}
\usepackage[labelfont=bf]{caption}

\setlength{\columnsep}{2.5em}



% TODO: check formatting!!! replace NeurIPS format by other
% Font size must be at least 12-point, figure captions and references can be typeset using a smaller font size (10 pt). Margins should be at least 0.5". This two-page PDF will be the only document seen by reviewers. (Abstracts exceeding two pages will have additional pages removed). Submissions that do not meet these guidelines may be rejected before review.

\setlength{\columnsep}{2.5em}

\usepackage{mathpazo,helvet,inconsolata,bbding}

\usepackage[utf8]{inputenc}
\usepackage[autolanguage,np]{numprint}

%\usepackage{nopageno}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{textcomp,gensymb}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{xspace}
%\usepackage{relsize}
\usepackage{stackengine}
\usepackage{tabularx}
%\usepackage{pgfplots}
\usepackage{authblk}
%\pgfplotsset{width=10cm,compat=1.9}
%\usepgfplotslibrary{external}
%\tikzexternalize

\usepackage[
style=authoryear-comp,
giveninits=true,
uniquename=init,
citestyle=authoryear,
natbib=true,
%backend=biber
]{biblatex}
\bibliography{ref}

% \usepackage[
% style=numeric,
% sorting=none,
% style=authoryear-icomp,
% giveninits=true,
% uniquename=init,
% citestyle=numeric,
% natbib=true,
% uniquelist=false,
% isbn=false
% ]{biblatex}

\bibliography{ref}
\renewcommand*{\bibfont}{\scriptsize}
\AtEveryBibitem{\clearlist{language}}
\AtEveryBibitem{\clearfield{url}\clearfield{urlyear}}
\AtEveryBibitem{\iffieldundef{doi}{}{\clearfield{url}\clearfield{urlyear}}}
\AtEveryBibitem{\clearfield{month}}
\AtEveryBibitem{\clearlist{publisher}}
\AtEveryBibitem{\clearfield{note}}
\AtEveryBibitem{\clearname{editor}}
\AtEveryBibitem{\clearfield{series}}
\AtEveryBibitem{\clearfield{issn}}
\AtEveryCitekey{\clearfield{issn}}
\renewbibmacro*{url+urldate}{}

\stackMath
\setstackgap{S}{2pt}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argminA}{arg\,min} % Jan Hlavacek
\DeclareMathOperator*{\fr}{\overrightarrow{r}}
\DeclareMathOperator*{\frp}{{\overrightarrow{r}}'}
\DeclareMathOperator*{\fmu}{\overrightarrow{\mu}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Agg}{\sum_{j=1}^{N}}
\DeclareMathOperator*{\Gagg}{\bigodot}
\DeclareMathOperator*{\gnn}{GNN}
\DeclareMathOperator*{\inr}{INR}
\DeclareMathOperator*{\mlp}{MLP}
\DeclareMathOperator*{\relu}{ReLu}
\newcommand{\dt}{\Delta t \:}

\newcommand{\force}{\textrm{Force}}
\newcommand{\loss}{L}

\renewcommand\Vec[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand\Mat[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand\trnsp{\ensuremath{\mathsf{T}}}

\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\hypersetup{%
	colorlinks=true,%
	linkcolor=BrickRed,%
	citecolor=ForestGreen,
	urlcolor=NavyBlue
}
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}
\crefname{figure}{Figure}{Figures}
\crefname{appendixfigure}{Supplementary Figure}{Supplementary Figures}
\crefname{table}{Table}{Tables}
\crefname{appendixtable}{Supplementary Table}{Supplementary Tables}
\creflabelformat{equation}{#2#1#3}

\usepackage{caption}
\captionsetup{font=footnotesize}

\def\etc{etc.\@\xspace}
\def\eg{e.g.\@\xspace}
\def\Etc{Etc.\@\xspace}
\def\Eg{E.g.\@\xspace}

\setlength{\fboxsep}{0pt} % Adjust the padding
\setlength{\fboxrule}{0.25pt} % Adjust the border width
\setlength{\belowcaptionskip}{-2pt}

\newcommand{\ftile}[1]{\raisebox{-\height+2ex}{\fbox{\includegraphics[width=\linewidth]{#1}}}}
%\title{Graph neural networks can uncover the hidden structure and function underlying the observed temporal activity in heterogeneous neural assemblies}
\title{Graph neural networks uncover structure and functions underlying the activity of simulated neural assemblies}
\author{C\'edric Allier}
\author{Larissa Heinrich}
\author{Magdalena Schneider}
\author{Stephan Saalfeld $^\text{\Envelope}$}
\affil{Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA 20147, USA.}
\affil{\href{mailto:saalfelds@janelia.hhmi.org}{\small\Envelope~saalfelds@janelia.hhmi.org}}
\date{\today}


\begin{document}

\twocolumn[ 
\begin{@twocolumnfalse}
\maketitle

% \begin{abstract}
% \noindent{}Graph neural networks trained to predict observable dynamics can be used to decompose the temporal activity of complex dynamical systems into simple, interpretable representations. 
% We demonstrate that they can jointly reveal the connectivity, heterogeneous functions, and external inputs underlying complex temporal activities of simulated neural networks with thousands of neurons.    
% \end{abstract}

\begin{abstract}
\noindent{}Graph neural networks trained to predict observable dynamics can be used to decompose the temporal activity of complex heterogeneous systems into simple, interpretable representations.
Here we apply this framework to simulated neural assemblies with thousands of neurons and demonstrate that it can jointly reveal the connectivity matrix, the neuron types, the signaling functions, and in some cases hidden external stimuli. 
In contrast to existing machine learning approaches such as recurrent neural networks and transformers, which emphasize predictive accuracy but offer limited interpretability, our method provides both reliable forecasts of neural activity and interpretable decomposition of the mechanisms governing large neural assemblies. 
\end{abstract}


\vspace{3em}

\end{@twocolumnfalse} 
]


\section{Introduction}

% The brain implements complex behaviors through coordinated interactions between neurons and glial cells.  These interactions occur via chemical synapses, electrical synapses (gap junctions), ephaptic coupling, diffusive release and reception of neuromodulators, and likely other, yet undiscovered mechanisms.  This complex multi-modal communication network processes sensory inputs, orchestrates precise control of the body, enables memory formation and retrieval, facilitates decision making, executes pattern recognition, allocates attention, and continuously adapts to a changing environment.  The computational operations span a vast range of time-scale from milliseconds to the life-time of the organism.

% The complete structural connectivity, cellular states, and diverse signals cannot be observed jointly in intact, functioning brains.  Parts of this orchestra, however, have become accessible thanks to modern imaging and analysis methods.  Calcium activity for all cells can be now acquired in small model organisms like \emph{Drosophila melanogaster} or larval zebrafish \citep{ahrens_whole-brain_2013}.  But the picture remains incomplete: we can record neural activity patterns but not observe the underlying architecture and diverse signaling mechanisms driving these patterns. 

% To estimate the unknown architecture from the activity patterns, we are studying Graph Neural Networks (GNNs, \citep{gilmer_neural_2017}). The latter have emerged as a promising tool for estimating brain connectivity from e.g. fMRI, PET or EEG measurements \citep{bullmore_complex_2009, bessadok_graph_2023, yuan_graph_2022, yuan_gnn-based_2023}. However, these methods rely on coarse grained measurements and therefore focus on a limited number of regions.  In this study, we explore how GNNs could be used to learn mechanistic models at the level of interconnected single neurons to model the activity from whole brain imaging of small animals \citep{ahrens_whole-brain_2013, lin_imaging_2022, markicevic_emerging_2021}. Retrieving such detailed model from data alone represents a significant challenge, as it requires retrieving a very large connectivity matrix ($>10^6$ weights) together with a set signaling functions. The latter may be neuron-dependent or neuron-to-neuron dependent increasing the complexity of the inverse problem further. At last one must account for the possibility of neuromodulations affecting the neuronal activity. Neuromodulation can take many different form in biological systems. For instance it can be global or local, it can be fixed or evolving over time and, it can be a function of the neuron activity or not. Retrieving neuromodulation indirectly from neuron activity is the most difficult part of the present inverse problem. To date, the inverse problem discussed as above have not yet been tackled. Solutions based on linear models exists to retrieve functional connectivity from data \citep{das_systematic_2020, creamer_bridging_2024}, but do not consider neuronal variability and neuromodulation. The inverse problem is often considered ill-posed owing to the degeneracy of the neural network domain \citep{das_systematic_2020} and, as a result, recent development to retrieve neural dynamics from data rely on connectome-constrained models \citep{mi_connectome-constrained_2021, beiran_prediction_2024} where the connectivity is given. In contrast, we show on simulations that GNNs can simultaneously uncover the connectivity, diverse functions, and external inputs driving the complex temporal activities of thousands of neurons. 

We have shown, that GNNs can decompose complex dynamical systems into interpretable representations \citep{allier_decomposing_2024}.  We jointly learned pairwise interaction functions and local update rules together with a latent representation of the different objects present in the system.  This approach can resolve the complexity arising from heterogeneity in large N-body systems.  Under certain conditions, it is possible to retrieve complex external inputs interacting with the N-body system. Here, we leverage this technique to model the simulated activity of large neuronal assemblies in the presence of external stimuli.  We retrieve the connectivity matrix, neuron types, signaling functions, local update rules, and external stimuli from activity data alone.  Our approach differs from other machine learning techniques used to model and/or forecast neural activity from activity data but are not easily interpretable \citep{lindsay_convolutional_2021, conwell_what_2022, li_v1t_2023, du_towards_2024}. It not only forecasts neural activity but, more importantly, provides an interpretable model of the assembly of thousands of neurons at the single-neuron level. 

\begin{figure}[t]
\centering%
\includegraphics[width=\columnwidth]{Fig1.png}
\caption{The temporal activity of a simulated neural network (\textbf{a}) is converted into graph time-series (\textbf{b}) processed by a message passing GNN (\textbf{c}). Each neuron (node $i$) receives activity signals $x_j$ from connected neurons (node $j$), processed by a transfer function $\psi^*$ and weighted by the matrix $\Mat{W}_{ij}$. The sum of these messages is updated with functions $\phi^*$ and $\Omega^*$ to obtain the predicted activity rate $\widehat{\dot{\Vec{x}}}_{i}$.  In addition to the observed activity $x_i$, the GNN has access to learnable latent vectors $\Vec{a}_i$ associated with each node $i$.}
\label{fig:GNN structure}
\end{figure}

\section{Methods}

\subsection*{Simulation of neural assemblies}

We simulated the activity of neural assemblies with a model described in \citep{stern_reservoir_2023}.  In these simulations, each neuron (node $i$) receives activity signals $x_j$ from connected neurons (nodes $j$), and updates its own activity $x_i$ according to
\begin{equation}
\dot{x}_i=-\frac{x_i}{\tau_i} + s_i\phi(x_i) + g_i\Agg \Mat{W}_{ij} \psi(x_j) + \eta_i(t).
\label{eqn:simulation}
\end{equation}
These systems can generate activity across a wide range of time scales similar to what is observed between cortex regions \citep{stern_reservoir_2023}.  The damping effect (first term) is parameterized by $\tau$. The self-coupling (second term) is parameterized by $s$. $g$ is a global scaling factor. The matrix $\Mat{W}_{ij}$ contains the synaptic weights multiplying $\psi(x_j)$. The last term is a Gaussian noise with zero mean. in our experiments, the number of neurons $N$ was \np{1000} or \np{8000}.  We fixed $g_i$ to 10, and used values between \np{0.25} and \np{8} for $\tau$ and $s$ to test different self-coupling regimes.  For both $\phi(x)$ and $\psi(x)$, we chose $\tanh$. The connectivity matrix $\Mat{W}_{ij}$ was drawn from Cauchy distribution with $\mu=0$ and $\sigma^2=\frac{1}{N}$. The weights of $\Mat{W}_{ij}$ are signed to distinguish between excitation and inhibition. In more complicated systems we made the function $\psi$ neuron dependent or neuron-neuron dependent, by changing $\psi(x_j)$ to respectivley $\psi(\frac{x_j}{\gamma_i})$ and $\psi(\frac{x_j}{\gamma_i})-\theta_j x_j$ where $\gamma$ and $\theta$ are scalars associated with different neuron types. Finally we introduced modulation into the dynamics with a time-dependent function $\Omega$ that scales the message passing at single neuron level. The update form used in the simulations with neuron dependencies and modulation is
\begin{equation}
\begin{split}
\dot{x}_i = {} & -\frac{x_i}{\tau_i} + s_i\phi(x_i) \\
& + g_i\Omega_i(t)\Agg \Mat{W}_{ij} (\psi (\frac{x_j}{\gamma_i}) - \theta_j x_j) + \eta_i(t).
\label{eqn:simulation2}
\end{split}
\end{equation}

\subsection{Graph neural networks}

\cref{fig:GNN structure} depicts the components of the GNNs trained on simulated data. The GNN learns the update rule
\begin{equation}
\widehat{\dot{\Vec{x}}}_{i} =\phi^*(\Vec{a_i}, x_i) + \Omega_i^*(t) \Agg \Mat{W}_{ij}\psi^*(\Vec{a_i},x_j).
\label{eqn:GNN}
\end{equation}
The optimized neural networks are $\phi^*$, $\psi^*$, modeled as MLPs (ReLU activation, hidden dimension~$=64$, 3 layers, output size~$= 1$), and $\Omega^*$ modeled as a coordinated-based MLP (\citep{sitzmann_implicit_nodate}, input size~$= 3$,  hidden dimension~$=128$, 5 layers, output size~$= 1$, $\omega=0.3$). Other learnables are the two-dimensional latent vector $\Vec{a}_i$ associated with each neuron, and the connectivity matrix $\Mat{W}$. The optimization loss is specified by 
\begin{equation}
\begin{split}
\loss = {} & \sum_{i = 1}^n \alpha\lVert \widehat{\dot{\Vec{x}}}_{i} - \dot{\Vec{x}}_{i}\rVert^2 + \sum_{i = 1}^n \beta\lVert\phi^*(\Vec{a_i}, 0)\rVert^2 \\
& + \sum_{i = 1}^n \gamma\lVert \relu(-\frac{\partial\psi^*}{\partial x}(\Vec{a_i}, x_i))\rVert^2 + \zeta\lVert \Mat{W}_{ij} \rVert.
\end{split}
\label{eqn:loss_prediction}
\end{equation}
The first term is the prediction error with $\widehat{\dot{\Vec{x}}}_{i}$ being the GNN prediction, the second term enforces the MLP $\phi^*$ to be zero at the origin, the third term enforces the MLP $\psi^*$ to be monotonically increasing, and the last term encourages sparsity of the connectivity matrix $\Mat{W}$. The four regularization terms are scaled with hyper-parameters $\alpha$, $\beta$, $\gamma$ and $\zeta$ respectively. We implemented the GNNs using the PyTorch Geometric library \citep{fey_fast_2019} and used AdamUniform gradient descent, with a learning rate of $10^{-4}$. Each GNN was trained over $2\cdot 10^3$ to $4\cdot 10^3$ epochs, each epoch covering typically $10^5$ time-points of training series.

\section{Results}

\begin{figure}[t]
\centering%
\includegraphics[width=\columnwidth]{Fig2.png}
\caption{\np{1000} densely connected neurons with 4 neuron dependent update functions (first two terms in \cref{eqn:simulation}). (\textbf{a}) Activity time series used for GNN training. This dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c}) True connectivity $\Mat{W}_{ij}$. The inset shows $20\times20$ weights. (\textbf{d})~Learned connectivity. (\textbf{e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref{eqn:simulation}). (\textbf{f}) Learned latent vectors $\Vec{a}_i$ of all neurons. (\textbf{g})~Learned update functions $\phi^*(\Vec{a_i}, x)$. There are 1000 plots one for each vector $\Vec{ai}$ (\textbf{h}) Learned transfer function $\psi^*(x_i)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{fig2}
\end{figure}

\begin{figure*}[t]
\centering%
\includegraphics[width=0.74\textwidth]{Fig3.png}
\caption{\np{2048} densely connected neurons with different neuron dependent update and transfer functions (4 neuron types), in the presence of external neuromodulation. The training dataset contains $10^5$ time-points. Results are obtained after 16 epochs. (\textbf{a}) Neuromodulation values at frame 4000. Modulation is a time dependent scalar field $\Omega_i(t)$ (\cref{eqn:simulation2}). 1024 neurons (left), spatially ordered, are modulated by this field. The other 1024 neurons are not affected ($\Omega_i=1$). (\textbf{b}) Activity time values at frame 4000. (\textbf{c}) Sample of 25 time series used for training. (\textbf{d}) Comparison of learned and true connectivity  $\Mat{W}_{ij}$(given $g_i=10$ in \cref{eqn:simulation}). (\textbf{e}) Comparison of learned and true modulation values. (\textbf{f}) True modulation field $\Omega_i(t)$ plotted at different time-points. (\textbf{g})~Learned modulation field $\Omega_i(t)^*$ plotted at different time-points.}
\label{fig3}
\end{figure*}

In a first set of experiment, we evaluate the approach on a simulated neural network consisting of \np{1000} densely connected neurons. Four different types of neuron are present, associated with different values of $\tau_i$ and $s_i$ in \cref{eqn:simulation}. Figure \ref{fig2} shows the results obtained with a noise-free simulation, spanning $10^5$ time points. We successfully recovered the connectivity matrix ($R^2=1.00$, slope $=0.99$ given $g_i=10$), the common transfer function $\psi$, and the four distinct neuron-specific update functions. Note, by update function we mean the first two terms in \cref{eqn:simulation}, i.e. $-\frac{x_i}{\tau_i} + s_i \tanh(x_i)$. In the GNN, this multivariate function is approximated by the MLP $\phi^*$ which is parameterized with the latent vectors $\Vec{a_i}$. The ability to reconstruct four different functions results from the joint optimization of $\phi^*$ and $\Vec{a_i}$. This process, depicted in \cref{supp1}, shows a virtuous circle at play. The better the separation of the neuron types in the the latent space (\cref{supp1}a), the better the definition of the learned functions (\cref{supp1}b), and vice-versa. Ultimately, four distinct clusters are present in the latent space and they perfectly map to well defined update functions. Symbolic regression (PySR package \citep{cranmer_interpretable_2023}) applied to the learned functions allows to retrieve their exact expressions (\cref{table1}). Hierarchical clustering of the learned latent vectors $\Vec{a}_i$ recovers the neuron types with a classification accuracy of $1.00$. Increasing the dimension of the latent space does not improve the results, whereas using one-dimensional latent space is detrimental (data not shown). In this initial experiment, we successfully address the inverse problem of retrieving all components of the neural assemblies solely from data. 
\\
To assess generalization capabilities, we performed rollout inference using the trained GNN model with initial activity values not seen during training (\cref{supp12}). With the trained GNN, it is possible to forecast the activity of 1000 neurons up to 400 time-points ($R^2=0.94$, slope$=1.00$). The forecast precision degrades at 800 time-points ($R^2=0.36$, slope$=0.9$).
For comparison we trained a GNN with the hypothesis that all neurons are identical. To do so, we fixed the latent vectors $\Vec{a_i}$ to a unique vector (\cref{supp13}). Without taking into account the neuron heterogeneity, the connectivity matrix is not well recovered ($R^2=0.92$, slope$=0.93$) and the update function is no longer neuron-dependent. The GNN model is only an approximation of the true model, which prevents precise rollout inference (\cref{supp14}). This demonstrates that, if needed, treating all neurons as identical imposes too strict a limitation when modeling neural assemblies. Next, we evaluated generalization to fundamentally modified network architectures. When testing the learned GNN model with altered neuron type distributions (from equal 25\% each to 10\%, 20\%, 30\%, 40\%) and reduced connectivity (25\% sparsity), the model maintains perfect short-term accuracy ($R^2=1.0$ at 400 time-steps) and excellent long-term performance ($R^2=0.996$ at 800 time-steps) (\cref{supp18}). Even more radical modifications, i.e. eliminating half the neuron types entirely (60\%, 40\% distribution of only two types) combined with reduced connectivity (50\% sparsity), still yield robust performance ($R^2=1.0$ at 400 time-steps, $R^2=0.975$ at 800 time-steps) (\cref{supp19}). These results demonstrate that the GNN captures fundamental dynamical principles that generalize across diverse network architectures, rather than memorizing specific connectivity or type configurations.
\\
Performance scales with the length of the training series. In initial runs we used up to \(10^5\) time points to solve the inverse problem; systematic sub-sampling shows that \(\approx 2\!\cdot\!10^4\) time points are sufficient to recover our \(\np{1000}\)-neuron system with good accuracy (\cref{supp2}).
\\
Recovery also depends on connection density. Varying the filling factor from \(5\%\) to \(100\%\), we find that even \(5\%\)-sparse \(\Mat{W}\) is accurately recovered when we enforce sparsity with an \(\ell_1\)-penalty on \(\Mat{W}\) (\cref{supp4,supp5}) or, alternatively, by masking entries known to be zero (data not shown).
\\
To assess robustness, we injected Gaussian process noise into the dynamics in \cref{eqn:simulation} to obtain an SNR of \(\sim 10\,\mathrm{dB}\). At this noise level, training remains stable and we still recover both the connectivity matrix and the signaling functions (\cref{supp6}).
\\
Finally, in this first set of experiments, we tested the ability to recover larger connectivity matrix, i.e. when the number of neurons is increased to \np{8000}, in the presence of Gaussian noise (SNR of $\sim$16 dB).  \cref{supp3} shows that the 64 million weights of the connectivity matrix are well recovered ($R^2=1.00$, slope$=1.00$).
\\
In a second set of experiments, we evaluate the ability to recover more complex dynamics by introducing additional variability in the signaling functions . \cref{supp7} shows the results obtained with a simulation containing up to 32 different update functions. All 32 functions can be well approximated by the MLP $\phi^*$ (\cref{supp7}g) and the different neuron types are well distinguished in the latent space (\cref{supp7}g, classification accuracy of 0.99).
\\
\Cref{supp8} considers neuron-specific transfer functions of the form
$\psi\!\big(x_j/\gamma_i\big)$. Jointly optimizing the shared MLP $\psi^\*$ and
the latent vectors $\Vec{a_i}$ accurately captures these heterogeneities; symbolic
regression recovers their exact forms (\cref{table2}). The learned connectivity
still closely matches the ground truth ($R^2=0.99$, slope$=0.99$;
\cref{supp8}d,e).
\\
Further, \Cref{supp9} examines neuron–neuron–specific transfer functions
$\psi\!\big(x_j/\gamma_i-\theta_i x_j\big)$. Here the multivariate MLP $\psi^\*$ takes
pairwise latents $(\Vec{a_i},\Vec{a_j})$ as input. The four neuron types are well
recovered (\cref{supp9}f), as are the four update functions (\cref{supp9}g); in total,
16 pairwise transfer functions are identified, and symbolic regression yields close
approximations (\cref{table3}). The learned connectivity again matches the ground truth
($R^2=0.99$, slope$=1.03$).
\\
In a third and last set of experiments, we evaluate the ability to recover external neuromodulation. The latter is implemented by introducing a time evolving function $\Omega_i(t)$ in the simulation (\cref{eqn:simulation2}).  We defined the function $\Omega$ as being spatially defined on a grid of 1024 neurons (\cref{fig3}a). Hence $\Omega_i(t)$ defines a time-dependent scalar field. During training it is approximated by a coordinate-based MLP $\Omega^*$ which, takes three inputs ($x$, $y$ and $t$) and output the corresponding modulation values. In addition to the grid of \np{1024} neurons experiencing neuromodulation, the simulation contains another set of \np{1024} neurons that are not affected by neuromodulation (disc in \cref{fig3}a). This second set of neurons act as a small processing unit. \cref{fig3}a shows that it is possible to learn the time-dependent neuromodulation ($R^2=0.99$, slope$=1.02$) together with the connectivity matrix ($\sim 4.2\cdot10^6$ neurons, $R^2=0.99$, slope$=0.94$) and the different signaling functions (\cref{supp10}).

\section{Discussion}

In preparation for applications on experimental data \citep{lueckmann_zapbench_2025}, we designed simulations that provide some of the required components to model a complex neuronal activity.  On these simulations, we showed that message-passing GNNs can be used to recover the structure and the functions of heterogeneous neural assemblies. However, for an application in neuroscience, key features are still missing. In future work, we will explore e.g. changing connectivity, changing signaling functions, time delays, and excitation by external signals. 

Still the present works allowed to circumvent several limitations of machine learning approach applied to neuronal data. Importantly, interpretability of the learned model is demonstrated. All results have direct representations without the need of an extra explainability step. The learned latent domain is two-dimensional, hence a scatter-plot can reveal its structure, wether it is discrete or continuous, wether clusters pointing different neuron type are presents. The signaling functions are approximated by simple MLPs. Profiles can be plotted for analytical analysis. Next, we demonstrated that the function approximations are accurate enough for symbolic regression to recover their analytical expressions.

Importantly, we showed that the joint optimization of MLPs and latent vectors is a powerful tool to account for the variability present in neural assemblies. It allows to distinguish different types of neurons, reveal their signaling functions, even in the presence of external neuromodulation. The complexity of a neural assembly can thus be decomposed into simple and small components. The complexity of the whole is preserved by the GNN structure and understood as the sum of simple components. As such our method differ from other machine learning techniques used to model neural assembly that prioritize forecast accuracy at the expense of interpretability.

\section{Conclusion}
In conclusion, our approach demonstrates the potential of GNNs to model complex neuronal activity with interpretability, recovering key components such as connectivity matrices, signaling functions, and modulation from data alone. The method effectively decomposes neural complexity into simple, understandable components. Future work will focus on improving efficiency and incorporating additional key features of neural activity, such as changing connectivity and time delays, to enhance its application to experimental data.


% \begin{figure}[t]
% \centering%
% \includegraphics[width=7cm]{Fig5.png}
% \caption{Experiment with 1000 neurons, 4 types, sparse $5\%$. Same caption as in \cref{fig2}.}
% \label{fig5}
% \end{figure}

\section*{Acknowledgements}

We thank Tosif Ahamed and Tirthabir Biswas for helpful discussions and comments that improved this work and manuscript. 
% Vivek Jayaraman, Sandro Romani and William Bialek 

\printbibliography

\appendix

\renewcommand{\figurename}{Supplementary Figure}
\setcounter{figure}{0}
\crefalias{figure}{appendixfigure}%
\renewcommand{\tablename}{Supplementary Table}
\setcounter{table}{0}
\crefalias{table}{appendixtable}%

\clearpage
\section{Supplementary notes}

\begin{figure*}[h]
\centering%
\includegraphics[width=0.9\textwidth]{Supp1.png}
\caption{\np{1000} densely connected neurons with 4 neuron dependent update functions.  Results plotted over 20 epochs.  (\textbf{a}) Learned latent vectors $\Vec{a}_i$ of all neurons. (\textbf{b})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{c}) Learned transfer function $\psi^*(x)$, normalized to a maximum value of 1. 
(\textbf{d})~Learned connectivity $\Mat{W}_{ij}$.
(\textbf{e}) Comparison of learned and true connectivity. Colors indicate true neuron types.}
\label{supp1}
\end{figure*}

\begin{figure}[h]
\centering%
\includegraphics[width=7 cm]{Supp12.png}
\caption{Rollout inference performed with the GNN model trained with a simulation of \np{1000} densely connected neurons (\cref{fig2}). Results plotted respectively at 400 and 800 time-steps. (\textbf{a}) and (\textbf{c}) 25 learned activity traces plotted as a function of time-points. True activity traces are overlaid in light gray. (\textbf{b}) and (\textbf{d}) Comparison between true and learned activity values of 1000 neurons.  
}
\label{supp12}
\end{figure}

% \begin{figure}[h]
% \centering%
% \includegraphics[width=\columnwidth]{Supp15.png}
% \caption{Generalization ablation test: rollout inference performance over 800 time-steps after randomly removing connections from the learned connectivity matrix $\Mat{W}_{ij}$. The GNN model was trained with \np{1000} densely connected neurons (\cref{fig2}), then tested with (\textbf{a,b}) 10\% ablation, (\textbf{c,d}) 50\% ablation, and (\textbf{e,f}) 90\% ablation of connections. Left panels show 25 activity traces with true values in gray. Right panels show learned vs. true activity correlations with $R^2$ and slope metrics. The model maintains high accuracy even with severe connectivity ablation.}
% \label{supp15}
% \end{figure}

% \begin{figure}[h]
% \centering%
% \includegraphics[width=\columnwidth]{Supp16.png}
% \caption{Generalization test with inactive neurons: rollout inference performance over 400 time-steps after randomly setting neurons as inactive. The GNN model was trained with \np{1000} densely connected neurons (\cref{fig2}), then tested with (\textbf{a,b}) no ablation (control), (\textbf{c,d}) 25\% neurons inactive, and (\textbf{e,f}) 50\% neurons inactive. Left panels show 25 activity traces with true values in gray. Right panels show learned vs. true activity correlations with $R^2$ and slope metrics. The model demonstrates perfect generalization even with half the neurons inactive ($R^2=1.0$, slope$=1.01$).}
% \label{supp16}
% \end{figure}

% \begin{figure}[h]
% \centering%
% \includegraphics[width=\columnwidth]{Supp17.png}
% \caption{Generalization test with permuted neuron types: rollout inference performance over 400 time-steps after randomly permuting neuron type assignments and their associated latent vectors $\Vec{a}_i$. The GNN model was trained with \np{1000} densely connected neurons (\cref{fig2}), then tested with (\textbf{a,b}) no permutation (control), (\textbf{c,d}) 25\% neuron types permuted, and (\textbf{e,f}) 90\% neuron types permuted. Left panels show 25 activity traces with true values in gray. Right panels show learned vs. true activity correlations with $R^2$ and slope metrics. Performance degrades progressively with type permutation, reaching $R^2=0.85$ with 90\% permutation, demonstrating the importance of correct neuron type assignment for accurate dynamics.}
% \label{supp17}
% \end{figure}

\begin{figure}[h]
\centering%
\includegraphics[width=\columnwidth]{Supp13.png}
\caption{\np{1000} densely connected neurons with 4 neuron dependent update functions (first two terms in \cref{eqn:simulation}).  Results are obtained with a GNN trained with the hypothesis that all neurons are identical. To do so, we fixed the learnable latent vectors $\Vec{a_i}$ to a unique vector. (\textbf{a}) Activity time series used for GNN training. This dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c}) True connectivity $\Mat{W}_{ij}$. The inset shows $20\times20$ weights. (\textbf{d})~Learned connectivity. (\textbf{e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref{eqn:simulation}).  (\textbf{f}) Fixed latent vectors $\Vec{a}_i$ of all neurons. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer function $\psi^*(x)$,  normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp13}
\end{figure}

\begin{figure}[h]
\centering%
\includegraphics[width=7 cm]{Supp14.png}
\caption{Rollout inference performed with the GNN model trained with the hypothesis that all neurons are identical (\cref{supp13}). Results plotted at respectivley 200 and 800 time-steps. (\textbf{a}) and (\textbf{c}) 25 learned activity traces plotted as a function of time-points. True activity traces are overlaid in light gray. (\textbf{b}) and (\textbf{d}) Comparison between true and learned activity values of 1000 neurons.  
}
\label{supp14}
\end{figure}

\begin{figure}[h]
\centering%
\includegraphics[width=7 cm]{Supp18.png}
\caption{Generalization test with modified network structure: performance evaluation after changing connectivity matrix and neuron type distribution. The GNN model was trained with \np{1000} densely connected neurons. (\textbf{a}) Original neuron type distribution (25\% each type). (\textbf{b}) Modified distribution (10\%, 20\%, 30\%, 40\%). (\textbf{c}) Original connectivity matrix ($10^6$ weights, fully connected). (\textbf{d}) Modified sparse connectivity matrix (243,831 weights, $\sim$25\% sparsity). (\textbf{e,f}) Rollout inference over 400 time-steps shows perfect performance ($R^2=1.0$, slope$=1.0$). (\textbf{g,h}) Extended rollout over 800 time-steps maintains high accuracy ($R^2=0.996$, slope$=1.0$).}
\label{supp18}
\end{figure}

\begin{figure}[h]
\centering%
\includegraphics[width=7 cm]{Supp19.png}
\caption{Generalization test with modified network structure. (\textbf{a}) Original neuron type distribution (25\% each type). (\textbf{b}) Modified distribution with only two types (60\%, 40\%, types 2 and 3 eliminated). (\textbf{c}) Original connectivity matrix ($10^6$ weights, fully connected). (\textbf{d}) Modified sparse connectivity matrix (487,401 weights, 50\% sparsity). (\textbf{e,f}) Rollout inference over 400 time-steps shows perfect performance ($R^2=1.0$, slope$=1.0$). (\textbf{g,h}) Extended rollout over 800 time-steps maintains high accuracy ($R^2=0.975$, slope$=0.99$).}
\label{supp19}
\end{figure}



\begin{figure}
\centering%
\includegraphics[width=7 cm]{Supp2.png}
\caption{\np{1000} densely connected neurons with 4 neuron dependent update functions. The plot displays the comparison between the true and learned connectivity matrices, $\Mat{W}_{ij}$, based on the linear regression coefficient, $R^2$. Results are given as a function of the number of training epochs and the number of time-points present in the training dataset (colors).}
\label{supp2}
\end{figure}


\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp4.png}
\caption{\np{1000} sparsely (5\%) connected neurons with 4 neuron dependent update functions. Results are obtained after 20 epochs. (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c}) True connectivity $\Mat{W}_{ij}$. The inset shows $20\times20$ weights. (\textbf{d})~Learned connectivity. (\textbf{e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref{eqn:simulation}). (\textbf{f}) Learned latent vectors $\Vec{a}_i$ of all neurons. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer function $\psi^*(x)$,  normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp4}
\end{figure}


\begin{figure}
\centering%
\includegraphics[width=7 cm]{Supp5.png}
\caption{\np{1000} densely connected neurons with 4 neuron dependent update functions. The plot displays the comparison between the true and learned connectivity matrices, $\Mat{W}_{ij}$, based on the linear regression coefficient, $R^2$. Results are given as a function of the number of training epochs and the filling factor of the connectivity matrix (colors).}
\label{supp5}
\end{figure}



\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp6.png}
\caption{\np{1000} densely connected neurons with 4 neuron dependent update functions in the presence of Gaussian noise (\cref{eqn:simulation2}). The signal-to-noise ratio is about 10 dB as measured by comparing filtered and raw signals. For comparison corresponding signals without noise are shown in \cref{fig2}. (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c}) True connectivity $\Mat{W}_{ij}$. The inset shows $20\times20$ weights. (\textbf{d})~Learned connectivity. (\textbf{e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref{eqn:simulation}). (\textbf{f}) Learned latent vectors $\Vec{a}_i$ of all neurons. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer function $\psi^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp6}
\end{figure}


\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp3.png}
\caption{\np{8000} densely connected neurons with 4 neuron dependent update functions.  Results obtained after 14 epochs. (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c}) True connectivity $\Mat{W}_{ij}$. The inset shows $20\times20$ weights. (\textbf{d})~Learned connectivity. (\textbf{e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref{eqn:simulation}). (\textbf{f}) Learned latent vectors $\Vec{a}_i$ of all neurons. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer function $\psi^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp3}
\end{figure}


\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp7.png}
\caption{\np{1000} densely connected neurons with 32 neuron dependent update functions. Results are obtained after 20 epochs. (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c}) True connectivity $\Mat{W}_{ij}$. (\textbf{d}) Learned connectivity. (\textbf{e}) Comparison between learned and true connectivity. (\textbf{f}) Learned latent vectors $\Vec{a}_i$. (\textbf{g}) Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer functions $\psi^*(\Vec{a},x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp7}
\end{figure}


\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp8.png}
\caption{\np{1000} densely connected neurons with neuron dependent update and transfer functions (4 neuron types). (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c})~True connectivity $\Mat{W}_{ij}$. (\textbf{d})~Learned connectivity. (\textbf{e})~Comparison between learned and true connectivity. (\textbf{f})~Learned latent vectors $\Vec{a}_i$. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer functions $\psi^*(\Vec{a},x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp8}
\end{figure}

\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp9.png}
\caption{\np{1000} densely connected neurons with neuron dependent update and transfer functions (4 neuron types). (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c})~True connectivity $\Mat{W}_{ij}$. (\textbf{d})~Learned connectivity. (\textbf{e})~Comparison between learned and true connectivity. (\textbf{f})~Learned latent vectors $\Vec{a}_i$. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. (\textbf{h}) Learned transfer functions $\psi^*(\Vec{a_i},\Vec{a_j}, x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp9}
\end{figure}


\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{Supp10.png}
\caption{\np{2048} densely connected neurons with neuron dependent update and transfer functions (4 neuron types) in the presence of external neuromodulation. Results are obtained after 16 epochs. (\textbf{a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf{b}) Sample of 25 time series taken from (\textbf{a}). (\textbf{c})~True connectivity $\Mat{W}_{ij}$. (\textbf{d})~Learned connectivity. (\textbf{e})~Comparison between learned and true connectivity. (\textbf{f})~Learned latent vectors $\Vec{a}_i$. (\textbf{g})~Learned update functions $\phi^*(\Vec{a}, x)$. The learned latent vectors spread out more, but the update functions were correctly learned. They can thus be used for clustering after UMAP projection of the profiles. Hierarchical clustering of the UMAP projections recovered the particle types with a classification accuracy of $1.00$. (\textbf{h}) Learned transfer functions $\psi^*(\Vec{a},x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}
\label{supp10}
\end{figure}

\clearpage
\section*{}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{|c|c|c|}
\hline
\textbf{function} & \textbf{true} & \textbf{learned} \\
\hline
$\phi_1$ & $-x + \tanh x$ & $-0.998x +\tanh x - 0.0016$ \\
\hline
$\phi_2$ & $-x + 2\tanh x$ & $-0.998x +1.996\tanh x$ \\
\hline
$\phi_3$ & $-2x + \tanh x$ & $-1.994x +\tanh x$ \\
\hline
$\phi_3$ & $-2x + 2\tanh x$ & $-1.996x +1.997\tanh x$ \\
\hline
$\psi$ & $\tanh x$ & $\tanh x$ \\
\hline
\end{tabular}
\caption{Comparison of true and learned functions plotted in \cref{fig2}. Symbolic regression (PySR package \citep{cranmer_interpretable_2023}) is applied to the learned functions to retrieve their expressions.}
\label{table1}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{|c|c|c|}
\hline
\textbf{function} & \textbf{true} & \textbf{learned} \\
\hline
$\phi_1$ & $-x + \tanh x$ & $-0.999x +\tanh x$ \\
\hline
$\phi_2$ & $-x + 2\tanh x$ & $-0.999x +1.992\tanh x$ \\
\hline
$\phi_3$ & $-2x + \tanh x$ & $-1.994x +\tanh x$ \\
\hline
$\phi_3$ & $-2x + 2\tanh x$ & $-1.984x +1.991\tanh x$ \\
\hline
$\psi_1$ & $\tanh x$ & $\tanh x$ \\
\hline
$\psi_2$ & $\tanh 0.5x$ & $\tanh 0.489x$ \\
\hline
$\psi_3$ & $\tanh 0.25x$ & $\tanh 0.247x$ \\
\hline
$\psi_4$ & $\tanh 0.125x$ & $\tanh 0.128x$ \\
\hline
\end{tabular}
\caption{Comparison of true and learned functions plotted in \cref{supp8}.}
\label{table2}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{|c|c|c|}
\hline
\textbf{function} & \textbf{true} & \textbf{learned} \\
\hline
$\phi_1$ & $-x + \tanh x$ & $-0.999x + \tanh x$ \\
\hline
$\phi_2$ & $-x + 2\tanh x$ & $-0.993x +1.999\tanh x$\\
\hline
$\phi_3$ & $-2x + \tanh x$ & $-1.992x +\tanh x$ \\
\hline
$\phi_3$ & $-2x + 2\tanh x$ & $-1.974x +1.989\tanh x$ \\
\hline
$\psi_{11}$ & $\tanh x$ & $\tanh x$ \\
\hline
$\psi_{12}$ & $\tanh x - 0.013 x$ & $\tanh x - 0.017x$ \\
\hline
$\psi_{13}$ & $\tanh x - 0.027 x$ & $\tanh x -0.028x$ \\
\hline
$\psi_{14}$ & $\tanh x - 0.040 x$ & $\tanh x -0.053x$ \\
\hline
$\psi_{21}$ & $\tanh 0.5x$ & $\tanh 0.486x$ \\
\hline
$\psi_{22}$ & $\tanh 0.5x - 0.013 x$ & $\tanh0.414x$ \\
\hline
$\psi_{23}$ & $\tanh 0.5x - 0.027 x$ & $0.814 \tanh 0.603x$ \\
\hline
$\psi_{24}$ & $\tanh 0.5x - 0.040 x$ & $0.67 \tanh x$ \\
\hline
$\psi_{31}$ & $\tanh 0.25x$ & $\tanh 0.222x$ \\
\hline
$\psi_{32}$ & $\tanh 0.25x - 0.013 x$ & $\tanh 0.204x$ \\
\hline
$\psi_{33}$ & $\tanh 0.25x - 0.027 x$ & $\tanh 0.163x$ \\
\hline
$\psi_{34}$ & $\tanh 0.25x - 0.040 x$ & $\tanh 0.172x$ \\
\hline
$\psi_{41}$ & $\tanh 0.125x$ & $\tanh 0.118x$ \\
\hline
$\psi_{42}$ & $\tanh 0.125x - 0.013 x$ & $\tanh 0.110x$ \\
\hline
$\psi_{43}$ & $\tanh 0.125x - 0.027 x$ & $\tanh 0.097x$ \\
\hline
$\psi_{44}$ & $\tanh 0.125x - 0.040 x$ & $\tanh 0.081x$ \\
\hline
\end{tabular}
\caption{Comparison of true and learned functions plotted in \cref{supp9}.}
\label{table3}
\end{table}







\end{document}



