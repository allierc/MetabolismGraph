---
title: "Results"
date: last-modified
date-format: "MMMM D, YYYY"
---

## Rate Constant Recovery — Oscillatory Regime (rank 50)

The LLM-driven exploration engine is running on the **oscillatory regime** (activity rank $\sim50$): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix $\mathbf{S}$ frozen from ground truth. The goal is to recover the 256 rate constants $k_j \in [10^{-2}, 10^{-1}]$ by optimizing training hyperparameters through UCB tree search with 4 parallel slots.

**136 iterations completed** across 12 blocks (34 batches). The exploration is ongoing.

### Simulation Setup

::: {layout-ncol=2}

![Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.](figures/rank_50/concentrations.png){#fig-concentrations}

![Stoichiometric matrix $\mathbf{S}$ (100 metabolites $\times$ 256 reactions). Red = products (+1), blue = substrates (--1). 100% autocatalytic 3-cycles.](figures/rank_50/stoichiometry.png){#fig-stoichiometry}

:::

### Metrics

The primary metric is **raw R²** computed on all 256 reactions (after MLP$_{\text{sub}}$ scalar correction). The **trimmed R²** excludes outlier reactions ($|\Delta \log_{10} k| > 0.3$) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.

### R² Trajectory Across 136 Iterations

```{python}
#| code-fold: true
#| label: fig-iterations
#| fig-cap: "Raw R² (bars) and outlier count (dots) across 136 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74), seed=77+sub_diff=8 (iter 82, 0.76), seed=79+aug=5000 (iter 96, 0.85), seed=77+sub_diff=6 (iter 106, 0.80), seed=77+sub_diff=6+aug=5500 (iter 116, 0.87). Training variance of ±0.2 R² is a dominant factor."

import matplotlib.pyplot as plt
import numpy as np

iters = list(range(1, 137))
r2 = [
    # Block 1 (Iter 1-12): Initial exploration
    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep
    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough
    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation
    # Block 2 (Iter 13-24): k_floor breakthrough
    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough
    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor
    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best
    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough
    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns
    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches
    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH
    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7
    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub
    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed
    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST
    # Block 5 (Iter 49-60): Confirming optimum
    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests
    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds
    0.701, 0.718, 0.603, 0.603,  # Batch 15: seed=99 promising
    # Block 6 (Iter 61-72): Variance discovery
    0.688, 0.430, 0.616, 0.704,  # Batch 16: k_floor=1.5 promising
    0.565, 0.674, 0.664, 0.609,  # Batch 17: k_floor non-monotonic
    0.658, 0.639, 0.473, 0.550,  # Batch 18: HIGH VARIANCE discovered
    # Block 7 (Iter 73-84): seed=77 breakthrough
    0.722, 0.583, 0.682, 0.636,  # Batch 19: seed=123 improved
    0.694, 0.515, 0.473, 0.748,  # Batch 20: seed=77 NEW BEST
    0.661, 0.764, 0.387, 0.689,  # Batch 21: seed=77+sub_diff=8 PEAK
    # Block 8 (Iter 85-96): seed=79 breakthrough + aug=5000
    0.619, 0.512, 0.748, 0.720,  # Batch 22: seed=79 golden seed discovered
    0.822, 0.640, 0.572, 0.704,  # Batch 23: seed=79 replicate R²=0.82
    0.592, 0.619, 0.546, 0.851,  # Batch 24: seed=79+aug=5000 NEW BEST
    # Block 9 (Iter 97-108): seed=77+sub_diff=6 discovery
    0.720, 0.609, 0.742, 0.471,  # Batch 25: aug=5500 hurts seed=79
    0.754, 0.634, 0.730, 0.764,  # Batch 26: sub_diff=7 for seed=77
    0.780, 0.804, 0.683, 0.623,  # Batch 27: sub_diff=6 2ND BEST
    # Block 10 (Iter 109-120): GLOBAL BEST iter 116
    0.550, 0.572, 0.655, 0.589,  # Batch 28: sub_diff=6 cross-seed (variance!)
    0.637, 0.643, 0.566, 0.869,  # Batch 29: seed=77+sub_diff=6+aug=5500 GLOBAL BEST
    0.657, 0.518, 0.581, 0.812,  # Batch 30: aug=5750 hurts, sub_diff=7 good
    # Block 11 (Iter 121-132): Robustness testing
    0.734, 0.733, 0.574, 0.509,  # Batch 31: sub_diff=7 robust, sub_diff=8 hurts
    0.782, 0.680, 0.544, 0.670,  # Batch 32: sub_diff=7 variance ~0.05
    0.509, 0.675, 0.721, 0.846,  # Batch 33: k_floor=1.5 2ND BEST
    # Block 12 (Iter 133-136): k_floor=1.5 variance confirmed
    0.635, 0.512, 0.677, 0.692,  # Batch 34: k_floor=1.5 not reproducible
]
outliers = [
    43, 47, 45, 53,
    61, 36, 45, 32,
    27, 38, 57, 30,
    28, 33, 36, 33,
    17, 24, 26, 35,
    16, 29, 19, 24,
    18, 18, 15, 17,
    19, 14, 21, 21,
    21, 21, 15, 22,  # Batch 9
    16, 19, 20, 21,  # Batch 10
    21, 16, 18, 19,  # Batch 11
    15, 20, 23, 21,  # Batch 12
    12, 21, 12, 25,  # Batch 13
    21, 21, 18, 19,  # Batch 14
    17, 17, 16, 20,  # Batch 15
    17, 20, 17, 16,  # Batch 16
    17, 14, 16, 14,  # Batch 17
    19, 14, 22, 17,  # Batch 18
    19, 16, 22, 20,  # Batch 19
    16, 16, 32, 12,  # Batch 20
    16, 15, 23, 18,  # Batch 21
    16, 22, 15, 20,  # Batch 22
    13, 13, 15, 17,  # Batch 23
    22, 14, 21, 11,  # Batch 24
    18, 20, 15, 16,  # Batch 25
    17, 16, 16, 13,  # Batch 26
    16, 12, 14, 12,  # Batch 27
    14, 15, 18, 17,  # Batch 28
    15, 17, 17, 10,  # Batch 29
    18, 20, 15, 10,  # Batch 30
    12, 10, 17, 14,  # Batch 31
    10, 10, 15, 14,  # Batch 32
    18, 10, 12, 12,  # Batch 33
    18, 21, 11, 20,  # Batch 34
]

# Color by block
block_colors = [
    '#3498db', '#2ecc71', '#e67e22', '#9b59b6',
    '#1abc9c', '#e84393', '#f39c12', '#e74c3c',
    '#2c3e50', '#d35400', '#8e44ad', '#27ae60',
]
block_bounds = [12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 136]
colors = []
for i in range(len(r2)):
    for b, bound in enumerate(block_bounds):
        if i < bound:
            colors.append(block_colors[b])
            break

fig, ax1 = plt.subplots(figsize=(18, 5))

bars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)
ax1.axhline(y=0.869, color='#e74c3c', linestyle='--', alpha=0.4, label='Peak R² = 0.869')
ax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')

# Block separators and labels
for b in range(len(block_bounds) - 1):
    ax1.axvline(x=block_bounds[b] + 0.5, color='gray', linestyle=':', alpha=0.4)
block_labels = [
    'Block 1', 'Block 2', 'Block 3', 'Block 4', 'Block 5', 'Block 6',
    'Block 7', 'Block 8', 'Block 9', 'Block 10', 'Block 11', 'Block 12',
]
block_centers = [6.5, 18.5, 30.5, 42.5, 54.5, 66.5, 78.5, 90.5, 102.5, 114.5, 126.5, 134.5]
for i, (lbl, cx) in enumerate(zip(block_labels, block_centers)):
    ax1.text(cx, 1.0, lbl, ha='center', fontsize=7, color=block_colors[i])

# Annotate key events
ax1.annotate('k_floor=1.0\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.25),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(33, 0.86),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('sub_diff=7', xy=(45, 0.736), xytext=(45, 0.86),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('seed=77+\nsub_diff=8', xy=(82, 0.764), xytext=(82, 0.90),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('seed=79+\naug=5000', xy=(96, 0.851), xytext=(93, 0.95),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('sub_diff=6\nR²=0.80', xy=(106, 0.804), xytext=(106, 0.90),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('GLOBAL BEST\nR²=0.87', xy=(116, 0.869), xytext=(116, 0.96),
            arrowprops=dict(arrowstyle='->', color='red'), fontsize=7, ha='center', color='red', fontweight='bold')
ax1.annotate('k_floor=1.5\nR²=0.85', xy=(132, 0.846), xytext=(132, 0.93),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')

ax1.set_xlabel('Iteration')
ax1.set_ylabel('rate_constants R² (raw)')
ax1.set_ylim(-0.02, 1.05)
ax1.set_xticks([1, 12, 24, 36, 48, 60, 72, 84, 96, 106, 108, 116, 120, 132, 136])

# outlier count on secondary axis
ax2 = ax1.twinx()
ax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')
ax2.set_ylabel('outlier count', color='#e74c3c')
ax2.tick_params(axis='y', labelcolor='#e74c3c')
ax2.set_ylim(0, 70)

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)

ax1.set_title('UCB Exploration: Rate Constant Recovery (136 iterations)')
plt.tight_layout()
plt.show()
```

### Block 1: Initial Exploration (Iter 1--12)

All 12 iterations achieved raw R² < 0.07. The key discovery was that `coeff_MLP_sub_norm=1.0` is essential — it corrects the MLP$_{\text{sub}}$ function shapes (c² becomes quadratic instead of linear) and enables MLP$_{\text{node}}$ to learn homeostasis.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 1 (iter 1--4) | lr sweep | 0.044 | All failed; MLP$_{\text{node}}$ dead, MLP$_{\text{sub}}$ c² linear |
| 2 (iter 5--8) | `sub_norm=1.0` | **0.067** | MLP$_{\text{sub}}$ normalization is the single most effective change |
| 3 (iter 9--12) | combine best | 0.061 | MLP$_{\text{node}}$ activated; `lr_node=0.005` hurts |

### Block 2: k_floor Breakthrough (Iter 13--24)

The `coeff_k_floor=1.0` penalty at iteration 14 produced a **10× improvement** in R² (0.06 → 0.51) by preventing outlier $\log k$ values from drifting below the true minimum. Longer training then pushed R² to 0.69.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 4 (iter 13--16) | `k_floor=1.0` | **0.508** | Breakthrough — R² jumped 10× |
| 5 (iter 17--20) | `aug=3000` | **0.642** | Longer training + k_floor synergistic |
| 6 (iter 21--24) | `aug=4000` | **0.690** | First to reach 0.69; $\alpha = 0.85$, 16 outliers |

**Best result — Iteration 116**: `seed=77`, `coeff_MLP_sub_diff=6`, `lr_sub=0.001`, `data_augmentation_loop=5500`, `coeff_k_floor=1.0`, `coeff_MLP_sub_norm=1.0`

| Metric | Value |
|--------|-------|
| Raw R² | **0.869** |
| Outliers | 10 / 256 (3.9%) |
| Slope | 0.98 |
| $\alpha$ | 0.93 |

*Note: Optimal `sub_diff` and `aug` are seed-dependent: `sub_diff=6` for seed=77 (R²=0.87), `sub_diff=7` for seed=42 (R²=0.74), `sub_diff=8` for seed=79 (R²=0.85). seed=77 benefits from `aug=5500`, seed=79 from `aug=5000`. Training has intrinsic variance of $\pm$0.2–0.3 R² even with identical configuration.*

### Rate Constants Recovery

::: {layout-ncol=3}

![Iteration 1 (baseline). R² = 0.044, 43 outliers. Before `k_floor` and `sub_norm` — predicted $\log k$ values scatter widely with no correlation to ground truth.](figures/rank_50/rate_constants_baseline.png){#fig-k-baseline}

![Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The `k_floor=1.0` penalty prevents outlier $\log k$ from drifting below the true minimum, producing a 10$\times$ R² jump.](figures/rank_50/rate_constants_breakthrough.png){#fig-k-breakthrough}

![Iteration 96 (best). R² = 0.851, 11 outliers. `seed=79` + `sub_diff=8` + `aug=5000` yields the tightest clustering around the diagonal.](figures/rank_50/rate_constants_best.png){#fig-k-best}

:::

### Kinograph

![Iteration 1 (baseline). Top-left: ground-truth $dc/dt$; top-right: GNN prediction; bottom-left: residual; bottom-right: scatter. The untrained GNN produces near-zero predictions — no reaction dynamics captured.](figures/rank_50/kinograph_baseline.png){#fig-kinograph-baseline}

![Iteration 96 (best, seed=79, sub_diff=8, aug=5000). Same layout. The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure. The residual reveals the missing homeostatic signal.](figures/rank_50/kinograph_best.png){#fig-kinograph-best}

### Learned MLPs

::: {layout-ncol=2}

![Iteration 1 (baseline). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.42 — under-scaled. MLP$_{\text{node}}$: flat at zero.](figures/rank_50/mlp_baseline.png){#fig-mlp-baseline}

![Iteration 96 (best R²). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.94 — close to the true scale. MLP$_{\text{node}}$: still flat at zero across all 96 iterations — homeostasis not learned.](figures/rank_50/mlp_best.png){#fig-mlp-best}

:::

### Block 3: Plateau and Breakthrough (Iter 25--36)

Twelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling `lr_sub` from 0.0005 to 0.001 broke through to R² = 0.726.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 7 (iter 25--28) | `aug=5000`, seed, batch_size | 0.652 | `aug=5000` hurt R² but $\alpha=0.95$ (best ever) |
| 8 (iter 29--32) | L1=0, `sub_norm=2.0`, lr_k | 0.619 | `sub_norm=2.0`: fewest outliers (14), best slope (0.99) |
| 9 (iter 33--36) | `sub_norm=2.0`+`aug=3500`, recurrent, **`lr_sub=0.001`**, `aug=3500` | **0.726** | `lr_sub=0.001` broke the plateau |

```{python}
#| code-fold: true
#| label: fig-strategies
#| fig-cap: "Impact of key strategies on R². Each dot is one iteration. seed=77+sub_diff=6+aug=5500 achieved the global best R²=0.869 (iter 116). Training has extreme intrinsic variance of ±0.2--0.3 R²."

import matplotlib.pyplot as plt
import numpy as np

strategies = {
    'No k_floor\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],
    'k_floor\nsub_diff=5\nseed=42': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],
    'sub_diff=7\nseed=42': [0.736, 0.696, 0.655, 0.662, 0.560, 0.701, 0.718, 0.658, 0.720, 0.574],
    'seed=77\nsub_diff=6': [0.804, 0.550, 0.869, 0.657, 0.581, 0.670, 0.512, 0.635],
    'seed=77\nsub_diff=7': [0.764, 0.780, 0.643, 0.812, 0.734, 0.733, 0.782, 0.680, 0.509, 0.675, 0.846],
    'seed=79\nsub_diff=8': [0.748, 0.822, 0.592, 0.619, 0.851, 0.720, 0.609, 0.634, 0.637],
    'seed=77\nsub_diff=8': [0.748, 0.661, 0.764, 0.619, 0.704, 0.742, 0.754, 0.509],
    'k_floor=1.5': [0.704, 0.674, 0.846, 0.635, 0.512, 0.677, 0.692],
}

fig, ax = plt.subplots(figsize=(16, 5))
positions = []
colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#d35400', '#e84393', '#9b59b6', '#95a5a6']
for i, (label, vals) in enumerate(strategies.items()):
    x = np.random.normal(i, 0.08, len(vals))
    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3, color=colors[i], edgecolors='none')
    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)
    positions.append(i)

ax.set_xticks(positions)
ax.set_xticklabels(list(strategies.keys()), fontsize=8)
ax.set_ylabel('raw R²', fontsize=12)
ax.set_ylim(-0.05, 0.95)
ax.axhline(y=0.869, color='#f39c12', linestyle='--', alpha=0.3, label='Peak R² = 0.869')
ax.grid(axis='y', alpha=0.3)
ax.legend(fontsize=8)
plt.tight_layout()
plt.show()
```

### Block 4: Seed Sensitivity, then sub_diff=7 Breakthrough (Iter 37--48)

Block 4 first tested combinations with `lr_sub=0.001` and revealed seed sensitivity ($\pm$0.2 R²). Then `sub_diff=7` (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 10 (iter 37--40) | `sub_norm=2.0`, `lr_sub=0.002`, `lr_node=0.002`, L1=0 | 0.662 | All combinations with `lr_sub=0.001` hurt R² vs baseline |
| 11 (iter 41--44) | seed=123, `aug=4500`, `sub_diff=3`, `lr_k=0.007` | 0.690 | `aug=4500` most stable ($\alpha = 0.94$); seed=123 crashed to R²=0.49 |
| 12 (iter 45--48) | **`sub_diff=7`**, `lr_k=0.004`, `hidden_dim=128`, `batch_size=16` | **0.736** | `sub_diff=7` new best; wider/deeper MLP and larger batch hurt |

Iteration 45 (`sub_diff=7`, `aug=4500`) achieved R² = 0.736 with $\alpha = 0.90$ and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46--48 confirmed that `lr_k=0.004` is too slow, `hidden_dim_sub=128` allows degenerate solutions, and `batch_size=16` degrades convergence.

### Block 5: Confirming Tight Optimization Bounds (Iter 49--60)

Block 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 13 (iter 49--52) | `aug=5000`, `sub_diff=8`, seed=123, `n_layers=4` | 0.696 | `aug=5000` hurts (confirmed twice); `sub_diff=7` improves seed robustness ($\pm$0.08 vs $\pm$0.24) |
| 14 (iter 53--56) | `aug=4250`, `sub_diff=6`, seed=123+`aug=4000`, `lr_sub=0.0015` | 0.701 | All variations hurt; `sub_diff=6` too weak, `lr_sub=0.0015` too high |
| 15 (iter 57--60) | seed=123+`aug=3500`, **seed=99**, L1=0.5, `sub_norm=0.5` | **0.718** | seed=99 promising (R²=0.72, $\alpha=0.92$); `sub_norm=0.5` confirmed essential principle |

Tight bounds confirmed: `aug=4500` (not 4000, 4250, or 5000), `sub_diff=7` (not 5, 6, or 8), `lr_sub=0.001` (not 0.0015 or 0.002), `lr_k=0.005` (not 0.004 or 0.007). `sub_diff=7` improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with `sub_diff=5`) to 0.08.

### Block 6: High Variance Discovery (Iter 61--72)

Block 6 explored new seeds and k_floor variations, then discovered that training has **high intrinsic variance** — an exact replica of the best configuration (iter 45) achieved only R²=0.66 instead of 0.74.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 16 (iter 61--64) | seed=7, seed=99+`aug=4250`, `lr_node=0.0005`, **`k_floor=1.5`** | **0.704** | `k_floor=1.5` promising (R²=0.70, $\alpha=0.92$); seed=99 very sensitive to aug |
| 17 (iter 65--68) | `k_floor=1.25`, `k_floor=1.5`+seed=99, `aug=4750`, `sub_diff=6`+`k_floor=1.5` | 0.674 | **k_floor response is non-monotonic**: 1.25 < 1.0 and 1.5; `aug=4750` hurts |
| 18 (iter 69--72) | Exact replica of iter 45, `lr_k=0.0045`, `hidden_dim_node=32`, `sub_norm=1.5` | 0.658 | **HIGH VARIANCE**: replica got R²=0.66 vs original 0.74; MLP$_{\text{sub}}$ c² failure mode |

The high variance discovery is significant: identical configurations can produce R² values differing by 0.08, meaning many earlier "improvements" may have been within noise. This makes seed selection and reproducibility central concerns.

### Block 7: seed=77 Breakthrough (Iter 73--84)

Block 7 explored new seeds and found that **seed=77 is a "golden seed"** — achieving R²=0.748 (iter 80) and then R²=0.764 with `sub_diff=8` (iter 82), surpassing the previous best.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 19 (iter 73--76) | seed=123, `lr_sub=0.0012`, `sub_diff=9`, `aug=4000` | 0.722 | seed=123 improved to R²=0.72 (vs 0.66 earlier); all other variations hurt |
| 20 (iter 77--80) | seed=7, `k_floor=1.5`, `n_layers=4`, **seed=77** | **0.748** | **seed=77 new best seed** (R²=0.748, 12 outliers); `n_layers=4` confirmed harmful |
| 21 (iter 81--84) | Replicate seed=77, **seed=77+`sub_diff=8`**, seed=78, `aug=5000` | **0.764** | **seed=77+sub_diff=8 = new global best**; replica got R²=0.66 (variance); seed=78 poor (R²=0.39) |

The key finding: **optimal `sub_diff` is seed-dependent**. `sub_diff=8` hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76). Adjacent seeds (78, 79) do not share seed=77's properties — "golden seeds" are rare and unpredictable.

### Block 8: seed=79 Breakthrough + aug=5000 (Iter 85--96)

Block 8 discovered **seed=79 as a new golden seed** (R²=0.748, iter 87) and then broke the R²=0.85 barrier when combined with `aug=5000` (R²=0.851, iter 96) — refuting the established principle that `aug>4500` always hurts.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 22 (iter 85--88) | replicate, seed=76, **seed=79**, seed=42+`sub_diff=8` | **0.748** | **seed=79 is a golden seed** (R²=0.75); seed=42+`sub_diff=8` works with `lr_sub=0.001` (R²=0.72, refutes earlier principle) |
| 23 (iter 89--92) | **replicate seed=79**, `sub_diff=9`, seed=80, seed=77+`sub_diff=9` | **0.822** | **seed=79 replicate broke 0.80 barrier** (R²=0.82); `sub_diff=9` hurts all golden seeds (3× confirmed) |
| 24 (iter 93--96) | replicate, `sub_diff=7`, seed=81, **seed=79+`aug=5000`** | **0.851** | **NEW GLOBAL BEST R²=0.851**; `aug=5000` helps seed=79 (principle refuted for this seed) |

The block revealed **extreme training variance** ($\pm$0.2 R²): the same seed=79+`sub_diff=8` config gave R²=0.82 (iter 89) and R²=0.59 (iter 93). The `aug=5000` result (iter 96, R²=0.851, 11 outliers, $\alpha$=0.94, slope=0.99) was the strongest single run at the time.

### Block 9: seed=77+sub_diff=6 Discovery (Iter 97--108)

Block 9 explored aug boundaries for golden seeds and discovered that **`sub_diff=6` is optimal for seed=77** — achieving R²=0.804 (iter 106), the second-best result at the time.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 25 (iter 97--100) | replicate seed=79, `aug=5500`, seed=42+`aug=5000`, seed=79+`sub_diff=7` | 0.742 | `aug=5500` hurts seed=79 (R²=0.61); `sub_diff=7` catastrophic for seed=79 (R²=0.47) |
| 26 (iter 101--104) | seed=77+`aug=4500`, replicate seed=79, seed=42+`aug=5000`, seed=77+`sub_diff=7` | **0.764** | seed=77 consistent R²≈0.75; seed=79 extreme variance (0.85→0.63) |
| 27 (iter 105--108) | replicate, **seed=77+`sub_diff=6`**, seed=42+`sub_diff=7`, seed=77+`sub_diff=9` | **0.804** | **sub_diff=6 is optimal for seed=77**; `sub_diff=9` confirmed harmful (3× refuted) |

The key insight: optimal `sub_diff` is not just seed-dependent but follows a pattern — lower values (6) work best for seed=77 while higher values (8) suit seed=79. `aug=5500` was found to hurt seed=79 but would later prove essential for seed=77.

### Block 10: Global Best R²=0.869 (Iter 109--120)

Block 10 tested `sub_diff=6` cross-seed transfer and then achieved the **global best R²=0.869** at iteration 116 with seed=77+`sub_diff=6`+`aug=5500`.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 28 (iter 109--112) | replicate iter 106, seed=77+`sub_diff=5`, seed=79+`sub_diff=6`, seed=42+`sub_diff=6` | 0.655 | **Extreme variance**: replica of R²=0.80 got R²=0.55; `sub_diff=6` doesn't transfer to seed=79/42 |
| 29 (iter 113--116) | replicate seed=79, seed=77+`sub_diff=7`+`aug=5000`, seed=79+`sub_diff=7`, **seed=77+`sub_diff=6`+`aug=5500`** | **0.869** | **NEW GLOBAL BEST** R²=0.869 (iter 116); aug=5500 helps seed=77 (refutes "aug>4500 hurts") |
| 30 (iter 117--120) | replicate iter 116, `aug=5750`, `sub_diff=5`, `sub_diff=7`+`aug=5500` | **0.812** | `aug=5750` hurts (R²=0.52); `sub_diff=7`+`aug=5500` also strong (R²=0.81) |

Iteration 116 (R²=0.869, 10 outliers, $\alpha$=0.93) established the current global best. The `aug=5500` finding refuted the principle that longer training always hurts — it is seed-dependent. However, `aug=5750` overshoots even for seed=77.

### Block 11: Robustness Testing (Iter 121--132)

Block 11 systematically tested robustness of the top configurations and discovered that **`k_floor=1.5` combined with `sub_diff=7` can achieve R²=0.846** (iter 132, second-best overall).

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 31 (iter 121--124) | replicate `sub_diff=7`, `aug=5250`, seed=42+`sub_diff=7`, seed=77+`sub_diff=8` | 0.734 | `sub_diff=7` more robust than `sub_diff=6` (variance ~0.08 vs ~0.21); `sub_diff=8` hurts seed=77 |
| 32 (iter 125--128) | replicate, `aug=5750`, seed=79+`sub_diff=7`, seed=77+`sub_diff=6` | **0.782** | `aug=5750` hurts `sub_diff=7` too; `sub_diff=7` doesn't transfer to seed=79 |
| 33 (iter 129--132) | worst replicate, `aug=5250`, seed=42+`sub_diff=6`, **seed=77+`sub_diff=7`+`k_floor=1.5`** | **0.846** | **2ND BEST R²=0.846** with `k_floor=1.5`; extreme variance persists (R²=0.51--0.85) |

The `sub_diff=7` configuration proved more robust than `sub_diff=6` for seed=77: R² range 0.73--0.81 (variance ~0.08) vs 0.55--0.87 (variance ~0.21). The trade-off is clear — `sub_diff=6` has a higher ceiling but lower floor.

### Block 12: k_floor=1.5 Variance Confirmed (Iter 133--136)

Block 12 tested whether `k_floor=1.5` reliably improves R² and found that **variance dominates configuration differences**.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 34 (iter 133--136) | replicate iter 132, `sub_diff=6`+`k_floor=1.5`, seed=42+`k_floor=1.5`, `k_floor=2.0` | 0.692 | Replicate R²=0.64 vs 0.85 (variance ~0.21); `sub_diff=6` requires `k_floor=1.0`; `k_floor=2.0` not catastrophic |

The main conclusion: with $\pm$0.2--0.3 R² training variance, **configuration differences below ~0.15 R² are indistinguishable from noise**.

### UCB Exploration Tree

![UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the `k_floor=1.0` breakthrough (node 14) and converged on the `aug=4000` regime. The tree has since grown to 136 nodes; the overall best is node 116 (R² = 0.869).](figures/rank_50/ucb_tree.png){#fig-ucb-tree}

### Established Principles

The LLM's persistent memory has accumulated these validated principles after 136 iterations:

1. **`coeff_MLP_sub_norm=1.0` is essential** — enables correct MLP shapes: c² becomes quadratic, MLP$_{\text{node}}$ activates. `sub_norm=0.5` and `sub_norm=1.5` both hurt (iter 60, 72).
2. **`coeff_k_floor=1.0`--`1.5` is the optimal range** — R² jumped from 0.06 to 0.51 (10× improvement) with `k_floor=1.0`. `k_floor=1.5`+`sub_diff=7` achieved R²=0.85 (iter 132, 2nd best). Response is **non-monotonic**: `k_floor=1.25` < both 1.0 and 1.5 (iter 65). `k_floor` response is `sub_diff`-dependent: `sub_diff=6` requires `k_floor=1.0` (iter 134).
3. **Optimal `aug` is seed-dependent** — `aug=5500` is optimal for seed=77 (R²=0.87, iter 116), `aug=5000` for seed=79 (R²=0.85, iter 96), `aug=4500` for seed=42. `aug=5750` hurts all configs (iter 118, 126). `aug=5500` is the hard ceiling.
4. **`lr_k=0.005` is optimal** — lower (0.003, 0.004, 0.0045) too slow, higher (0.007, 0.01) destabilizes (confirmed across 6 iterations)
5. **`lr_sub=0.001` is optimal** — 2× increase from 0.0005 broke the R² plateau; `lr_sub=0.002` too high (iter 38); `lr_sub=0.0015` hurts (iter 56); `lr_sub=0.0012` hurts (iter 74)
6. **`lr_node=0.001` is optimal** — `lr_node=0.005` destabilizes (iter 11); `lr_node=0.0005` hurts (iter 63)
7. **Optimal `sub_diff` is seed-specific** — `sub_diff=6` for seed=77 (R²=0.87, iter 116), `sub_diff=7` for seed=42 (R²=0.74, iter 45), `sub_diff=8` for seed=79 (R²=0.85, iter 96). `sub_diff=9` hurts all golden seeds (3× confirmed). `sub_diff=5` underperforms (iter 119).
8. **Training has extreme intrinsic variance ($\pm$0.2--0.3 R²)** — exact replicas: iter 132 R²=0.85 vs iter 133 R²=0.64 ($\Delta$=0.21); iter 106 R²=0.80 vs iter 109 R²=0.55 ($\Delta$=0.25). Configuration differences below ~0.15 R² are indistinguishable from noise.
9. **seed=77 is the best seed** — R²=0.869 with `sub_diff=6`+`aug=5500` (iter 116, global best). `sub_diff=7` more robust (variance ~0.08) but lower ceiling. seed=79 second best (R²=0.851, iter 96) but higher variance. Adjacent seeds (76, 78, 80, 81) are poor — "golden seeds" are rare.
10. **`batch_size=8` is optimal** — `batch_size=16` hurts R² (iter 48)
11. **Default MLP architecture is optimal** — `hidden_dim_sub=128` allows degenerate solutions (iter 47); `hidden_dim_node=32` significantly worse (iter 71); `n_layers_sub=4` hurts (iter 52, 79)
12. **`coeff_MLP_node_L1=1.0` is optimal** — L1=0.0 + long training harmful (iter 22, 29, 40); L1=0.5 hurts (iter 59)
13. **`sub_diff=7` is the most robust setting** — variance ~0.08 R² for seed=77 (range 0.73--0.81) vs `sub_diff=6`'s ~0.21 (range 0.55--0.87). Does not transfer well to seed=42 or seed=79.
14. **MLP$_{\text{node}}$ remains flat across all 136 iterations** — homeostasis $-\lambda(c - c^{\text{base}})$ is never learned, regardless of configuration

### Refuted Hypotheses

| Hypothesis | Evidence |
|-----------|---------|
| Recurrent training breaks degeneracy | No R² improvement, 3.5× slower (iter 13, 34) |
| Smaller MLP improves k recovery | Worst R² = 0.011 (iter 15) |
| Higher `lr_node` activates MLP$_{\text{node}}$ | `lr_node=0.005` hurts (iter 11); 0.002 no effect (iter 26, 39) |
| Different seed breaks degeneracy | Same MLP$_{\text{node}}$ flatness (iter 27); R² variance $\pm$0.2 (iter 41) |
| Stronger monotonicity (`sub_diff=10`) helps | R² dropped to 0.41 (iter 32) |
| Weaker monotonicity (`sub_diff=3`) helps | R² dropped to 0.61 (iter 43) |
| Smaller batch size helps convergence | R² dropped (iter 28) |
| `aug=5000` continues to improve | Confirmed 3×: iter 25 (0.65), iter 49 (0.70), iter 84 (0.69) |
| `lr_sub=0.002` improves over 0.001 | R² dropped from 0.73 to 0.52 (iter 38) |
| `sub_norm=2.0` improves R² | Improves $\alpha$ but hurts R² (iter 30, 37) |
| Combining `lr_sub=0.001` with other changes | All combinations hurt R² vs baseline (iter 37--40) |
| Wider MLP$_{\text{sub}}$ (`hidden_dim=128`) helps | R² dropped to 0.56 — allows degenerate solutions (iter 47) |
| Larger `batch_size=16` stabilizes gradients | R² dropped to 0.56 (iter 48) |
| `sub_diff=8` hurts all seeds | Hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76) |
| `sub_diff=6` is better than 7 | R² dropped from 0.74 to 0.56 (iter 54) |
| Deeper MLP$_{\text{sub}}$ (`n_layers=4`) helps | R² dropped to 0.55 (iter 52); confirmed iter 79 (R²=0.47) |
| `lr_sub=0.0015` (intermediate) helps | R² dropped from 0.74 to 0.60 (iter 56) |
| `lr_k=0.004` gives finer convergence | R² dropped to 0.48 — too slow (iter 46) |
| `sub_norm=0.5` helps | R² dropped from 0.74 to 0.60 (iter 60) |
| Intermediate `k_floor=1.25` is optimal | R²=0.56, worse than both 1.0 and 1.5 — non-monotonic (iter 65) |
| `aug=4750` within safe range | R² dropped to 0.66 despite $\alpha$=0.96 (iter 67) |
| `sub_norm=1.5` better than 1.0 | R²=0.55 vs 0.66 (iter 72) |
| `hidden_dim_node=32` simpler is better | R²=0.47, significantly worse (iter 71) |
| `lr_sub=0.0012` helps MLP$_{\text{sub}}$ | R²=0.58 vs 0.66 (iter 74) |
| seed=42 is the best seed | seed=79 achieved R²=0.851 (iter 96), seed=77 R²=0.764 (iter 82) |
| Adjacent seeds share properties | seed=76 R²=0.51 (iter 86), seed=78 R²=0.39 (iter 83), seed=80 R²=0.57 (iter 91), seed=81 R²=0.55 (iter 95) — all poor |
| `k_floor=1.5` is reproducible | Same config gave R²=0.70 (iter 64) and R²=0.51 (iter 78) |
| `sub_diff=8` hurts seed=42 | Refuted: iter 88 got R²=0.72 with seed=42+`sub_diff=8` (key: `lr_sub=0.001` not 0.0005) |
| `sub_diff=9` forces c² quadratic | Hurt all golden seeds: seed=79 0.75→0.64 (iter 90), seed=77 0.76→0.70 (iter 92), 3× confirmed |
| `aug=5000` always hurts R² | Refuted for seed=79: R²=0.851 with `aug=5000` (iter 96); then `aug=5500` helps seed=77 to R²=0.869 (iter 116) |
| `aug=5500` hurts all seeds | Refuted: seed=77+`sub_diff=6`+`aug=5500` gave R²=0.869 (iter 116, global best) |
| `sub_diff=6` transfers across seeds | `sub_diff=6` doesn't help seed=79 (R²=0.65, iter 111) or seed=42 (R²=0.59, iter 112) |
| `sub_diff=7` transfers to all seeds | seed=42 R²=0.57 (iter 123), seed=79 R²=0.54 (iter 127) |
| `sub_diff=8` works for seed=77 | R²=0.51 (iter 124) vs `sub_diff=6` R²=0.87 and `sub_diff=7` R²=0.81 |
| `aug=5750` tolerates any `sub_diff` | Hurts both `sub_diff=6` (iter 118, R²=0.52) and `sub_diff=7` (iter 126, R²=0.68) |
| `k_floor=1.5` reduces variance | Same config: iter 132 R²=0.85 vs iter 133 R²=0.64 (variance ~0.21 persists) |
| `k_floor=1.5` helps `sub_diff=6` | R²=0.51 (iter 134) vs `k_floor=1.0` R²=0.87 (iter 116) |
| `k_floor=2.0` is catastrophically worse | R²=0.69 (iter 136), within variance range of `k_floor=1.5` |

### Open Questions

1. **Is R² $\geq$ 0.90 achievable?** After 136 iterations, the best R²=0.869 (seed=77+sub_diff=6+aug=5500). Training variance of $\pm$0.2--0.3 means a lucky run could reach 0.90+, but systematic improvement requires reducing variance.
2. **Can variance be reduced?** The dominant bottleneck is now stochastic variance ($\pm$0.2--0.3 R²), not hyperparameter tuning. Ensemble averaging, longer training with learning rate scheduling, or gradient accumulation could help.
3. **Are there more "golden seeds"?** Only seeds 77, 79, and 42 are confirmed productive. Adjacent seeds (76, 78, 80, 81) are all poor. What makes a seed "golden" remains unknown.
4. **Why is the k_floor response non-monotonic?** `k_floor=1.25` is worse than both 1.0 and 1.5 (iter 65). The mechanism is unknown.
5. **Is the remaining 13% error from identifiability issues?** Multiple $k$ combinations may produce similar $dc/dt$, setting a fundamental ceiling on single-run recovery.

## Why Homeostasis Is Not Learned

Across all 136 iterations, MLP$_{\text{node}}$ remains flat — the homeostasis function $-\lambda_t(c_i - c_i^{\text{baseline}})$ is never recovered. This is not a hyperparameter issue. It is a structural limitation of single-step ($t \to t+1$) training.

### The scale mismatch problem

The GNN predicts $dc/dt$ at each time step:

$$
\frac{dc_i}{dt} = \underbrace{\text{MLP}_{\text{node}}(c_i, a_i)}_{\text{homeostasis}} + \underbrace{\sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(c_k, |S_{kj}|)}_{\text{reaction}}
$$

The reaction term dominates the instantaneous $dc/dt$ — it drives the fast oscillatory dynamics. Homeostasis acts as a slow restoring force that only manifests over many time steps: it prevents concentrations from drifting away from baseline. In a single-step loss $\| \hat{y}_{t+1} - y_{t+1} \|^2$, the gradient signal from homeostasis is negligible compared to the reaction term. The optimizer has no reason to learn it.

### Homeostasis is an integral problem

Homeostasis determines the **long-term trajectory envelope**, not the instantaneous derivative. Its effect accumulates over time:

$$
c_i(t + T) \approx c_i(t) + \int_t^{t+T} \left[ -\lambda_i (c_i - c_i^{\text{base}}) + \text{reaction terms} \right] d\tau
$$

A model trained on $t \to t+1$ can achieve low loss by fitting the dominant reaction signal and ignoring the small homeostatic correction. Over many steps this error accumulates — but the single-step loss never sees it.

### Proposed approach: two-phase training

To recover homeostasis, we propose a two-phase training scheme:

1. **Phase 1 — Reaction recovery** ($t \to t+1$): Train as currently done. Recover $k_j$, MLP$_{\text{sub}}$, and the stoichiometric structure. Freeze these parameters.

2. **Phase 2 — Homeostasis recovery** (recurrent, $t \to t+T$): With the reaction term frozen, train only MLP$_{\text{node}}$ and embeddings $a_i$ using multi-step rollout. The recurrent loss forces the model to match trajectories over $T$ steps, where the homeostatic drift becomes visible.

This separates the two learning problems by time scale: fast reactions are learned from instantaneous gradients, slow homeostasis from trajectory matching. The key insight is that recurrent training is not needed for $k_j$ recovery (it was tried and failed — iter 13, 34), but may be essential specifically for homeostasis once the reaction parameters are frozen.

### Alternative approach: residual-based supervision

The kinograph residual (@fig-kinograph-best, bottom-left) directly reveals what the single-step model cannot learn. After Phase 1, we can roll out the learned model autoregressively:

$$
\hat{c}_i(t+1) = \hat{c}_i(t) + \Delta t \cdot \left[ \sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(\hat{c}_k, |S_{kj}|) \right]
$$

The rollout trajectory $\hat{c}(t)$ drifts from the observation $c(t)$ because the missing homeostatic restoring force is not applied at each step. The accumulated residual

$$
r_i(t) = c_i(t) - \hat{c}_i(t)
$$

is precisely the integrated effect of the missing slow terms — homeostasis, external sources, and degradation. This residual provides a direct supervision signal for Phase 2 without requiring backpropagation through time: we can compute a per-step target

$$
\frac{r_i(t+1) - r_i(t)}{\Delta t} \approx \text{MLP}_{\text{node}}(c_i(t), a_i)
$$

and train MLP$_{\text{node}}$ with a standard single-step loss on this derived target. This avoids the instabilities of recurrent training while still capturing the long-term dynamics that the reaction-only model structurally cannot predict.

### Direct evidence: the one-step derivative residual is the homeostasis signal

The one-step derivative diagnostic feeds the **true concentration** at each timestep and records the GNN's predicted $dc/dt$. Since MLP$_{\text{node}} \approx 0$ across all 136 iterations, the prediction is purely the reaction term:

$$
\widehat{\frac{dc_i}{dt}} = \underbrace{0}_{\text{MLP}_{\text{node}}} + \sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(c_k, |S_{kj}|)
$$

The residual between prediction and ground truth is therefore:

$$
\widehat{\frac{dc_i}{dt}} - \frac{dc_i}{dt} = -\text{MLP}_{\text{node}}^{\text{true}}(c_i, a_i) = \lambda_i(c_i - c_i^{\text{base}})
$$

This is the **negative of the unlearned homeostasis term**, directly observable per metabolite over time.

![One-step derivative residual traces (Iteration 96, best model). Each trace shows $\widehat{dc/dt} - dc/dt$ for a single metabolite — the signal that single-step training structurally cannot capture. The smooth, slow-varying shape is consistent with homeostatic regulation $\lambda(c - c^{\text{base}})$. Metabolites with the highest MAE (36, 99, 10) correspond to the strongest homeostatic restoring forces, not GNN prediction errors.](figures/rank_50/deriv_residual_traces.png){#fig-deriv-residual-traces}

The derivative residual traces confirm three properties expected of the homeostasis signal:

1. **Smooth and slow-varying** — the traces lack the fast oscillatory structure of the reaction term, consistent with a restoring force that operates on longer timescales.
2. **Non-zero mean per metabolite** — each metabolite has a persistent bias, reflecting the time-averaged homeostatic drive $\lambda_i(c_i - c_i^{\text{base}})$.
3. **Metabolite-specific amplitude** — the MAE varies by metabolite (0.02 to 0.14), suggesting heterogeneous homeostatic strength $\lambda_i$ across the network.

These residual traces provide a ready-made supervision signal for Phase 2: rather than requiring backpropagation through time, MLP$_{\text{node}}$ can be trained directly on $-(\widehat{dc/dt} - dc/dt)$ at each timestep, using the frozen Phase 1 reaction model.

This approach is an instance of the **Universal Differential Equations** framework ([Rackauckas et al., 2021](https://arxiv.org/abs/2001.04385)), where a partially known ODE is augmented with a neural network that learns the missing terms from data. Here, the known part is the reaction dynamics $\sum_j S_{ij} k_j \prod_k \text{MLP}_{\text{sub}}$, and the unknown part is the homeostatic correction learned by MLP$_{\text{node}}$. The residual-based supervision strategy is also related to **PDE-Refiner** ([Lippe et al., NeurIPS 2023](https://arxiv.org/abs/2308.05732)), which uses iterative refinement on rollout residuals to recover low-amplitude dynamics that single-pass neural solvers miss — analogous to the small homeostatic signal masked by dominant reactions.

### Related work

The difficulty of learning slow dynamics from single-step training is well established in the literature:

- **Teacher forcing and long-term dependencies.** [Williams & Zipser (1989)](https://direct.mit.edu/neco/article/1/2/270/5490/) introduced teacher forcing for RNNs: feeding ground-truth inputs at each step. [Bengio et al. (1994)](https://ieeexplore.ieee.org/document/279181) showed that gradient-based learning of long-term dependencies is fundamentally difficult because short-term gradient contributions dominate, exactly the mechanism that prevents our GNN from learning homeostasis.

- **Scheduled sampling.** [Bengio et al. (2015)](https://arxiv.org/abs/1506.03099) proposed a curriculum strategy that gradually transitions from teacher forcing (single-step) to free-running (multi-step) prediction, reducing the train–inference discrepancy. Our two-phase proposal is conceptually related: Phase 1 is teacher-forced, Phase 2 uses multi-step rollout.

- **Multiple shooting for Neural ODEs.** [Massaroli et al. (2021)](https://arxiv.org/abs/2106.03885) introduced differentiable multiple shooting layers that parallelize trajectory integration. [Turan & Jäschke (2021)](https://arxiv.org/abs/2109.06786) showed that standard Neural ODE fitting on oscillatory data produces "flattened" trajectories — multiple shooting recovers the true dynamics by breaking long horizons into segments. This directly addresses the failure mode we observe.

- **Stiff Neural ODEs.** [Kim et al. (2021)](https://arxiv.org/abs/2103.15341) showed that learning neural ODEs for systems with widely separated time scales (stiff systems) requires proper output scaling and stabilized gradients — the fast dynamics otherwise dominate training, exactly as in our reaction-vs-homeostasis scale mismatch.

- **Multi-scale separation in dynamical systems.** [Fenichel (1979)](https://doi.org/10.1016/0022-0396(79)90152-9) established geometric singular perturbation theory: for systems with fast and slow time scales, the fast subsystem is solved first, then the dynamics are reduced to a slow manifold. Our two-phase training mirrors this decomposition — Phase 1 recovers the fast reaction dynamics, Phase 2 learns the slow homeostatic manifold.

- **Latent timescales in Neural ODEs.** [Gupta et al. (2024)](https://arxiv.org/abs/2403.02224) showed that training trajectory length directly controls the timescales a Neural ODE can recover: longer trajectories are needed to capture slower dynamics. This supports the need for multi-step rollout in Phase 2.

The scale mismatch / integral argument is essentially the same observation that Bengio (1994) made for RNNs and Kim et al. (2021) made for stiff ODEs. The two-phase proposal mirrors Fenichel's fast-then-slow decomposition. The novelty here is applying this reasoning to GNN-based metabolic network recovery, where the bipartite graph structure and the identifiability of rate constants $k_j$ add domain-specific constraints.
