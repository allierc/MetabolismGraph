---
title: "Results"
date: last-modified
date-format: "MMMM D, YYYY"
---

## Rate Constant Recovery — Oscillatory Regime (rank 50)

The LLM-driven exploration engine is running on the **oscillatory regime** (activity rank $\sim50$): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix $\mathbf{S}$ frozen from ground truth. The goal is to recover the 256 rate constants $k_j \in [10^{-2}, 10^{-1}]$ by optimizing training hyperparameters through UCB tree search with 4 parallel slots.

**32 iterations completed** across 3 blocks (8 batches). The exploration is ongoing.

### Simulation Setup

::: {layout-ncol=2}

![Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.](figures/rank_50/concentrations.png){#fig-concentrations}

![Stoichiometric matrix $\mathbf{S}$ (100 metabolites $\times$ 256 reactions). Red = products (+1), blue = substrates (--1). 100% autocatalytic 3-cycles.](figures/rank_50/stoichiometry.png){#fig-stoichiometry}

:::

### Metrics

The primary metric is **raw R²** computed on all 256 reactions (after MLP$_{\text{sub}}$ scalar correction). The **trimmed R²** excludes outlier reactions ($|\Delta \log_{10} k| > 0.3$) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.

### R² Trajectory Across 32 Iterations

```{python}
#| code-fold: true
#| label: fig-iterations
#| fig-cap: "Raw R² (bars) and outlier count (dots) across 32 iterations, colored by block. The k_floor breakthrough at iteration 14 lifted R² from 0.07 to 0.51, and longer training pushed it to 0.69 by iteration 21. Block 3 failed to break the plateau."

import matplotlib.pyplot as plt
import numpy as np

iters = list(range(1, 33))
r2 = [
    # Block 1 (Iter 1-12): Initial exploration
    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep
    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough
    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation
    # Block 2 (Iter 13-24): k_floor breakthrough
    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough
    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor
    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best
    # Block 3 (Iter 25-32): Plateau
    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns
    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches
]
outliers = [
    43, 47, 45, 53,
    61, 36, 45, 32,
    27, 38, 57, 30,
    28, 33, 36, 33,
    17, 24, 26, 35,
    16, 29, 19, 24,
    18, 18, 15, 17,
    19, 14, 21, 21,
]

# Color by block
colors = []
for i, r in enumerate(r2):
    if i < 12:
        colors.append('#3498db')  # blue: Block 1
    elif i < 24:
        colors.append('#2ecc71')  # green: Block 2
    else:
        colors.append('#e67e22')  # orange: Block 3

fig, ax1 = plt.subplots(figsize=(14, 5))

bars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)
ax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.4, label='Best R² = 0.690')

# Block separators and labels
ax1.axvline(x=12.5, color='gray', linestyle=':', alpha=0.4)
ax1.axvline(x=24.5, color='gray', linestyle=':', alpha=0.4)
ax1.text(6.5, 1.0, 'Block 1', ha='center', fontsize=9, color='#3498db', fontweight='bold')
ax1.text(18.5, 1.0, 'Block 2', ha='center', fontsize=9, color='#2ecc71', fontweight='bold')
ax1.text(28.5, 1.0, 'Block 3', ha='center', fontsize=9, color='#e67e22', fontweight='bold')

# Annotate key events
ax1.annotate('sub_norm=1.0', xy=(6, 0.067), xytext=(6, 0.18),
            arrowprops=dict(arrowstyle='->', color='#3498db'), fontsize=7, ha='center', color='#3498db')
ax1.annotate('k_floor=1.0\nBREAKTHROUGH', xy=(14, 0.508), xytext=(14, 0.28),
            arrowprops=dict(arrowstyle='->', color='#2ecc71'), fontsize=7, ha='center', color='#2ecc71', fontweight='bold')
ax1.annotate('aug=4000\nBEST', xy=(21, 0.690), xytext=(21, 0.82),
            arrowprops=dict(arrowstyle='->', color='#2ecc71'), fontsize=7, ha='center', color='#2ecc71', fontweight='bold')

ax1.set_xlabel('Iteration')
ax1.set_ylabel('rate_constants R² (raw)')
ax1.set_ylim(-0.02, 1.05)
ax1.set_xticks([1, 4, 8, 12, 14, 17, 21, 24, 28, 32])

# outlier count on secondary axis
ax2 = ax1.twinx()
ax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')
ax2.set_ylabel('outlier count', color='#e74c3c')
ax2.tick_params(axis='y', labelcolor='#e74c3c')
ax2.set_ylim(0, 70)

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)

ax1.set_title('UCB Exploration: Rate Constant Recovery (32 iterations)')
plt.tight_layout()
plt.show()
```

### Block 1: Initial Exploration (Iter 1--12)

All 12 iterations achieved raw R² < 0.07. The key discovery was that `coeff_MLP_sub_norm=1.0` is essential — it corrects the MLP$_{\text{sub}}$ function shapes (c² becomes quadratic instead of linear) and enables MLP$_{\text{node}}$ to learn homeostasis.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 1 (iter 1--4) | lr sweep | 0.044 | All failed; MLP$_{\text{node}}$ dead, MLP$_{\text{sub}}$ c² linear |
| 2 (iter 5--8) | `sub_norm=1.0` | **0.067** | MLP$_{\text{sub}}$ normalization is the single most effective change |
| 3 (iter 9--12) | combine best | 0.061 | MLP$_{\text{node}}$ activated; `lr_node=0.005` hurts |

### Block 2: k_floor Breakthrough (Iter 13--24)

The `coeff_k_floor=1.0` penalty at iteration 14 produced a **10× improvement** in R² (0.06 → 0.51) by preventing outlier $\log k$ values from drifting below the true minimum. Longer training then pushed R² to 0.69.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 4 (iter 13--16) | `k_floor=1.0` | **0.508** | Breakthrough — R² jumped 10× |
| 5 (iter 17--20) | `aug=3000` | **0.642** | Longer training + k_floor synergistic |
| 6 (iter 21--24) | `aug=4000` | **0.690** | Best overall; $\alpha = 0.85$, 16 outliers |

**Best result — Iteration 21**: `data_augmentation_loop=4000`, `coeff_k_floor=1.0`, `coeff_MLP_sub_norm=1.0`

| Metric | Value |
|--------|-------|
| Raw R² | **0.690** |
| Trimmed R² | 0.962 |
| Outliers | 16 / 256 (6.3%) |
| Slope | 0.976 |
| $\alpha$ | 0.85 |
| test_pearson | 0.33 |

### Block 3: Plateau at R² ≈ 0.69 (Iter 25--32)

Eight additional iterations failed to break the R² = 0.69 plateau. Every variation — longer training (`aug=5000`), higher `lr_node`, different seeds, smaller batches, stronger regularization — produced equal or worse R².

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 7 (iter 25--28) | `aug=5000`, seed, batch_size | 0.652 | `aug=5000` hurt R² but $\alpha=0.95$ (best ever) |
| 8 (iter 29--32) | L1=0, `sub_norm=2.0`, lr_k | 0.619 | `sub_norm=2.0`: fewest outliers (14), best slope (0.99) |

```{python}
#| code-fold: true
#| label: fig-strategies
#| fig-cap: "Impact of key strategies on R². Each dot is one iteration. The k_floor penalty is the single most important factor, with training duration as the second lever."

import matplotlib.pyplot as plt
import numpy as np

strategies = {
    'No k_floor\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],
    'k_floor=1.0\naug≤2000': [0.508],
    'k_floor=1.0\naug=3000': [0.638, 0.419, 0.658, 0.559, 0.470, 0.373],
    'k_floor=1.0\naug=4000': [0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409],
    'k_floor=1.0\naug=5000': [0.652],
}

fig, ax = plt.subplots(figsize=(10, 5))
positions = []
for i, (label, vals) in enumerate(strategies.items()):
    x = np.random.normal(i, 0.08, len(vals))
    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3)
    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)
    positions.append(i)

ax.set_xticks(positions)
ax.set_xticklabels(list(strategies.keys()), fontsize=9)
ax.set_ylabel('Raw R²')
ax.set_ylim(-0.05, 0.85)
ax.axhline(y=0.690, color='green', linestyle='--', alpha=0.3)
ax.set_title('R² by Strategy Group')
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Established Principles

The LLM's persistent memory has accumulated these validated principles after 32 iterations:

1. **`coeff_MLP_sub_norm=1.0` is essential** — enables correct MLP shapes: c² becomes quadratic, MLP$_{\text{node}}$ activates
2. **`coeff_k_floor=1.0` is critical** — R² jumped from 0.06 to 0.51 (10× improvement). `k_floor=2.0` too strong.
3. **Longer training helps (up to `aug=4000`)** — aug=2000→3000→4000 consistently improves R². aug=5000 hurts R².
4. **`lr_k=0.005` is optimal** — lower (0.003) too slow, higher (0.01) destabilizes
5. **`k_floor_threshold=-2.0` matches $\log k_{\min}$** — tighter threshold (-2.5) worsens R²
6. **`coeff_MLP_node_L1=0.0` + long training don't combine** — harmful interaction
7. **MLP$_{\text{node}}$ activation doesn't correlate with R² improvement** — active in batch 8 but R² worse

### Refuted Hypotheses

| Hypothesis | Evidence |
|-----------|---------|
| Recurrent training breaks degeneracy | No R² improvement, 3.5× slower (iter 13) |
| Smaller MLP improves k recovery | Worst R² = 0.011 (iter 15) |
| Higher `lr_node` activates MLP$_{\text{node}}$ | `lr_node=0.005` hurts (iter 11); 0.002 no effect (iter 26) |
| Different seed breaks degeneracy | Same MLP$_{\text{node}}$ flatness (iter 27) |
| Stronger monotonicity (`sub_diff=10`) helps | R² dropped to 0.41 (iter 32) |
| Smaller batch size helps convergence | R² dropped (iter 28) |
| `aug=5000` continues to improve | R² dropped from 0.69 to 0.65 (iter 25) |

### Open Questions

1. **Why does R² plateau at 0.69?** Is this a fundamental limit of single-step training with the current loss function?
2. **Can recurrent training break the plateau with the optimal config?** Previous test (iter 13) was before k_floor.
3. **Is the remaining 31% error from identifiability issues?** Multiple $k$ combinations may produce similar $dc/dt$.
