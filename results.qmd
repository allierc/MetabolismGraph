---
title: "Results"
date: last-modified
date-format: "MMMM D, YYYY"
---

## Rate Constant Recovery — Oscillatory Regime (rank 50)

The LLM-driven exploration engine is running on the **oscillatory regime** (activity rank $\sim50$): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix $\mathbf{S}$ frozen from ground truth. The goal is to recover the 256 rate constants $k_j \in [10^{-2}, 10^{-1}]$ by optimizing training hyperparameters through UCB tree search with 4 parallel slots.

**56 iterations completed** across 5 blocks (14 batches). The exploration is ongoing.

### Simulation Setup

::: {layout-ncol=2}

![Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.](figures/rank_50/concentrations.png){#fig-concentrations}

![Stoichiometric matrix $\mathbf{S}$ (100 metabolites $\times$ 256 reactions). Red = products (+1), blue = substrates (--1). 100% autocatalytic 3-cycles.](figures/rank_50/stoichiometry.png){#fig-stoichiometry}

:::

### Metrics

The primary metric is **raw R²** computed on all 256 reactions (after MLP$_{\text{sub}}$ scalar correction). The **trimmed R²** excludes outlier reactions ($|\Delta \log_{10} k| > 0.3$) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.

### R² Trajectory Across 56 Iterations

```{python}
#| code-fold: true
#| label: fig-iterations
#| fig-cap: "Raw R² (bars) and outlier count (dots) across 56 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74). Block 5 confirmed tight optimization bounds."

import matplotlib.pyplot as plt
import numpy as np

iters = list(range(1, 57))
r2 = [
    # Block 1 (Iter 1-12): Initial exploration
    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep
    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough
    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation
    # Block 2 (Iter 13-24): k_floor breakthrough
    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough
    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor
    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best
    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough
    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns
    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches
    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH
    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7
    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub
    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed
    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST
    # Block 5 (Iter 49-56): Confirming optimum
    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests
    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds
]
outliers = [
    43, 47, 45, 53,
    61, 36, 45, 32,
    27, 38, 57, 30,
    28, 33, 36, 33,
    17, 24, 26, 35,
    16, 29, 19, 24,
    18, 18, 15, 17,
    19, 14, 21, 21,
    21, 21, 15, 22,  # Batch 9
    16, 19, 20, 21,  # Batch 10
    21, 16, 18, 19,  # Batch 11
    15, 20, 23, 21,  # Batch 12
    12, 21, 12, 25,  # Batch 13
    21, 21, 18, 19,  # Batch 14
]

# Color by block
colors = []
for i, r in enumerate(r2):
    if i < 12:
        colors.append('#3498db')  # blue: Block 1
    elif i < 24:
        colors.append('#2ecc71')  # green: Block 2
    elif i < 36:
        colors.append('#e67e22')  # orange: Block 3
    elif i < 48:
        colors.append('#9b59b6')  # purple: Block 4
    else:
        colors.append('#1abc9c')  # teal: Block 5

fig, ax1 = plt.subplots(figsize=(14, 5))

bars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)
ax1.axhline(y=0.736, color='#9b59b6', linestyle='--', alpha=0.4, label='Peak R² = 0.736')
ax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')

# Block separators and labels
ax1.axvline(x=12.5, color='gray', linestyle=':', alpha=0.4)
ax1.axvline(x=24.5, color='gray', linestyle=':', alpha=0.4)
ax1.axvline(x=36.5, color='gray', linestyle=':', alpha=0.4)
ax1.axvline(x=48.5, color='gray', linestyle=':', alpha=0.4)
ax1.text(6.5, 1.0, 'Block 1', ha='center', fontsize=9, color='#3498db')
ax1.text(18.5, 1.0, 'Block 2', ha='center', fontsize=9, color='#2ecc71')
ax1.text(30.5, 1.0, 'Block 3', ha='center', fontsize=9, color='#e67e22')
ax1.text(42.5, 1.0, 'Block 4', ha='center', fontsize=9, color='#9b59b6')
ax1.text(52.5, 1.0, 'Block 5', ha='center', fontsize=9, color='#1abc9c')

# Annotate key events
ax1.annotate('sub_norm=1.0', xy=(6, 0.067), xytext=(6, 0.18),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('k_floor=1.0\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.28),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(35, 0.88),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('sub_diff=7\npeak', xy=(45, 0.736), xytext=(45, 0.88),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')

ax1.set_xlabel('Iteration')
ax1.set_ylabel('rate_constants R² (raw)')
ax1.set_ylim(-0.02, 1.05)
ax1.set_xticks([1, 6, 12, 14, 21, 24, 35, 36, 45, 48, 52, 56])

# outlier count on secondary axis
ax2 = ax1.twinx()
ax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')
ax2.set_ylabel('outlier count', color='#e74c3c')
ax2.tick_params(axis='y', labelcolor='#e74c3c')
ax2.set_ylim(0, 70)

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)

ax1.set_title('UCB Exploration: Rate Constant Recovery (56 iterations)')
plt.tight_layout()
plt.show()
```

### Block 1: Initial Exploration (Iter 1--12)

All 12 iterations achieved raw R² < 0.07. The key discovery was that `coeff_MLP_sub_norm=1.0` is essential — it corrects the MLP$_{\text{sub}}$ function shapes (c² becomes quadratic instead of linear) and enables MLP$_{\text{node}}$ to learn homeostasis.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 1 (iter 1--4) | lr sweep | 0.044 | All failed; MLP$_{\text{node}}$ dead, MLP$_{\text{sub}}$ c² linear |
| 2 (iter 5--8) | `sub_norm=1.0` | **0.067** | MLP$_{\text{sub}}$ normalization is the single most effective change |
| 3 (iter 9--12) | combine best | 0.061 | MLP$_{\text{node}}$ activated; `lr_node=0.005` hurts |

### Block 2: k_floor Breakthrough (Iter 13--24)

The `coeff_k_floor=1.0` penalty at iteration 14 produced a **10× improvement** in R² (0.06 → 0.51) by preventing outlier $\log k$ values from drifting below the true minimum. Longer training then pushed R² to 0.69.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 4 (iter 13--16) | `k_floor=1.0` | **0.508** | Breakthrough — R² jumped 10× |
| 5 (iter 17--20) | `aug=3000` | **0.642** | Longer training + k_floor synergistic |
| 6 (iter 21--24) | `aug=4000` | **0.690** | First to reach 0.69; $\alpha = 0.85$, 16 outliers |

**Best result — Iteration 45**: `coeff_MLP_sub_diff=7`, `lr_sub=0.001`, `data_augmentation_loop=4500`, `coeff_k_floor=1.0`, `coeff_MLP_sub_norm=1.0`

| Metric | Value |
|--------|-------|
| Raw R² | **0.736** |
| Trimmed R² | 0.978 |
| Outliers | 15 / 256 (5.9%) |
| Slope | 0.97 |
| $\alpha$ | 0.90 |
| test_pearson | 0.37 |

*Note: Peak R² has seed-dependent variance of $\pm$0.08 with `sub_diff=7` (reduced from $\pm$0.2 with `sub_diff=5`). Block 5 confirmed tight optimization bounds around this configuration.*

### Rate Constants Recovery

::: {layout-ncol=3}

![Iteration 1 (baseline). R² = 0.044, 43 outliers. Before `k_floor` and `sub_norm` — predicted $\log k$ values scatter widely with no correlation to ground truth.](figures/rank_50/rate_constants_baseline.png){#fig-k-baseline}

![Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The `k_floor=1.0` penalty prevents outlier $\log k$ from drifting below the true minimum, producing a 10$\times$ R² jump.](figures/rank_50/rate_constants_breakthrough.png){#fig-k-breakthrough}

![Iteration 45 (best). R² = 0.736 (trimmed: 0.978), 15 outliers. Combining `k_floor`, `sub_norm`, `aug=4500`, `lr_sub=0.001`, and `sub_diff=7` yields the tightest clustering around the diagonal.](figures/rank_50/rate_constants_best.png){#fig-k-best}

:::

### Kinograph — Best Run (Iteration 45)

![Kinograph montage for iteration 45. Top-left: ground-truth $dc/dt$; top-right: GNN prediction; bottom-left: residual (same color scale as GT); bottom-right: predicted vs. true $dc/dt$ scatter (Pearson = 0.37). The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure.](figures/rank_50/kinograph_best.png){#fig-kinograph-best}

### Learned MLPs

::: {layout-ncol=2}

![Iteration 1 (baseline). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.42 — under-scaled. MLP$_{\text{node}}$: flat at zero.](figures/rank_50/mlp_baseline.png){#fig-mlp-baseline}

![Iteration 45 (best R²). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.90 — closest to the true scale. MLP$_{\text{node}}$: still flat at zero across all 56 iterations — homeostasis not learned.](figures/rank_50/mlp_best.png){#fig-mlp-best}

:::

### Block 3: Plateau and Breakthrough (Iter 25--36)

Twelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling `lr_sub` from 0.0005 to 0.001 broke through to R² = 0.726.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 7 (iter 25--28) | `aug=5000`, seed, batch_size | 0.652 | `aug=5000` hurt R² but $\alpha=0.95$ (best ever) |
| 8 (iter 29--32) | L1=0, `sub_norm=2.0`, lr_k | 0.619 | `sub_norm=2.0`: fewest outliers (14), best slope (0.99) |
| 9 (iter 33--36) | `sub_norm=2.0`+`aug=3500`, recurrent, **`lr_sub=0.001`**, `aug=3500` | **0.726** | `lr_sub=0.001` broke the plateau |

```{python}
#| code-fold: true
#| label: fig-strategies
#| fig-cap: "Impact of key strategies on R². Each dot is one iteration. The k_floor penalty is the single most important factor, with training duration as the second lever."

import matplotlib.pyplot as plt
import numpy as np

strategies = {
    'No k_floor\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],
    'k_floor\nsub_diff=5': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],
    'k_floor\nsub_diff=7': [0.736, 0.696, 0.655, 0.662, 0.701],
    'k_floor\nsub_diff≠5,7': [0.409, 0.591, 0.560, 0.545],
    'Arch/LR\nchanges': [0.483, 0.559, 0.556, 0.600],
}

fig, ax = plt.subplots(figsize=(10, 5))
positions = []
for i, (label, vals) in enumerate(strategies.items()):
    x = np.random.normal(i, 0.08, len(vals))
    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3)
    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)
    positions.append(i)

ax.set_xticks(positions)
ax.set_xticklabels(list(strategies.keys()), fontsize=9)
ax.set_ylabel('Raw R²')
ax.set_ylim(-0.05, 0.85)
ax.axhline(y=0.736, color='green', linestyle='--', alpha=0.3)
ax.set_title('R² by Strategy Group')
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Block 4: Seed Sensitivity, then sub_diff=7 Breakthrough (Iter 37--48)

Block 4 first tested combinations with `lr_sub=0.001` and revealed seed sensitivity ($\pm$0.2 R²). Then `sub_diff=7` (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 10 (iter 37--40) | `sub_norm=2.0`, `lr_sub=0.002`, `lr_node=0.002`, L1=0 | 0.662 | All combinations with `lr_sub=0.001` hurt R² vs baseline |
| 11 (iter 41--44) | seed=123, `aug=4500`, `sub_diff=3`, `lr_k=0.007` | 0.690 | `aug=4500` most stable ($\alpha = 0.94$); seed=123 crashed to R²=0.49 |
| 12 (iter 45--48) | **`sub_diff=7`**, `lr_k=0.004`, `hidden_dim=128`, `batch_size=16` | **0.736** | `sub_diff=7` new best; wider/deeper MLP and larger batch hurt |

Iteration 45 (`sub_diff=7`, `aug=4500`) achieved R² = 0.736 with $\alpha = 0.90$ and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46--48 confirmed that `lr_k=0.004` is too slow, `hidden_dim_sub=128` allows degenerate solutions, and `batch_size=16` degrades convergence.

### Block 5: Confirming Tight Optimization Bounds (Iter 49--56)

Block 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 13 (iter 49--52) | `aug=5000`, `sub_diff=8`, seed=123, `n_layers=4` | 0.696 | `aug=5000` hurts (confirmed twice); `sub_diff=7` improves seed robustness ($\pm$0.08 vs $\pm$0.24) |
| 14 (iter 53--56) | `aug=4250`, `sub_diff=6`, seed=123+`aug=4000`, `lr_sub=0.0015` | 0.701 | All variations hurt; `sub_diff=6` too weak, `lr_sub=0.0015` too high |

The tight bounds: `aug=4500` (not 4000, 4250, or 5000), `sub_diff=7` (not 5, 6, or 8), `lr_sub=0.001` (not 0.0015 or 0.002), `lr_k=0.005` (not 0.004 or 0.007). `sub_diff=7` also improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with `sub_diff=5`) to 0.08.

### UCB Exploration Tree

![UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the `k_floor=1.0` breakthrough (node 14) and converged on the `aug=4000` regime. The tree has since grown to 56 nodes; the overall best is node 45 (R² = 0.736).](figures/rank_50/ucb_tree.png){#fig-ucb-tree}

### Established Principles

The LLM's persistent memory has accumulated these validated principles after 56 iterations:

1. **`coeff_MLP_sub_norm=1.0` is essential** — enables correct MLP shapes: c² becomes quadratic, MLP$_{\text{node}}$ activates
2. **`coeff_k_floor=1.0` is critical** — R² jumped from 0.06 to 0.51 (10× improvement). `k_floor=2.0` too strong.
3. **Longer training helps (up to `aug=4000--4500`)** — aug=2000→3000→4000 consistently improves R². aug=5000 hurts R² (confirmed twice: iter 25, 49). aug=4500 gives best stability.
4. **`lr_k=0.005` is optimal** — lower (0.003, 0.004) too slow, higher (0.007, 0.01) destabilizes
5. **`k_floor_threshold=-2.0` matches $\log k_{\min}$** — tighter threshold (-2.5) worsens R²
6. **`coeff_MLP_node_L1=0.0` + long training don't combine** — harmful interaction (confirmed 3 times: iter 22, 29, 40)
7. **`lr_sub=0.001` is optimal** — 2× increase from 0.0005 broke the R² plateau; `lr_sub=0.002` too high; `lr_sub=0.0015` also hurts
8. **Seed sensitivity is significant but reducible** — R² variance of $\pm$0.2 with `sub_diff=5` (iter 35 vs 41), reduced to $\pm$0.08 with `sub_diff=7` (iter 45 vs 51)
9. **`sub_diff=7` is optimal** — stronger monotonicity than `sub_diff=5` improves R² from 0.69 to 0.74 (iter 45); `sub_diff=8` too strong, `sub_diff=6` too weak, `sub_diff=3` and `sub_diff=10` also hurt
10. **`batch_size=8` is optimal** — `batch_size=16` hurts R² (iter 48)
11. **Default MLP architecture is optimal** — `hidden_dim_sub=128` allows degenerate solutions (iter 47); `n_layers_sub=4` also hurts (iter 52)
12. **MLP$_{\text{node}}$ remains flat across all 56 iterations** — homeostasis $-\lambda(c - c^{\text{base}})$ is never learned, regardless of configuration

### Refuted Hypotheses

| Hypothesis | Evidence |
|-----------|---------|
| Recurrent training breaks degeneracy | No R² improvement, 3.5× slower (iter 13, 34) |
| Smaller MLP improves k recovery | Worst R² = 0.011 (iter 15) |
| Higher `lr_node` activates MLP$_{\text{node}}$ | `lr_node=0.005` hurts (iter 11); 0.002 no effect (iter 26, 39) |
| Different seed breaks degeneracy | Same MLP$_{\text{node}}$ flatness (iter 27); R² variance $\pm$0.2 (iter 41) |
| Stronger monotonicity (`sub_diff=10`) helps | R² dropped to 0.41 (iter 32) |
| Weaker monotonicity (`sub_diff=3`) helps | R² dropped to 0.61 (iter 43) |
| Smaller batch size helps convergence | R² dropped (iter 28) |
| `aug=5000` continues to improve | R² dropped from 0.69 to 0.65 (iter 25); from 0.74 to 0.70 (iter 49) |
| `lr_sub=0.002` improves over 0.001 | R² dropped from 0.73 to 0.52 (iter 38) |
| `sub_norm=2.0` improves R² | Improves $\alpha$ but hurts R² (iter 30, 37) |
| Combining `lr_sub=0.001` with other changes | All combinations hurt R² vs baseline (iter 37--40) |
| Wider MLP$_{\text{sub}}$ (`hidden_dim=128`) helps | R² dropped to 0.56 — allows degenerate solutions (iter 47) |
| Larger `batch_size=16` stabilizes gradients | R² dropped to 0.56 (iter 48) |
| `sub_diff=8` further improves over 7 | R² dropped from 0.74 to 0.59 (iter 50) |
| `sub_diff=6` is better than 7 | R² dropped from 0.74 to 0.56 (iter 54) |
| Deeper MLP$_{\text{sub}}$ (`n_layers=4`) helps | R² dropped to 0.55 (iter 52) |
| `lr_sub=0.0015` (intermediate) helps | R² dropped from 0.74 to 0.60 (iter 56) |
| `lr_k=0.004` gives finer convergence | R² dropped to 0.48 — too slow (iter 46) |

### Open Questions

1. **Is R² $\approx$ 0.74 the true ceiling for single-step training?** After 56 iterations, every variation around the optimal configuration hurts. The `sub_diff=7` improvement (0.69→0.74) was the last breakthrough.
2. **Can ensemble averaging across seeds reduce variance?** Seed sensitivity of $\pm$0.08 (with `sub_diff=7`) suggests averaging could further improve robustness.
3. **Is the remaining 26% error from identifiability issues?** Multiple $k$ combinations may produce similar $dc/dt$.

## Why Homeostasis Is Not Learned

Across all 56 iterations, MLP$_{\text{node}}$ remains flat — the homeostasis function $-\lambda_t(c_i - c_i^{\text{baseline}})$ is never recovered. This is not a hyperparameter issue. It is a structural limitation of single-step ($t \to t+1$) training.

### The scale mismatch problem

The GNN predicts $dc/dt$ at each time step:

$$
\frac{dc_i}{dt} = \underbrace{\text{MLP}_{\text{node}}(c_i, a_i)}_{\text{homeostasis}} + \underbrace{\sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(c_k, |S_{kj}|)}_{\text{reaction}}
$$

The reaction term dominates the instantaneous $dc/dt$ — it drives the fast oscillatory dynamics. Homeostasis acts as a slow restoring force that only manifests over many time steps: it prevents concentrations from drifting away from baseline. In a single-step loss $\| \hat{y}_{t+1} - y_{t+1} \|^2$, the gradient signal from homeostasis is negligible compared to the reaction term. The optimizer has no reason to learn it.

### Homeostasis is an integral problem

Homeostasis determines the **long-term trajectory envelope**, not the instantaneous derivative. Its effect accumulates over time:

$$
c_i(t + T) \approx c_i(t) + \int_t^{t+T} \left[ -\lambda_i (c_i - c_i^{\text{base}}) + \text{reaction terms} \right] d\tau
$$

A model trained on $t \to t+1$ can achieve low loss by fitting the dominant reaction signal and ignoring the small homeostatic correction. Over many steps this error accumulates — but the single-step loss never sees it.

### Proposed approach: two-phase training

To recover homeostasis, we propose a two-phase training scheme:

1. **Phase 1 — Reaction recovery** ($t \to t+1$): Train as currently done. Recover $k_j$, MLP$_{\text{sub}}$, and the stoichiometric structure. Freeze these parameters.

2. **Phase 2 — Homeostasis recovery** (recurrent, $t \to t+T$): With the reaction term frozen, train only MLP$_{\text{node}}$ and embeddings $a_i$ using multi-step rollout. The recurrent loss forces the model to match trajectories over $T$ steps, where the homeostatic drift becomes visible.

This separates the two learning problems by time scale: fast reactions are learned from instantaneous gradients, slow homeostasis from trajectory matching. The key insight is that recurrent training is not needed for $k_j$ recovery (it was tried and failed — iter 13, 34), but may be essential specifically for homeostasis once the reaction parameters are frozen.

### Alternative approach: residual-based supervision

The kinograph residual (@fig-kinograph-best, bottom-left) directly reveals what the single-step model cannot learn. After Phase 1, we can roll out the learned model autoregressively:

$$
\hat{c}_i(t+1) = \hat{c}_i(t) + \Delta t \cdot \left[ \sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(\hat{c}_k, |S_{kj}|) \right]
$$

The rollout trajectory $\hat{c}(t)$ drifts from the observation $c(t)$ because the missing homeostatic restoring force is not applied at each step. The accumulated residual

$$
r_i(t) = c_i(t) - \hat{c}_i(t)
$$

is precisely the integrated effect of the missing slow terms — homeostasis, external sources, and degradation. This residual provides a direct supervision signal for Phase 2 without requiring backpropagation through time: we can compute a per-step target

$$
\frac{r_i(t+1) - r_i(t)}{\Delta t} \approx \text{MLP}_{\text{node}}(c_i(t), a_i)
$$

and train MLP$_{\text{node}}$ with a standard single-step loss on this derived target. This avoids the instabilities of recurrent training while still capturing the long-term dynamics that the reaction-only model structurally cannot predict.

This approach is an instance of the **Universal Differential Equations** framework ([Rackauckas et al., 2021](https://arxiv.org/abs/2001.04385)), where a partially known ODE is augmented with a neural network that learns the missing terms from data. Here, the known part is the reaction dynamics $\sum_j S_{ij} k_j \prod_k \text{MLP}_{\text{sub}}$, and the unknown part is the homeostatic correction learned by MLP$_{\text{node}}$. The residual-based supervision strategy is also related to **PDE-Refiner** ([Lippe et al., NeurIPS 2023](https://arxiv.org/abs/2308.05732)), which uses iterative refinement on rollout residuals to recover low-amplitude dynamics that single-pass neural solvers miss — analogous to the small homeostatic signal masked by dominant reactions.

### Related work

The difficulty of learning slow dynamics from single-step training is well established in the literature:

- **Teacher forcing and long-term dependencies.** [Williams & Zipser (1989)](https://direct.mit.edu/neco/article/1/2/270/5490/) introduced teacher forcing for RNNs: feeding ground-truth inputs at each step. [Bengio et al. (1994)](https://ieeexplore.ieee.org/document/279181) showed that gradient-based learning of long-term dependencies is fundamentally difficult because short-term gradient contributions dominate, exactly the mechanism that prevents our GNN from learning homeostasis.

- **Scheduled sampling.** [Bengio et al. (2015)](https://arxiv.org/abs/1506.03099) proposed a curriculum strategy that gradually transitions from teacher forcing (single-step) to free-running (multi-step) prediction, reducing the train–inference discrepancy. Our two-phase proposal is conceptually related: Phase 1 is teacher-forced, Phase 2 uses multi-step rollout.

- **Multiple shooting for Neural ODEs.** [Massaroli et al. (2021)](https://arxiv.org/abs/2106.03885) introduced differentiable multiple shooting layers that parallelize trajectory integration. [Turan & Jäschke (2021)](https://arxiv.org/abs/2109.06786) showed that standard Neural ODE fitting on oscillatory data produces "flattened" trajectories — multiple shooting recovers the true dynamics by breaking long horizons into segments. This directly addresses the failure mode we observe.

- **Stiff Neural ODEs.** [Kim et al. (2021)](https://arxiv.org/abs/2103.15341) showed that learning neural ODEs for systems with widely separated time scales (stiff systems) requires proper output scaling and stabilized gradients — the fast dynamics otherwise dominate training, exactly as in our reaction-vs-homeostasis scale mismatch.

- **Multi-scale separation in dynamical systems.** [Fenichel (1979)](https://doi.org/10.1016/0022-0396(79)90152-9) established geometric singular perturbation theory: for systems with fast and slow time scales, the fast subsystem is solved first, then the dynamics are reduced to a slow manifold. Our two-phase training mirrors this decomposition — Phase 1 recovers the fast reaction dynamics, Phase 2 learns the slow homeostatic manifold.

- **Latent timescales in Neural ODEs.** [Gupta et al. (2024)](https://arxiv.org/abs/2403.02224) showed that training trajectory length directly controls the timescales a Neural ODE can recover: longer trajectories are needed to capture slower dynamics. This supports the need for multi-step rollout in Phase 2.

The scale mismatch / integral argument is essentially the same observation that Bengio (1994) made for RNNs and Kim et al. (2021) made for stiff ODEs. The two-phase proposal mirrors Fenichel's fast-then-slow decomposition. The novelty here is applying this reasoning to GNN-based metabolic network recovery, where the bipartite graph structure and the identifiability of rate constants $k_j$ add domain-specific constraints.
