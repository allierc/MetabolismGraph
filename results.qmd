---
title: "Results"
date: last-modified
date-format: "MMMM D, YYYY"
---

## Rate Constant Recovery — Oscillatory Regime (rank 50)

The LLM-driven exploration engine is running on the **oscillatory regime** (activity rank $\sim50$): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix $\mathbf{S}$ frozen from ground truth. The goal is to recover the 256 rate constants $k_j \in [10^{-2}, 10^{-1}]$ by optimizing training hyperparameters through UCB tree search with 4 parallel slots.

**32 iterations completed** across 3 blocks (8 batches). The exploration is ongoing.

### Simulation Setup

::: {layout-ncol=2}

![Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.](figures/rank_50/concentrations.png){#fig-concentrations}

![Stoichiometric matrix $\mathbf{S}$ (100 metabolites $\times$ 256 reactions). Red = products (+1), blue = substrates (--1). 100% autocatalytic 3-cycles.](figures/rank_50/stoichiometry.png){#fig-stoichiometry}

:::

### Metrics

The primary metric is **raw R²** computed on all 256 reactions (after MLP$_{\text{sub}}$ scalar correction). The **trimmed R²** excludes outlier reactions ($|\Delta \log_{10} k| > 0.3$) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.

### R² Trajectory Across 32 Iterations

```{python}
#| code-fold: true
#| label: fig-iterations
#| fig-cap: "Raw R² (bars) and outlier count (dots) across 32 iterations, colored by block. The k_floor breakthrough at iteration 14 lifted R² from 0.07 to 0.51, and longer training pushed it to 0.69 by iteration 21. Block 3 failed to break the plateau."

import matplotlib.pyplot as plt
import numpy as np

iters = list(range(1, 33))
r2 = [
    # Block 1 (Iter 1-12): Initial exploration
    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep
    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough
    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation
    # Block 2 (Iter 13-24): k_floor breakthrough
    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough
    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor
    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best
    # Block 3 (Iter 25-32): Plateau
    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns
    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches
]
outliers = [
    43, 47, 45, 53,
    61, 36, 45, 32,
    27, 38, 57, 30,
    28, 33, 36, 33,
    17, 24, 26, 35,
    16, 29, 19, 24,
    18, 18, 15, 17,
    19, 14, 21, 21,
]

# Color by block
colors = []
for i, r in enumerate(r2):
    if i < 12:
        colors.append('#3498db')  # blue: Block 1
    elif i < 24:
        colors.append('#2ecc71')  # green: Block 2
    else:
        colors.append('#e67e22')  # orange: Block 3

fig, ax1 = plt.subplots(figsize=(14, 5))

bars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)
ax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.4, label='Best R² = 0.690')

# Block separators and labels
ax1.axvline(x=12.5, color='gray', linestyle=':', alpha=0.4)
ax1.axvline(x=24.5, color='gray', linestyle=':', alpha=0.4)
ax1.text(6.5, 1.0, 'Block 1', ha='center', fontsize=9, color='#3498db', fontweight='bold')
ax1.text(18.5, 1.0, 'Block 2', ha='center', fontsize=9, color='#2ecc71', fontweight='bold')
ax1.text(28.5, 1.0, 'Block 3', ha='center', fontsize=9, color='#e67e22', fontweight='bold')

# Annotate key events
ax1.annotate('sub_norm=1.0', xy=(6, 0.067), xytext=(6, 0.18),
            arrowprops=dict(arrowstyle='->', color='#3498db'), fontsize=7, ha='center', color='#3498db')
ax1.annotate('k_floor=1.0\nBREAKTHROUGH', xy=(14, 0.508), xytext=(14, 0.28),
            arrowprops=dict(arrowstyle='->', color='#2ecc71'), fontsize=7, ha='center', color='#2ecc71', fontweight='bold')
ax1.annotate('aug=4000\nBEST', xy=(21, 0.690), xytext=(21, 0.82),
            arrowprops=dict(arrowstyle='->', color='#2ecc71'), fontsize=7, ha='center', color='#2ecc71', fontweight='bold')

ax1.set_xlabel('Iteration')
ax1.set_ylabel('rate_constants R² (raw)')
ax1.set_ylim(-0.02, 1.05)
ax1.set_xticks([1, 4, 8, 12, 14, 17, 21, 24, 28, 32])

# outlier count on secondary axis
ax2 = ax1.twinx()
ax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')
ax2.set_ylabel('outlier count', color='#e74c3c')
ax2.tick_params(axis='y', labelcolor='#e74c3c')
ax2.set_ylim(0, 70)

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)

ax1.set_title('UCB Exploration: Rate Constant Recovery (32 iterations)')
plt.tight_layout()
plt.show()
```

### Block 1: Initial Exploration (Iter 1--12)

All 12 iterations achieved raw R² < 0.07. The key discovery was that `coeff_MLP_sub_norm=1.0` is essential — it corrects the MLP$_{\text{sub}}$ function shapes (c² becomes quadratic instead of linear) and enables MLP$_{\text{node}}$ to learn homeostasis.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 1 (iter 1--4) | lr sweep | 0.044 | All failed; MLP$_{\text{node}}$ dead, MLP$_{\text{sub}}$ c² linear |
| 2 (iter 5--8) | `sub_norm=1.0` | **0.067** | MLP$_{\text{sub}}$ normalization is the single most effective change |
| 3 (iter 9--12) | combine best | 0.061 | MLP$_{\text{node}}$ activated; `lr_node=0.005` hurts |

### Block 2: k_floor Breakthrough (Iter 13--24)

The `coeff_k_floor=1.0` penalty at iteration 14 produced a **10× improvement** in R² (0.06 → 0.51) by preventing outlier $\log k$ values from drifting below the true minimum. Longer training then pushed R² to 0.69.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 4 (iter 13--16) | `k_floor=1.0` | **0.508** | Breakthrough — R² jumped 10× |
| 5 (iter 17--20) | `aug=3000` | **0.642** | Longer training + k_floor synergistic |
| 6 (iter 21--24) | `aug=4000` | **0.690** | Best overall; $\alpha = 0.85$, 16 outliers |

**Best result — Iteration 21**: `data_augmentation_loop=4000`, `coeff_k_floor=1.0`, `coeff_MLP_sub_norm=1.0`

| Metric | Value |
|--------|-------|
| Raw R² | **0.690** |
| Trimmed R² | 0.962 |
| Outliers | 16 / 256 (6.3%) |
| Slope | 0.976 |
| $\alpha$ | 0.85 |
| test_pearson | 0.33 |

### Rate Constants Recovery

::: {layout-ncol=3}

![Iteration 1 (baseline). R² = 0.044, 43 outliers. Before `k_floor` and `sub_norm` — predicted $\log k$ values scatter widely with no correlation to ground truth.](figures/rank_50/rate_constants_baseline.png){#fig-k-baseline}

![Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The `k_floor=1.0` penalty prevents outlier $\log k$ from drifting below the true minimum, producing a 10$\times$ R² jump.](figures/rank_50/rate_constants_breakthrough.png){#fig-k-breakthrough}

![Iteration 21 (best). R² = 0.690 (trimmed: 0.962), 16 outliers. Combining `k_floor`, `sub_norm`, and `aug=4000` yields tight clustering around the diagonal.](figures/rank_50/rate_constants_best.png){#fig-k-best}

:::

### Kinograph — Best Run (Iteration 21)

![Kinograph montage for iteration 21. Top-left: ground-truth $dc/dt$; top-right: GNN prediction; bottom-left: residual (same color scale as GT); bottom-right: predicted vs. true $dc/dt$ scatter (Pearson = 0.33). The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure.](figures/rank_50/kinograph_best.png){#fig-kinograph-best}

### Learned MLPs

::: {layout-ncol=2}

![Iteration 1 (baseline). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.42 — under-scaled. MLP$_{\text{node}}$: flat at zero.](figures/rank_50/mlp_baseline.png){#fig-mlp-baseline}

![Iteration 21 (best R²). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.79 — closer to the true scale. MLP$_{\text{node}}$: still flat at zero across all 32 iterations — homeostasis not learned.](figures/rank_50/mlp_best.png){#fig-mlp-best}

:::

### Block 3: Plateau and Breakthrough (Iter 25--36)

Twelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling `lr_sub` from 0.0005 to 0.001 broke through to R² = 0.726.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 7 (iter 25--28) | `aug=5000`, seed, batch_size | 0.652 | `aug=5000` hurt R² but $\alpha=0.95$ (best ever) |
| 8 (iter 29--32) | L1=0, `sub_norm=2.0`, lr_k | 0.619 | `sub_norm=2.0`: fewest outliers (14), best slope (0.99) |
| 9 (iter 33--36) | `sub_norm=2.0`+`aug=3500`, recurrent, **`lr_sub=0.001`**, `aug=3500` | **0.726** | `lr_sub=0.001` broke the plateau |

```{python}
#| code-fold: true
#| label: fig-strategies
#| fig-cap: "Impact of key strategies on R². Each dot is one iteration. The k_floor penalty is the single most important factor, with training duration as the second lever."

import matplotlib.pyplot as plt
import numpy as np

strategies = {
    'No k_floor\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],
    'k_floor=1.0\naug≤2000': [0.508],
    'k_floor=1.0\naug=3000': [0.638, 0.419, 0.658, 0.559, 0.470, 0.373],
    'k_floor=1.0\naug=3500-4500': [0.521, 0.544, 0.690],
    'k_floor=1.0\naug=4000': [0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.478, 0.726, 0.588, 0.518, 0.654, 0.662, 0.487, 0.608, 0.593],
    'k_floor=1.0\naug=5000': [0.652],
}

fig, ax = plt.subplots(figsize=(10, 5))
positions = []
for i, (label, vals) in enumerate(strategies.items()):
    x = np.random.normal(i, 0.08, len(vals))
    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3)
    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)
    positions.append(i)

ax.set_xticks(positions)
ax.set_xticklabels(list(strategies.keys()), fontsize=9)
ax.set_ylabel('Raw R²')
ax.set_ylim(-0.05, 0.85)
ax.axhline(y=0.690, color='green', linestyle='--', alpha=0.3)
ax.set_title('R² by Strategy Group')
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Block 4: Seed Sensitivity and Convergence (Iter 37--44)

Block 4 tested combinations with `lr_sub=0.001` and revealed that the R² = 0.726 breakthrough (iter 35) was partly seed-dependent. Running the identical configuration with a different seed (iter 41, seed=123) dropped R² by 0.24.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 10 (iter 37--40) | `sub_norm=2.0`, `lr_sub=0.002`, `lr_node=0.002`, L1=0 | 0.662 | All combinations with `lr_sub=0.001` hurt R² vs baseline |
| 11 (iter 41--44) | seed=123, **`aug=4500`**, `sub_diff=3`, `lr_k=0.007` | **0.690** | `aug=4500` most stable; seed=123 crashed to R²=0.49 |

The most stable configuration (iter 42: `aug=4500`) achieved R² = 0.690 with $\alpha = 0.94$ (best ever) and 16 outliers. This suggests the true performance ceiling for single-step training is R² $\approx$ 0.69, with seed-dependent variance of $\pm$0.2.

### UCB Exploration Tree

![UCB tree after 24 iterations. Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the `k_floor=1.0` breakthrough (node 14) and converged on the `aug=4000` regime. Node 21 (best R² = 0.69) is highlighted.](figures/rank_50/ucb_tree.png){#fig-ucb-tree}

### Established Principles

The LLM's persistent memory has accumulated these validated principles after 44 iterations:

1. **`coeff_MLP_sub_norm=1.0` is essential** — enables correct MLP shapes: c² becomes quadratic, MLP$_{\text{node}}$ activates
2. **`coeff_k_floor=1.0` is critical** — R² jumped from 0.06 to 0.51 (10× improvement). `k_floor=2.0` too strong.
3. **Longer training helps (up to `aug=4000--4500`)** — aug=2000→3000→4000 consistently improves R². aug=5000 hurts R². aug=4500 gives best stability.
4. **`lr_k=0.005` is optimal** — lower (0.003) too slow, higher (0.007, 0.01) destabilizes
5. **`k_floor_threshold=-2.0` matches $\log k_{\min}$** — tighter threshold (-2.5) worsens R²
6. **`coeff_MLP_node_L1=0.0` + long training don't combine** — harmful interaction (confirmed 3 times: iter 22, 29, 40)
7. **`lr_sub=0.001` is optimal** — 2× increase from 0.0005 broke the R² plateau; `lr_sub=0.002` too high
8. **Seed sensitivity is significant** — R² variance of $\pm$0.2 across seeds with identical config (iter 35 vs 41)
9. **`sub_diff=5` is optimal** — less monotonicity (`sub_diff=3`) hurts R²; more (`sub_diff=10`) also hurts
10. **MLP$_{\text{node}}$ remains flat across all 44 iterations** — homeostasis $-\lambda(c - c^{\text{base}})$ is never learned, regardless of configuration

### Refuted Hypotheses

| Hypothesis | Evidence |
|-----------|---------|
| Recurrent training breaks degeneracy | No R² improvement, 3.5× slower (iter 13, 34) |
| Smaller MLP improves k recovery | Worst R² = 0.011 (iter 15) |
| Higher `lr_node` activates MLP$_{\text{node}}$ | `lr_node=0.005` hurts (iter 11); 0.002 no effect (iter 26, 39) |
| Different seed breaks degeneracy | Same MLP$_{\text{node}}$ flatness (iter 27); R² variance $\pm$0.2 (iter 41) |
| Stronger monotonicity (`sub_diff=10`) helps | R² dropped to 0.41 (iter 32) |
| Weaker monotonicity (`sub_diff=3`) helps | R² dropped to 0.61 (iter 43) |
| Smaller batch size helps convergence | R² dropped (iter 28) |
| `aug=5000` continues to improve | R² dropped from 0.69 to 0.65 (iter 25) |
| `lr_sub=0.002` improves over 0.001 | R² dropped from 0.73 to 0.52 (iter 38) |
| `sub_norm=2.0` improves R² | Improves $\alpha$ but hurts R² (iter 30, 37) |
| Combining `lr_sub=0.001` with other changes | All combinations hurt R² vs baseline (iter 37--40) |

### Open Questions

1. **Is R² $\approx$ 0.69 the true ceiling for single-step training?** The R² = 0.726 peak (iter 35) appears seed-dependent; stable performance converges to $\sim$0.69.
2. **Can ensemble averaging across seeds reduce variance?** Seed sensitivity of $\pm$0.2 suggests averaging could improve robustness.
3. **Is the remaining 31% error from identifiability issues?** Multiple $k$ combinations may produce similar $dc/dt$.

## Why Homeostasis Is Not Learned

Across all 44 iterations, MLP$_{\text{node}}$ remains flat — the homeostasis function $-\lambda_t(c_i - c_i^{\text{baseline}})$ is never recovered. This is not a hyperparameter issue. It is a structural limitation of single-step ($t \to t+1$) training.

### The scale mismatch problem

The GNN predicts $dc/dt$ at each time step:

$$
\frac{dc_i}{dt} = \underbrace{\text{MLP}_{\text{node}}(c_i, a_i)}_{\text{homeostasis}} + \underbrace{\sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(c_k, |S_{kj}|)}_{\text{reaction}}
$$

The reaction term dominates the instantaneous $dc/dt$ — it drives the fast oscillatory dynamics. Homeostasis acts as a slow restoring force that only manifests over many time steps: it prevents concentrations from drifting away from baseline. In a single-step loss $\| \hat{y}_{t+1} - y_{t+1} \|^2$, the gradient signal from homeostasis is negligible compared to the reaction term. The optimizer has no reason to learn it.

### Homeostasis is an integral problem

Homeostasis determines the **long-term trajectory envelope**, not the instantaneous derivative. Its effect accumulates over time:

$$
c_i(t + T) \approx c_i(t) + \int_t^{t+T} \left[ -\lambda_i (c_i - c_i^{\text{base}}) + \text{reaction terms} \right] d\tau
$$

A model trained on $t \to t+1$ can achieve low loss by fitting the dominant reaction signal and ignoring the small homeostatic correction. Over many steps this error accumulates — but the single-step loss never sees it.

### Proposed approach: two-phase training

To recover homeostasis, we propose a two-phase training scheme:

1. **Phase 1 — Reaction recovery** ($t \to t+1$): Train as currently done. Recover $k_j$, MLP$_{\text{sub}}$, and the stoichiometric structure. Freeze these parameters.

2. **Phase 2 — Homeostasis recovery** (recurrent, $t \to t+T$): With the reaction term frozen, train only MLP$_{\text{node}}$ and embeddings $a_i$ using multi-step rollout. The recurrent loss forces the model to match trajectories over $T$ steps, where the homeostatic drift becomes visible.

This separates the two learning problems by time scale: fast reactions are learned from instantaneous gradients, slow homeostasis from trajectory matching. The key insight is that recurrent training is not needed for $k_j$ recovery (it was tried and failed — iter 13, 34), but may be essential specifically for homeostasis once the reaction parameters are frozen.

### Alternative approach: residual-based supervision

The kinograph residual (@fig-kinograph-best, bottom-left) directly reveals what the single-step model cannot learn. After Phase 1, we can roll out the learned model autoregressively:

$$
\hat{c}_i(t+1) = \hat{c}_i(t) + \Delta t \cdot \left[ \sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(\hat{c}_k, |S_{kj}|) \right]
$$

The rollout trajectory $\hat{c}(t)$ drifts from the observation $c(t)$ because the missing homeostatic restoring force is not applied at each step. The accumulated residual

$$
r_i(t) = c_i(t) - \hat{c}_i(t)
$$

is precisely the integrated effect of the missing slow terms — homeostasis, external sources, and degradation. This residual provides a direct supervision signal for Phase 2 without requiring backpropagation through time: we can compute a per-step target

$$
\frac{r_i(t+1) - r_i(t)}{\Delta t} \approx \text{MLP}_{\text{node}}(c_i(t), a_i)
$$

and train MLP$_{\text{node}}$ with a standard single-step loss on this derived target. This avoids the instabilities of recurrent training while still capturing the long-term dynamics that the reaction-only model structurally cannot predict.

This approach is an instance of the **Universal Differential Equations** framework ([Rackauckas et al., 2021](https://arxiv.org/abs/2001.04385)), where a partially known ODE is augmented with a neural network that learns the missing terms from data. Here, the known part is the reaction dynamics $\sum_j S_{ij} k_j \prod_k \text{MLP}_{\text{sub}}$, and the unknown part is the homeostatic correction learned by MLP$_{\text{node}}$. The residual-based supervision strategy is also related to **PDE-Refiner** ([Lippe et al., NeurIPS 2023](https://arxiv.org/abs/2308.05732)), which uses iterative refinement on rollout residuals to recover low-amplitude dynamics that single-pass neural solvers miss — analogous to the small homeostatic signal masked by dominant reactions.

### Related work

The difficulty of learning slow dynamics from single-step training is well established in the literature:

- **Teacher forcing and long-term dependencies.** [Williams & Zipser (1989)](https://direct.mit.edu/neco/article/1/2/270/5490/) introduced teacher forcing for RNNs: feeding ground-truth inputs at each step. [Bengio et al. (1994)](https://ieeexplore.ieee.org/document/279181) showed that gradient-based learning of long-term dependencies is fundamentally difficult because short-term gradient contributions dominate, exactly the mechanism that prevents our GNN from learning homeostasis.

- **Scheduled sampling.** [Bengio et al. (2015)](https://arxiv.org/abs/1506.03099) proposed a curriculum strategy that gradually transitions from teacher forcing (single-step) to free-running (multi-step) prediction, reducing the train–inference discrepancy. Our two-phase proposal is conceptually related: Phase 1 is teacher-forced, Phase 2 uses multi-step rollout.

- **Multiple shooting for Neural ODEs.** [Massaroli et al. (2021)](https://arxiv.org/abs/2106.03885) introduced differentiable multiple shooting layers that parallelize trajectory integration. [Turan & Jäschke (2021)](https://arxiv.org/abs/2109.06786) showed that standard Neural ODE fitting on oscillatory data produces "flattened" trajectories — multiple shooting recovers the true dynamics by breaking long horizons into segments. This directly addresses the failure mode we observe.

- **Stiff Neural ODEs.** [Kim et al. (2021)](https://arxiv.org/abs/2103.15341) showed that learning neural ODEs for systems with widely separated time scales (stiff systems) requires proper output scaling and stabilized gradients — the fast dynamics otherwise dominate training, exactly as in our reaction-vs-homeostasis scale mismatch.

- **Multi-scale separation in dynamical systems.** [Fenichel (1979)](https://doi.org/10.1016/0022-0396(79)90152-9) established geometric singular perturbation theory: for systems with fast and slow time scales, the fast subsystem is solved first, then the dynamics are reduced to a slow manifold. Our two-phase training mirrors this decomposition — Phase 1 recovers the fast reaction dynamics, Phase 2 learns the slow homeostatic manifold.

- **Latent timescales in Neural ODEs.** [Gupta et al. (2024)](https://arxiv.org/abs/2403.02224) showed that training trajectory length directly controls the timescales a Neural ODE can recover: longer trajectories are needed to capture slower dynamics. This supports the need for multi-step rollout in Phase 2.

The scale mismatch / integral argument is essentially the same observation that Bengio (1994) made for RNNs and Kim et al. (2021) made for stiff ODEs. The two-phase proposal mirrors Fenichel's fast-then-slow decomposition. The novelty here is applying this reasoning to GNN-based metabolic network recovery, where the bipartite graph structure and the identifiability of rate constants $k_j$ add domain-specific constraints.
