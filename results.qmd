---
title: "Results"
date: last-modified
date-format: "MMMM D, YYYY"
---

## Rate Constant Recovery — Oscillatory Regime (rank 50)

The LLM-driven exploration engine is running on the **oscillatory regime** (activity rank $\sim50$): 100 metabolites, 256 autocatalytic reactions, 2880 time frames, mass-action kinetics with the stoichiometric matrix $\mathbf{S}$ frozen from ground truth. The goal is to recover the 256 rate constants $k_j \in [10^{-2}, 10^{-1}]$ by optimizing training hyperparameters through UCB tree search with 4 parallel slots.

**84 iterations completed** across 7 blocks (21 batches). The exploration is ongoing.

### Simulation Setup

::: {layout-ncol=2}

![Concentration dynamics of 100 metabolites over 2880 time frames. Activity rank = 47 — most reactions actively contribute to the dynamics.](figures/rank_50/concentrations.png){#fig-concentrations}

![Stoichiometric matrix $\mathbf{S}$ (100 metabolites $\times$ 256 reactions). Red = products (+1), blue = substrates (--1). 100% autocatalytic 3-cycles.](figures/rank_50/stoichiometry.png){#fig-stoichiometry}

:::

### Metrics

The primary metric is **raw R²** computed on all 256 reactions (after MLP$_{\text{sub}}$ scalar correction). The **trimmed R²** excludes outlier reactions ($|\Delta \log_{10} k| > 0.3$) and is reported in parentheses. Raw R² is what drives the UCB exploration — it penalizes outliers directly instead of hiding them.

### R² Trajectory Across 84 Iterations

```{python}
#| code-fold: true
#| label: fig-iterations
#| fig-cap: "Raw R² (bars) and outlier count (dots) across 84 iterations, colored by block. Key breakthroughs: k_floor (iter 14, 0.07→0.51), aug=4000 (iter 21, 0.69), lr_sub=0.001 (iter 35, 0.73), sub_diff=7 (iter 45, 0.74), seed=77+sub_diff=8 (iter 82, 0.76). Training variance of ±0.08 R² is a dominant factor beyond Block 5."

import matplotlib.pyplot as plt
import numpy as np

iters = list(range(1, 85))
r2 = [
    # Block 1 (Iter 1-12): Initial exploration
    0.044, 0.027, 0.044, 0.031,  # Batch 1: initial sweep
    0.013, 0.067, 0.041, 0.051,  # Batch 2: sub_norm breakthrough
    0.054, 0.061, 0.017, 0.061,  # Batch 3: MLP_node activation
    # Block 2 (Iter 13-24): k_floor breakthrough
    0.056, 0.508, 0.011, 0.057,  # Batch 4: k_floor=1.0 breakthrough
    0.638, 0.642, 0.470, 0.373,  # Batch 5: exploiting k_floor
    0.690, 0.419, 0.658, 0.559,  # Batch 6: aug=4000 best
    # Block 3 (Iter 25-36): Plateau then lr_sub breakthrough
    0.652, 0.638, 0.614, 0.600,  # Batch 7: diminishing returns
    0.507, 0.619, 0.530, 0.409,  # Batch 8: alternative approaches
    0.521, 0.478, 0.726, 0.544,  # Batch 9: lr_sub=0.001 BREAKTHROUGH
    # Block 4 (Iter 37-48): Seed sensitivity then sub_diff=7
    0.588, 0.518, 0.654, 0.662,  # Batch 10: combinations with lr_sub
    0.487, 0.690, 0.608, 0.593,  # Batch 11: seed sensitivity revealed
    0.736, 0.483, 0.559, 0.556,  # Batch 12: sub_diff=7 NEW BEST
    # Block 5 (Iter 49-60): Confirming optimum
    0.696, 0.591, 0.655, 0.545,  # Batch 13: robustness tests
    0.662, 0.560, 0.701, 0.600,  # Batch 14: fine-tuning bounds
    0.701, 0.718, 0.603, 0.603,  # Batch 15: seed=99 promising
    # Block 6 (Iter 61-72): Variance discovery
    0.688, 0.430, 0.616, 0.704,  # Batch 16: k_floor=1.5 promising
    0.565, 0.674, 0.664, 0.609,  # Batch 17: k_floor non-monotonic
    0.658, 0.639, 0.473, 0.550,  # Batch 18: HIGH VARIANCE discovered
    # Block 7 (Iter 73-84): seed=77 breakthrough
    0.722, 0.583, 0.682, 0.636,  # Batch 19: seed=123 improved
    0.694, 0.515, 0.473, 0.748,  # Batch 20: seed=77 NEW BEST
    0.661, 0.764, 0.387, 0.689,  # Batch 21: seed=77+sub_diff=8 PEAK
]
outliers = [
    43, 47, 45, 53,
    61, 36, 45, 32,
    27, 38, 57, 30,
    28, 33, 36, 33,
    17, 24, 26, 35,
    16, 29, 19, 24,
    18, 18, 15, 17,
    19, 14, 21, 21,
    21, 21, 15, 22,  # Batch 9
    16, 19, 20, 21,  # Batch 10
    21, 16, 18, 19,  # Batch 11
    15, 20, 23, 21,  # Batch 12
    12, 21, 12, 25,  # Batch 13
    21, 21, 18, 19,  # Batch 14
    17, 17, 16, 20,  # Batch 15
    17, 20, 17, 16,  # Batch 16
    17, 14, 16, 14,  # Batch 17
    19, 14, 22, 17,  # Batch 18
    19, 16, 22, 20,  # Batch 19
    16, 16, 32, 12,  # Batch 20
    16, 15, 23, 18,  # Batch 21
]

# Color by block
block_colors = ['#3498db', '#2ecc71', '#e67e22', '#9b59b6', '#1abc9c', '#e84393', '#f39c12']
block_bounds = [12, 24, 36, 48, 60, 72, 84]
colors = []
for i in range(len(r2)):
    for b, bound in enumerate(block_bounds):
        if i < bound:
            colors.append(block_colors[b])
            break

fig, ax1 = plt.subplots(figsize=(16, 5))

bars = ax1.bar(iters, r2, color=colors, edgecolor='white', linewidth=0.5, alpha=0.85)
ax1.axhline(y=0.764, color='#f39c12', linestyle='--', alpha=0.4, label='Peak R² = 0.764')
ax1.axhline(y=0.690, color='#2ecc71', linestyle='--', alpha=0.3, label='Stable R² ≈ 0.69')

# Block separators and labels
for b in range(len(block_bounds) - 1):
    ax1.axvline(x=block_bounds[b] + 0.5, color='gray', linestyle=':', alpha=0.4)
block_labels = ['Block 1', 'Block 2', 'Block 3', 'Block 4', 'Block 5', 'Block 6', 'Block 7']
block_centers = [6.5, 18.5, 30.5, 42.5, 54.5, 66.5, 78.5]
for i, (lbl, cx) in enumerate(zip(block_labels, block_centers)):
    ax1.text(cx, 1.0, lbl, ha='center', fontsize=8, color=block_colors[i])

# Annotate key events
ax1.annotate('k_floor=1.0\nbreakthrough', xy=(14, 0.508), xytext=(14, 0.25),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('aug=4000', xy=(21, 0.690), xytext=(21, 0.82),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('lr_sub=0.001', xy=(35, 0.726), xytext=(35, 0.86),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('sub_diff=7', xy=(45, 0.736), xytext=(45, 0.86),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')
ax1.annotate('seed=77+\nsub_diff=8', xy=(82, 0.764), xytext=(82, 0.90),
            arrowprops=dict(arrowstyle='->', color='black'), fontsize=7, ha='center', color='black')

ax1.set_xlabel('Iteration')
ax1.set_ylabel('rate_constants R² (raw)')
ax1.set_ylim(-0.02, 1.05)
ax1.set_xticks([1, 12, 14, 21, 24, 35, 36, 45, 48, 60, 72, 80, 82, 84])

# outlier count on secondary axis
ax2 = ax1.twinx()
ax2.plot(iters, outliers, 'o', color='#e74c3c', markersize=3, alpha=0.5, label='outliers')
ax2.set_ylabel('outlier count', color='#e74c3c')
ax2.tick_params(axis='y', labelcolor='#e74c3c')
ax2.set_ylim(0, 70)

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=8)

ax1.set_title('UCB Exploration: Rate Constant Recovery (84 iterations)')
plt.tight_layout()
plt.show()
```

### Block 1: Initial Exploration (Iter 1--12)

All 12 iterations achieved raw R² < 0.07. The key discovery was that `coeff_MLP_sub_norm=1.0` is essential — it corrects the MLP$_{\text{sub}}$ function shapes (c² becomes quadratic instead of linear) and enables MLP$_{\text{node}}$ to learn homeostasis.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 1 (iter 1--4) | lr sweep | 0.044 | All failed; MLP$_{\text{node}}$ dead, MLP$_{\text{sub}}$ c² linear |
| 2 (iter 5--8) | `sub_norm=1.0` | **0.067** | MLP$_{\text{sub}}$ normalization is the single most effective change |
| 3 (iter 9--12) | combine best | 0.061 | MLP$_{\text{node}}$ activated; `lr_node=0.005` hurts |

### Block 2: k_floor Breakthrough (Iter 13--24)

The `coeff_k_floor=1.0` penalty at iteration 14 produced a **10× improvement** in R² (0.06 → 0.51) by preventing outlier $\log k$ values from drifting below the true minimum. Longer training then pushed R² to 0.69.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 4 (iter 13--16) | `k_floor=1.0` | **0.508** | Breakthrough — R² jumped 10× |
| 5 (iter 17--20) | `aug=3000` | **0.642** | Longer training + k_floor synergistic |
| 6 (iter 21--24) | `aug=4000` | **0.690** | First to reach 0.69; $\alpha = 0.85$, 16 outliers |

**Best result — Iteration 82**: `seed=77`, `coeff_MLP_sub_diff=8`, `lr_sub=0.001`, `data_augmentation_loop=4500`, `coeff_k_floor=1.0`, `coeff_MLP_sub_norm=1.0`

| Metric | Value |
|--------|-------|
| Raw R² | **0.764** |
| Outliers | 15 / 256 (5.9%) |
| Slope | 0.97 |
| $\alpha$ | 0.87 |

*Note: Optimal `sub_diff` is seed-dependent: 7 for seed=42 (R²=0.74), 8 for seed=77 (R²=0.76). Training has intrinsic variance of $\pm$0.08 R² even with identical configuration.*

### Rate Constants Recovery

::: {layout-ncol=3}

![Iteration 1 (baseline). R² = 0.044, 43 outliers. Before `k_floor` and `sub_norm` — predicted $\log k$ values scatter widely with no correlation to ground truth.](figures/rank_50/rate_constants_baseline.png){#fig-k-baseline}

![Iteration 14 (breakthrough). R² = 0.508, 33 outliers. The `k_floor=1.0` penalty prevents outlier $\log k$ from drifting below the true minimum, producing a 10$\times$ R² jump.](figures/rank_50/rate_constants_breakthrough.png){#fig-k-breakthrough}

![Iteration 82 (best). R² = 0.764, 15 outliers. `seed=77` + `sub_diff=8` + `aug=4500` yields the tightest clustering around the diagonal.](figures/rank_50/rate_constants_best.png){#fig-k-best}

:::

### Kinograph — Best Run (Iteration 82)

![Kinograph montage for iteration 82 (seed=77, sub_diff=8). Top-left: ground-truth $dc/dt$; top-right: GNN prediction; bottom-left: residual (same color scale as GT); bottom-right: predicted vs. true $dc/dt$ scatter. The GNN captures the dominant oscillatory patterns but misses fine-grained temporal structure.](figures/rank_50/kinograph_best.png){#fig-kinograph-best}

### Learned MLPs

::: {layout-ncol=2}

![Iteration 1 (baseline). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.42 — under-scaled. MLP$_{\text{node}}$: flat at zero.](figures/rank_50/mlp_baseline.png){#fig-mlp-baseline}

![Iteration 82 (best R²). MLP$_{\text{sub}}$: $\alpha$ at $|s|=1$ is 0.87 — close to the true scale. MLP$_{\text{node}}$: still flat at zero across all 84 iterations — homeostasis not learned.](figures/rank_50/mlp_best.png){#fig-mlp-best}

:::

### Block 3: Plateau and Breakthrough (Iter 25--36)

Twelve iterations explored the R² = 0.69 plateau. Eight variations failed — but doubling `lr_sub` from 0.0005 to 0.001 broke through to R² = 0.726.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 7 (iter 25--28) | `aug=5000`, seed, batch_size | 0.652 | `aug=5000` hurt R² but $\alpha=0.95$ (best ever) |
| 8 (iter 29--32) | L1=0, `sub_norm=2.0`, lr_k | 0.619 | `sub_norm=2.0`: fewest outliers (14), best slope (0.99) |
| 9 (iter 33--36) | `sub_norm=2.0`+`aug=3500`, recurrent, **`lr_sub=0.001`**, `aug=3500` | **0.726** | `lr_sub=0.001` broke the plateau |

```{python}
#| code-fold: true
#| label: fig-strategies
#| fig-cap: "Impact of key strategies on R². Each dot is one iteration. The k_floor penalty is the single most important factor. Optimal sub_diff is seed-dependent: sub_diff=7 for seed=42, sub_diff=8 for seed=77. Training has intrinsic variance of ±0.08 R²."

import matplotlib.pyplot as plt
import numpy as np

strategies = {
    'No k_floor\n(Block 1)': [0.044, 0.027, 0.044, 0.031, 0.013, 0.067, 0.041, 0.051, 0.054, 0.061, 0.017, 0.061],
    'k_floor\nsub_diff=5\nseed=42': [0.508, 0.638, 0.419, 0.658, 0.559, 0.470, 0.373, 0.690, 0.652, 0.638, 0.614, 0.600, 0.507, 0.619, 0.530, 0.409, 0.521, 0.478, 0.726, 0.544, 0.588, 0.518, 0.654, 0.662, 0.487, 0.690, 0.608, 0.593],
    'sub_diff=7\nseed=42': [0.736, 0.696, 0.655, 0.662, 0.560, 0.701, 0.718, 0.658],
    'seed=77\nsub_diff=8': [0.748, 0.661, 0.764],
    'Other seeds\n(7,99,123)': [0.688, 0.430, 0.722, 0.694, 0.387, 0.689],
    'Arch/k_floor\nchanges': [0.616, 0.704, 0.565, 0.674, 0.664, 0.609, 0.639, 0.473, 0.550, 0.583, 0.515, 0.473, 0.636, 0.603, 0.603],
}

fig, ax = plt.subplots(figsize=(12, 5))
positions = []
colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#95a5a6']
for i, (label, vals) in enumerate(strategies.items()):
    x = np.random.normal(i, 0.08, len(vals))
    ax.scatter(x, vals, s=40, alpha=0.7, zorder=3, color=colors[i], edgecolors='none')
    ax.plot([i-0.3, i+0.3], [np.mean(vals), np.mean(vals)], 'k-', linewidth=2, zorder=4)
    positions.append(i)

ax.set_xticks(positions)
ax.set_xticklabels(list(strategies.keys()), fontsize=8)
ax.set_ylabel('raw R²', fontsize=12)
ax.set_ylim(-0.05, 0.90)
ax.axhline(y=0.764, color='#f39c12', linestyle='--', alpha=0.3, label='Peak R² = 0.764')
ax.grid(axis='y', alpha=0.3)
ax.legend(fontsize=8)
plt.tight_layout()
plt.show()
```

### Block 4: Seed Sensitivity, then sub_diff=7 Breakthrough (Iter 37--48)

Block 4 first tested combinations with `lr_sub=0.001` and revealed seed sensitivity ($\pm$0.2 R²). Then `sub_diff=7` (stronger monotonicity) achieved a new peak R² = 0.736, lifting the ceiling from 0.73 to 0.74.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 10 (iter 37--40) | `sub_norm=2.0`, `lr_sub=0.002`, `lr_node=0.002`, L1=0 | 0.662 | All combinations with `lr_sub=0.001` hurt R² vs baseline |
| 11 (iter 41--44) | seed=123, `aug=4500`, `sub_diff=3`, `lr_k=0.007` | 0.690 | `aug=4500` most stable ($\alpha = 0.94$); seed=123 crashed to R²=0.49 |
| 12 (iter 45--48) | **`sub_diff=7`**, `lr_k=0.004`, `hidden_dim=128`, `batch_size=16` | **0.736** | `sub_diff=7` new best; wider/deeper MLP and larger batch hurt |

Iteration 45 (`sub_diff=7`, `aug=4500`) achieved R² = 0.736 with $\alpha = 0.90$ and 15 outliers — the strongest monotonicity constraint within the effective range. Iterations 46--48 confirmed that `lr_k=0.004` is too slow, `hidden_dim_sub=128` allows degenerate solutions, and `batch_size=16` degrades convergence.

### Block 5: Confirming Tight Optimization Bounds (Iter 49--60)

Block 5 systematically probed the boundaries around the optimal configuration. Every variation hurt R², confirming that the hyperparameter optimum is tightly constrained.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 13 (iter 49--52) | `aug=5000`, `sub_diff=8`, seed=123, `n_layers=4` | 0.696 | `aug=5000` hurts (confirmed twice); `sub_diff=7` improves seed robustness ($\pm$0.08 vs $\pm$0.24) |
| 14 (iter 53--56) | `aug=4250`, `sub_diff=6`, seed=123+`aug=4000`, `lr_sub=0.0015` | 0.701 | All variations hurt; `sub_diff=6` too weak, `lr_sub=0.0015` too high |
| 15 (iter 57--60) | seed=123+`aug=3500`, **seed=99**, L1=0.5, `sub_norm=0.5` | **0.718** | seed=99 promising (R²=0.72, $\alpha=0.92$); `sub_norm=0.5` confirmed essential principle |

Tight bounds confirmed: `aug=4500` (not 4000, 4250, or 5000), `sub_diff=7` (not 5, 6, or 8), `lr_sub=0.001` (not 0.0015 or 0.002), `lr_k=0.005` (not 0.004 or 0.007). `sub_diff=7` improved seed robustness: the R² gap between seed=42 and seed=123 dropped from 0.24 (with `sub_diff=5`) to 0.08.

### Block 6: High Variance Discovery (Iter 61--72)

Block 6 explored new seeds and k_floor variations, then discovered that training has **high intrinsic variance** — an exact replica of the best configuration (iter 45) achieved only R²=0.66 instead of 0.74.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 16 (iter 61--64) | seed=7, seed=99+`aug=4250`, `lr_node=0.0005`, **`k_floor=1.5`** | **0.704** | `k_floor=1.5` promising (R²=0.70, $\alpha=0.92$); seed=99 very sensitive to aug |
| 17 (iter 65--68) | `k_floor=1.25`, `k_floor=1.5`+seed=99, `aug=4750`, `sub_diff=6`+`k_floor=1.5` | 0.674 | **k_floor response is non-monotonic**: 1.25 < 1.0 and 1.5; `aug=4750` hurts |
| 18 (iter 69--72) | Exact replica of iter 45, `lr_k=0.0045`, `hidden_dim_node=32`, `sub_norm=1.5` | 0.658 | **HIGH VARIANCE**: replica got R²=0.66 vs original 0.74; MLP$_{\text{sub}}$ c² failure mode |

The high variance discovery is significant: identical configurations can produce R² values differing by 0.08, meaning many earlier "improvements" may have been within noise. This makes seed selection and reproducibility central concerns.

### Block 7: seed=77 Breakthrough (Iter 73--84)

Block 7 explored new seeds and found that **seed=77 is a "golden seed"** — achieving R²=0.748 (iter 80) and then R²=0.764 with `sub_diff=8` (iter 82), surpassing the previous best.

| Batch | Key mutation | Best R² | Finding |
|-------|-------------|---------|---------|
| 19 (iter 73--76) | seed=123, `lr_sub=0.0012`, `sub_diff=9`, `aug=4000` | 0.722 | seed=123 improved to R²=0.72 (vs 0.66 earlier); all other variations hurt |
| 20 (iter 77--80) | seed=7, `k_floor=1.5`, `n_layers=4`, **seed=77** | **0.748** | **seed=77 new best seed** (R²=0.748, 12 outliers); `n_layers=4` confirmed harmful |
| 21 (iter 81--84) | Replicate seed=77, **seed=77+`sub_diff=8`**, seed=78, `aug=5000` | **0.764** | **seed=77+sub_diff=8 = new global best**; replica got R²=0.66 (variance); seed=78 poor (R²=0.39) |

The key finding: **optimal `sub_diff` is seed-dependent**. `sub_diff=8` hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76). Adjacent seeds (78, 79) do not share seed=77's properties — "golden seeds" are rare and unpredictable.

### UCB Exploration Tree

![UCB tree after 24 iterations (snapshot). Each node represents a hyperparameter configuration; color encodes R² (green = high, red = low). The tree shows how the exploration branched from the `k_floor=1.0` breakthrough (node 14) and converged on the `aug=4000` regime. The tree has since grown to 84 nodes; the overall best is node 82 (R² = 0.764).](figures/rank_50/ucb_tree.png){#fig-ucb-tree}

### Established Principles

The LLM's persistent memory has accumulated these validated principles after 84 iterations:

1. **`coeff_MLP_sub_norm=1.0` is essential** — enables correct MLP shapes: c² becomes quadratic, MLP$_{\text{node}}$ activates. `sub_norm=0.5` and `sub_norm=1.5` both hurt (iter 60, 72).
2. **`coeff_k_floor=1.0` is critical** — R² jumped from 0.06 to 0.51 (10× improvement). `k_floor=2.0` too strong. `k_floor=1.5` gives R²=0.70 but response is **non-monotonic**: `k_floor=1.25` is worse than both 1.0 and 1.5 (iter 65).
3. **Longer training helps (up to `aug=4500`)** — aug=2000→3000→4000→4500 consistently improves R². aug=5000 hurts R² (confirmed 3×: iter 25, 49, 84). aug=4750 also hurts (iter 67). aug=4000 slightly suboptimal (iter 76).
4. **`lr_k=0.005` is optimal** — lower (0.003, 0.004, 0.0045) too slow, higher (0.007, 0.01) destabilizes (confirmed across 6 iterations)
5. **`lr_sub=0.001` is optimal** — 2× increase from 0.0005 broke the R² plateau; `lr_sub=0.002` too high (iter 38); `lr_sub=0.0015` hurts (iter 56); `lr_sub=0.0012` hurts (iter 74)
6. **`lr_node=0.001` is optimal** — `lr_node=0.005` destabilizes (iter 11); `lr_node=0.0005` hurts (iter 63)
7. **Optimal `sub_diff` is seed-dependent** — `sub_diff=7` for seed=42 (R²=0.74, iter 45), `sub_diff=8` for seed=77 (R²=0.76, iter 82). `sub_diff=8` hurts seed=42 (iter 50) but helps seed=77.
8. **Training has high intrinsic variance ($\pm$0.08 R²)** — exact replicas: iter 45 R²=0.74 vs iter 69 R²=0.66; iter 80 R²=0.75 vs iter 81 R²=0.66. Many earlier "improvements" may be within noise.
9. **seed=77 is the best seed** — R²=0.748 (iter 80), R²=0.764 with `sub_diff=8` (iter 82). Adjacent seeds (78, 79) are poor — "golden seeds" are rare and unpredictable.
10. **`batch_size=8` is optimal** — `batch_size=16` hurts R² (iter 48)
11. **Default MLP architecture is optimal** — `hidden_dim_sub=128` allows degenerate solutions (iter 47); `hidden_dim_node=32` significantly worse (iter 71); `n_layers_sub=4` hurts (iter 52, 79)
12. **`coeff_MLP_node_L1=1.0` is optimal** — L1=0.0 + long training harmful (iter 22, 29, 40); L1=0.5 hurts (iter 59)
13. **`sub_diff=7` improves seed robustness** — R² gap between seed=42 and seed=123 dropped from 0.24 (with `sub_diff=5`) to 0.08 (with `sub_diff=7`)
14. **MLP$_{\text{node}}$ remains flat across all 84 iterations** — homeostasis $-\lambda(c - c^{\text{base}})$ is never learned, regardless of configuration

### Refuted Hypotheses

| Hypothesis | Evidence |
|-----------|---------|
| Recurrent training breaks degeneracy | No R² improvement, 3.5× slower (iter 13, 34) |
| Smaller MLP improves k recovery | Worst R² = 0.011 (iter 15) |
| Higher `lr_node` activates MLP$_{\text{node}}$ | `lr_node=0.005` hurts (iter 11); 0.002 no effect (iter 26, 39) |
| Different seed breaks degeneracy | Same MLP$_{\text{node}}$ flatness (iter 27); R² variance $\pm$0.2 (iter 41) |
| Stronger monotonicity (`sub_diff=10`) helps | R² dropped to 0.41 (iter 32) |
| Weaker monotonicity (`sub_diff=3`) helps | R² dropped to 0.61 (iter 43) |
| Smaller batch size helps convergence | R² dropped (iter 28) |
| `aug=5000` continues to improve | Confirmed 3×: iter 25 (0.65), iter 49 (0.70), iter 84 (0.69) |
| `lr_sub=0.002` improves over 0.001 | R² dropped from 0.73 to 0.52 (iter 38) |
| `sub_norm=2.0` improves R² | Improves $\alpha$ but hurts R² (iter 30, 37) |
| Combining `lr_sub=0.001` with other changes | All combinations hurt R² vs baseline (iter 37--40) |
| Wider MLP$_{\text{sub}}$ (`hidden_dim=128`) helps | R² dropped to 0.56 — allows degenerate solutions (iter 47) |
| Larger `batch_size=16` stabilizes gradients | R² dropped to 0.56 (iter 48) |
| `sub_diff=8` hurts all seeds | Hurts seed=42 (iter 50, R²=0.59) but helps seed=77 (iter 82, R²=0.76) |
| `sub_diff=6` is better than 7 | R² dropped from 0.74 to 0.56 (iter 54) |
| Deeper MLP$_{\text{sub}}$ (`n_layers=4`) helps | R² dropped to 0.55 (iter 52); confirmed iter 79 (R²=0.47) |
| `lr_sub=0.0015` (intermediate) helps | R² dropped from 0.74 to 0.60 (iter 56) |
| `lr_k=0.004` gives finer convergence | R² dropped to 0.48 — too slow (iter 46) |
| `sub_norm=0.5` helps | R² dropped from 0.74 to 0.60 (iter 60) |
| Intermediate `k_floor=1.25` is optimal | R²=0.56, worse than both 1.0 and 1.5 — non-monotonic (iter 65) |
| `aug=4750` within safe range | R² dropped to 0.66 despite $\alpha$=0.96 (iter 67) |
| `sub_norm=1.5` better than 1.0 | R²=0.55 vs 0.66 (iter 72) |
| `hidden_dim_node=32` simpler is better | R²=0.47, significantly worse (iter 71) |
| `lr_sub=0.0012` helps MLP$_{\text{sub}}$ | R²=0.58 vs 0.66 (iter 74) |
| seed=42 is the best seed | seed=77 achieved R²=0.764 (iter 82), surpassing seed=42's best of 0.736 |
| Adjacent seeds share properties | seed=78 R²=0.39 (iter 83), seed=79 R²=0.47 (iter 79) — both poor |
| `k_floor=1.5` is reproducible | Same config gave R²=0.70 (iter 64) and R²=0.51 (iter 78) |

### Open Questions

1. **Is R² $\approx$ 0.77 achievable?** After 84 iterations, the best R²=0.764 (seed=77+sub_diff=8). Training variance of $\pm$0.08 means a lucky run could reach 0.77+, but systematic improvement is unclear.
2. **How much variance is seed-dependent vs stochastic?** Identical configs yield $\pm$0.08 R² (iter 45 vs 69, iter 80 vs 81). Seed choice adds another $\pm$0.15 on top.
3. **Can ensemble averaging across seeds reduce variance?** Averaging predictions from multiple seeds could improve robustness beyond any single run.
4. **Are there more "golden seeds" like 77?** Adjacent seeds (76, 78, 79) are poor — what makes seed=77 special?
5. **Why is the k_floor response non-monotonic?** `k_floor=1.25` is worse than both 1.0 and 1.5 (iter 65). The mechanism is unknown.
6. **Is the remaining 24% error from identifiability issues?** Multiple $k$ combinations may produce similar $dc/dt$.

## Why Homeostasis Is Not Learned

Across all 84 iterations, MLP$_{\text{node}}$ remains flat — the homeostasis function $-\lambda_t(c_i - c_i^{\text{baseline}})$ is never recovered. This is not a hyperparameter issue. It is a structural limitation of single-step ($t \to t+1$) training.

### The scale mismatch problem

The GNN predicts $dc/dt$ at each time step:

$$
\frac{dc_i}{dt} = \underbrace{\text{MLP}_{\text{node}}(c_i, a_i)}_{\text{homeostasis}} + \underbrace{\sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(c_k, |S_{kj}|)}_{\text{reaction}}
$$

The reaction term dominates the instantaneous $dc/dt$ — it drives the fast oscillatory dynamics. Homeostasis acts as a slow restoring force that only manifests over many time steps: it prevents concentrations from drifting away from baseline. In a single-step loss $\| \hat{y}_{t+1} - y_{t+1} \|^2$, the gradient signal from homeostasis is negligible compared to the reaction term. The optimizer has no reason to learn it.

### Homeostasis is an integral problem

Homeostasis determines the **long-term trajectory envelope**, not the instantaneous derivative. Its effect accumulates over time:

$$
c_i(t + T) \approx c_i(t) + \int_t^{t+T} \left[ -\lambda_i (c_i - c_i^{\text{base}}) + \text{reaction terms} \right] d\tau
$$

A model trained on $t \to t+1$ can achieve low loss by fitting the dominant reaction signal and ignoring the small homeostatic correction. Over many steps this error accumulates — but the single-step loss never sees it.

### Proposed approach: two-phase training

To recover homeostasis, we propose a two-phase training scheme:

1. **Phase 1 — Reaction recovery** ($t \to t+1$): Train as currently done. Recover $k_j$, MLP$_{\text{sub}}$, and the stoichiometric structure. Freeze these parameters.

2. **Phase 2 — Homeostasis recovery** (recurrent, $t \to t+T$): With the reaction term frozen, train only MLP$_{\text{node}}$ and embeddings $a_i$ using multi-step rollout. The recurrent loss forces the model to match trajectories over $T$ steps, where the homeostatic drift becomes visible.

This separates the two learning problems by time scale: fast reactions are learned from instantaneous gradients, slow homeostasis from trajectory matching. The key insight is that recurrent training is not needed for $k_j$ recovery (it was tried and failed — iter 13, 34), but may be essential specifically for homeostasis once the reaction parameters are frozen.

### Alternative approach: residual-based supervision

The kinograph residual (@fig-kinograph-best, bottom-left) directly reveals what the single-step model cannot learn. After Phase 1, we can roll out the learned model autoregressively:

$$
\hat{c}_i(t+1) = \hat{c}_i(t) + \Delta t \cdot \left[ \sum_j S_{ij} \cdot k_j \prod_k \text{MLP}_{\text{sub}}(\hat{c}_k, |S_{kj}|) \right]
$$

The rollout trajectory $\hat{c}(t)$ drifts from the observation $c(t)$ because the missing homeostatic restoring force is not applied at each step. The accumulated residual

$$
r_i(t) = c_i(t) - \hat{c}_i(t)
$$

is precisely the integrated effect of the missing slow terms — homeostasis, external sources, and degradation. This residual provides a direct supervision signal for Phase 2 without requiring backpropagation through time: we can compute a per-step target

$$
\frac{r_i(t+1) - r_i(t)}{\Delta t} \approx \text{MLP}_{\text{node}}(c_i(t), a_i)
$$

and train MLP$_{\text{node}}$ with a standard single-step loss on this derived target. This avoids the instabilities of recurrent training while still capturing the long-term dynamics that the reaction-only model structurally cannot predict.

This approach is an instance of the **Universal Differential Equations** framework ([Rackauckas et al., 2021](https://arxiv.org/abs/2001.04385)), where a partially known ODE is augmented with a neural network that learns the missing terms from data. Here, the known part is the reaction dynamics $\sum_j S_{ij} k_j \prod_k \text{MLP}_{\text{sub}}$, and the unknown part is the homeostatic correction learned by MLP$_{\text{node}}$. The residual-based supervision strategy is also related to **PDE-Refiner** ([Lippe et al., NeurIPS 2023](https://arxiv.org/abs/2308.05732)), which uses iterative refinement on rollout residuals to recover low-amplitude dynamics that single-pass neural solvers miss — analogous to the small homeostatic signal masked by dominant reactions.

### Related work

The difficulty of learning slow dynamics from single-step training is well established in the literature:

- **Teacher forcing and long-term dependencies.** [Williams & Zipser (1989)](https://direct.mit.edu/neco/article/1/2/270/5490/) introduced teacher forcing for RNNs: feeding ground-truth inputs at each step. [Bengio et al. (1994)](https://ieeexplore.ieee.org/document/279181) showed that gradient-based learning of long-term dependencies is fundamentally difficult because short-term gradient contributions dominate, exactly the mechanism that prevents our GNN from learning homeostasis.

- **Scheduled sampling.** [Bengio et al. (2015)](https://arxiv.org/abs/1506.03099) proposed a curriculum strategy that gradually transitions from teacher forcing (single-step) to free-running (multi-step) prediction, reducing the train–inference discrepancy. Our two-phase proposal is conceptually related: Phase 1 is teacher-forced, Phase 2 uses multi-step rollout.

- **Multiple shooting for Neural ODEs.** [Massaroli et al. (2021)](https://arxiv.org/abs/2106.03885) introduced differentiable multiple shooting layers that parallelize trajectory integration. [Turan & Jäschke (2021)](https://arxiv.org/abs/2109.06786) showed that standard Neural ODE fitting on oscillatory data produces "flattened" trajectories — multiple shooting recovers the true dynamics by breaking long horizons into segments. This directly addresses the failure mode we observe.

- **Stiff Neural ODEs.** [Kim et al. (2021)](https://arxiv.org/abs/2103.15341) showed that learning neural ODEs for systems with widely separated time scales (stiff systems) requires proper output scaling and stabilized gradients — the fast dynamics otherwise dominate training, exactly as in our reaction-vs-homeostasis scale mismatch.

- **Multi-scale separation in dynamical systems.** [Fenichel (1979)](https://doi.org/10.1016/0022-0396(79)90152-9) established geometric singular perturbation theory: for systems with fast and slow time scales, the fast subsystem is solved first, then the dynamics are reduced to a slow manifold. Our two-phase training mirrors this decomposition — Phase 1 recovers the fast reaction dynamics, Phase 2 learns the slow homeostatic manifold.

- **Latent timescales in Neural ODEs.** [Gupta et al. (2024)](https://arxiv.org/abs/2403.02224) showed that training trajectory length directly controls the timescales a Neural ODE can recover: longer trajectories are needed to capture slower dynamics. This supports the need for multi-step rollout in Phase 2.

The scale mismatch / integral argument is essentially the same observation that Bengio (1994) made for RNNs and Kim et al. (2021) made for stiff ODEs. The two-phase proposal mirrors Fenichel's fast-then-slow decomposition. The novelty here is applying this reasoning to GNN-based metabolic network recovery, where the bipartite graph structure and the identifiability of rate constants $k_j$ add domain-specific constraints.
