---
title: "Training Guide"
---

## Training Overview

Training a MetabolismGraph model involves learning three components simultaneously:

1. **Stoichiometric coefficients** — the sparse integer matrix $\mathbf{S}$
2. **Rate functions** — the MLPs that compute reaction rates
3. **Rate constants** — the per-reaction scaling factors $k_j$

## Configuration

Training is configured via YAML files. Here's an example:

```yaml
description: metabolism simulation
dataset: metabolism_Claude_01

simulation:
  n_metabolites: 100
  n_reactions: 256
  max_metabolites_per_reaction: 4
  n_frames: 2880
  delta_t: 1
  seed: 42

graph_model:
  model_name: PDE_M1
  hidden_dim: 32
  output_size: 16
  n_layers: 3

training:
  n_epochs: 1
  batch_size: 8
  data_augmentation_loop: 1000
  learning_rate_start: 0.001
  learning_rate_S_start: 0.0005
  coeff_S_L1: 0.001
  coeff_S_integer: 0.001
  coeff_mass_conservation: 0.001
```

## Key Training Parameters

### Learning Rates

| Parameter | Description | Typical Range |
|-----------|-------------|---------------|
| `learning_rate_start` | LR for MLP parameters | 1E-4 to 1E-2 |
| `learning_rate_S_start` | Separate LR for stoichiometry | 1E-4 to 1E-2 |

**Key insight**: `lr_S` controls how fast stoichiometry is learned relative to the MLPs. Too fast and $\mathbf{S}$ overfits before the MLPs learn good rate functions.

### Regularization Coefficients

| Parameter | Loss Term | Effect |
|-----------|-----------|--------|
| `coeff_S_L1` | $\lambda_1 \|\mathbf{S}\|_1$ | Promotes sparsity (zeros) |
| `coeff_S_integer` | $\lambda_2 \sin^2(\pi S)$ | Encourages integer values |
| `coeff_mass_conservation` | $\lambda_3 (\sum_i S_{ij})^2$ | Enforces mass balance |

## Loss Visualization

Training progress is saved to `{log_dir}/tmp_training/loss.tif`:

```{mermaid}
%%| fig-width: 8
xychart-beta
    title "Loss Components Over Training"
    x-axis "Iteration" 0 --> 72000
    y-axis "Loss" 0 --> 1
    line "Prediction Loss" [1.0, 0.8, 0.5, 0.3, 0.2, 0.15, 0.1]
    line "L1 Regularization" [0.5, 0.4, 0.3, 0.2, 0.15, 0.1, 0.08]
    line "Integer Penalty" [0.4, 0.35, 0.3, 0.2, 0.1, 0.05, 0.02]
```

### Color Scheme

| Color | Component | What to Look For |
|-------|-----------|------------------|
| **Blue** (thick) | Prediction loss | Should decrease steadily |
| **Cyan** | Total regularization | Should be 10-100x smaller than blue |
| **Red** | S L1 sparsity | Decreasing = learning sparse S |
| **Orange** | S integer penalty | Decreasing = converging to integers |
| **Green** | Mass conservation | Low = reactions are balanced |

## Training Metrics

At the end of training, these metrics are written to `analysis.log`:

| Metric | Description | Good Value |
|--------|-------------|------------|
| `stoichiometry_R2` | $R^2$ between learned and true S | > 0.9 |
| `substrate_func_R2` | $R^2$ for substrate function | > 0.8 |
| `rate_func_R2` | $R^2$ for rate function | > 0.8 |

## Advanced Options

### Variance-Weighted Sampling

When `variance_weighted_sampling: true`, timepoints are sampled proportionally to the variance of $dc/dt$. This focuses training on "active" frames rather than equilibrium.

```yaml
training:
  variance_weighted_sampling: true
```

**Sampling strategy:**

- 80% from variance-weighted distribution
- 20% uniform (ensures steady-state coverage)

### Two-Phase Training

Train with different L1 coefficients in two phases:

```yaml
training:
  n_epochs_init: 5       # First 5 epochs with low L1
  first_coeff_L1: 0.0    # No sparsity initially
  coeff_S_L1: 0.01       # Then apply strong sparsity
```

This allows the model to find approximate S values before enforcing sparsity.

### Recurrent Training

For improved temporal consistency:

```yaml
training:
  recurrent_training: true
  time_step: 4
  noise_recurrent_level: 0.01
```

This trains the model to predict multiple steps ahead using its own predictions.

## Troubleshooting

### Low stoichiometry_R2

- **Cause**: Regularization too weak or learning rates imbalanced
- **Fix**: Increase `coeff_S_L1`, decrease `lr_S`, or train longer

### Loss oscillating

- **Cause**: Learning rates too high
- **Fix**: Reduce `learning_rate_start` and/or `learning_rate_S_start`

### Prediction loss increasing

- **Cause**: Regularization too strong
- **Fix**: Reduce regularization coefficients

### Non-integer S values

- **Cause**: `coeff_S_integer` too low
- **Fix**: Increase `coeff_S_integer` (try 0.01-0.1)
